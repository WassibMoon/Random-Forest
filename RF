 2/1:
import cv2
import numpy as np
import os
import pandas as pd
import matplotlib.pyplot as plt
from pylab import rcParams

TRAIN = './input/train/'
TEST = './input/test/'
LABELS = './input/train.csv'
IMG_SIZE = 128
 2/2:
import cv2
import numpy as np
import os
import pandas as pd
import matplotlib.pyplot as plt
from pylab import rcParams

TRAIN = './input/train/'
TEST = './input/test/'
LABELS = './input/train.csv'
IMG_SIZE = 128
 2/3:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
 2/4:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
 2/5:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
 2/6:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
 2/7:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
 2/8:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
 2/9:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
2/10:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
2/11: import keras
2/12:
import sys; sys.path
import keras
2/13:
import sys; sys.path
#import keras
2/14:
import sys; sys.path
source deactivate && source activate [my_env]
#import keras
2/15:
import sys; sys.path
sys.executable
#import keras
 3/1:
import sys; sys.path
sys.executable
#import keras
 3/2:
import sys; sys.path
sys.executable
import keras
 6/1: import numpy as np
 6/2: import pandas as pd
 6/3:
import sys
print(sys.v
 6/4:
import sys
print(sys.v)
 6/5:
import sys
print(sys.path)
 7/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output.
 7/2:
import matplotlib.pyplot as plt
from PIL import Image
from torch import nn
from torch import optim
from torch.autograd import Variable
import torch.nn.functional as F
from torch.optim import lr_scheduler
import torch.utils.data as data
from torch.utils.data.dataset import Dataset
from torch.utils.data.dataloader import DataLoader
from torchvision import models
from torchvision import transforms
import torchvision.datasets as datasets
 7/3:
import matplotlib.pyplot as plt
from PIL import Image

from torchvision import models
from torchvision import transforms
import torchvision.datasets as datasets
 7/4:
import matplotlib.pyplot as plt
from PIL import Image
 7/5:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
train = pd.read_csv(path+'train.csv')
print(train.shape)
train.head(3)
 7/6:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
 7/7:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
 7/8:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
 7/9: popular_whale=whale_images('w_23a388d',train)
7/10:
for pic in popular_whale: 
    p=path+'train/'+pic
    if  os.path.isdir(p)==True:
        with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
7/11:
for pic in popular_whale: 
    p=path+'train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
7/12:
for pic in popular_whale[0:10]: 
    p=path+'train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
7/13:
unique_whale_data=train.groupby('Id').count().sort_values('Image',ascending = False)
unique_whale_data.head(10)
7/14:
for pic in popular_whale[0:5]: 
    p=path+'train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
7/15: pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
7/16:
unique_whale_data = df_train.groupby("Id").Image.nunique()
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
7/17:
unique_whale_data = train.groupby("Id").Image.nunique()
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
7/18:
unique_whale_data = train.groupby("Id").Image.nunique()
unique_whale_data.head(10)
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
7/19:
unique_whale_data = train.groupby("Id").Image.nunique()
unique_whale_data.head(10)
#pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
7/20:
id_count=train.groupby('Id').count().sort_values('Image',ascending = False)
id_count.head(10)
7/21:
#train.groupby('Id').count().plot.hist(range = (0,10))
id_count.plot.bar()
7/22:
id_count.plot.hist(range = (0,10))
#id_count.plot.bar()
7/23:
id_count.plot.hist(range = (0,20))
#id_count.plot.bar()
7/24:
unique_whale_data = df_train.groupby("Id").Image.nunique()
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
7/25:
unique_whale_data = train.groupby("Id").Image.nunique()
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
7/26:
id_count.plot.bar(range = (0,20))
#id_count.plot.bar()
7/27:
id_count.plot.hist(range = (0,20))
#id_count.plot.bar()
 5/1:
import cv2
import numpy as np
import os
import pandas as pd
import matplotlib.pyplot as plt
from pylab import rcParams

TRAIN = './input/train/'
TEST = './input/test/'
LABELS = './input/train.csv'
IMG_SIZE = 128
 5/2:
file_names = os.listdir(TRAIN)
df_train = pd.read_csv(LABELS)
 5/3:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return saliencyMap, np.array(polar_img, dtype="float") / 255.0, np.array(normalized_img, dtype="float") / 255.0
 5/4:
unique_whale_data = df_train.groupby("Id").Image.nunique()
#unique_whale_data

#unique_whale_data.head()

sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.head()
7/28: id_map = train[['Id']].drop_duplicates()
7/29: print("Number of distinct Ids:",id_map.shape[0]-1)
7/30: train=train.sample(frac=0.2,random_state=1)
7/31:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
print(df_train.shape)
train.head(3)
7/32:
id_count=df_train.groupby('Id').count().sort_values('Image',ascending = False)
id_count.head(10)
7/33:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
7/34: popular_whale=whale_images('w_23a388d',df_train)
7/35: df_train=df_train.sample(frac=0.2,random_state=1)
7/36:
for pic in df_train['Image']:
        p=path+'train/'+pic
        #if  os.path.isdir(p)==True:
        with Image.open(p) as img:
                 width, height = img.size
        #img.shape
                 print(img.size) 
        #train['Size']=train['Image'].apply(lambda x: cv2.imread('../input/train/'+x).size)
7/37:
for pic in df_train['Image']:
        p=path+'train/'+pic
        #if  os.path.isdir(p)==True:
        with Image.open(p) as img:
                 width, height = img.size
        #img.shape
        #train['Size']=train['Image'].apply(lambda x: cv2.imread('../input/train/'+x).size)
7/38:
#for pic in df_train['Image']:
        #p=path+'train/'+pic
        #if  os.path.isdir(p)==True:
        #with Image.open(p) as img:
         #       width, height = img.size
          #      print(img.shape)
df_train['Size']=df_train['Image'].apply(lambda x: cv2.imread('../input/train/'+x).size)
7/39:
#for pic in df_train['Image']:
        #p=path+'train/'+pic
        #if  os.path.isdir(p)==True:
        #with Image.open(p) as img:
         #       width, height = img.size
          #      print(img.shape)
df_train['Size']=df_train['Image'].apply(lambda x: Image.open(path+'train/'+x).size)
7/40:
#for pic in df_train['Image']:
        #p=path+'train/'+pic
        #if  os.path.isdir(p)==True:
        #with Image.open(p) as img:
         #       width, height = img.size
          #      print(img.shape)
df_train['Size']=df_train['Image'].apply(lambda x: Image.open(path+'train/'+x).size)
7/41: df_train.head()
7/42: sorted_unique.iloc[1:30].plot.bar()
7/43:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar()
7/44:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
7/45:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return saliencyMap, np.array(polar_img, dtype="float") / 255.0, np.array(normalized_img, dtype="float") / 255.0
7/46:
for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)


    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/47:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return saliencyMap, np.array(polar_img, dtype="float") / 255.0, np.array(normalized_img, dtype="float") / 255.0
7/48:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
7/49: df_train=df_train.sample(frac=0.2,random_state=1)
7/50:
for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)


    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/51:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
7/52:
for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)


    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/53:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
7/54:
for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)


    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/55:
for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' ))

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/56:
for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/57:
from PIL import Image, ImageFilter
for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/58:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', gray_image );
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/59:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar );
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/60:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    img_polar.convertTo(img_polar, CV_8UC3, 255.0); 
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar );
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/61:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    image.convertTo(img_polar, CV_8UC3, 255.0); 
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar );
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/62:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    #image.convertTo(img_polar, CV_8UC3, 255.0); 
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255);
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
7/63:
#saving the transferred images 
for pic in df_train[Image]:
    image_path = TRAIN + file_names[i]
    img_salient, img_polar, img_normalized = load_image(image_path)
    
    cv2.imwrite( path+'salient_train/salient_'+file_names[i]+'.jpg', img_salient_train*255);
    cv2.imwrite( path+'normalized_train/normalized_'+file_names[i]+'.jpg', img_normalized*255);
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255);
7/64: len(file_name)
7/65: len(file_names)
7/66:
#saving the transferred images 
for i in range(0,len(file_names)):
    image_path = TRAIN + file_names[i]
    img_salient, img_polar, img_normalized = load_image(image_path)
    cv2.imwrite( path+'salient_train/salient_'+file_names[i]+'.jpg', img_salient_train*255)
    cv2.imwrite( path+'normalized_train/normalized_'+file_names[i]+'.jpg', img_normalized*255)
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255)
7/67:
#saving the transferred images 
for i in range(0,len(file_names)):
    image_path = TRAIN + file_names[i]
    img_salient, img_polar, img_normalized = load_image(image_path)
    cv2.imwrite( path+'salient_train/salient_'+file_names[i]+'.jpg', img_salient*255)
    cv2.imwrite( path+'normalized_train/normalized_'+file_names[i]+'.jpg', img_normalized*255)
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255)
 9/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
 9/2:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
 9/3: popular_whale=whale_images('w_23a388d',df_train)
 9/4:
for pic in popular_whale[0:5]: 
    p=path+'train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
 9/5:
unique_whale_data = df_train.groupby("Id").Image.nunique()
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
 9/6:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar()
 9/7:
id_count.plot.hist(range = (0,20))
#id_count.plot.bar()
 9/8:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
 9/9:
import matplotlib.pyplot as plt
from PIL import Image
9/10:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
print(df_train.shape)
train.head(3)
9/11:
id_count=df_train.groupby('Id').count().sort_values('Image',ascending = False)
id_count.head(10)
9/12:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
9/13: popular_whale=whale_images('w_23a388d',df_train)
9/14:
for pic in popular_whale[0:5]: 
    p=path+'train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
9/15:
unique_whale_data = df_train.groupby("Id").Image.nunique()
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
9/16:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar()
9/17:
id_count.plot.hist(range = (0,20))
#id_count.plot.bar()
9/18: id_map = df_train[['Id']].drop_duplicates()
9/19: print("Number of distinct Ids:",id_map.shape[0]-1)
9/20: df_train=df_train.sample(frac=0.1,random_state=1)
9/21:
#for pic in df_train['Image']:
        #p=path+'train/'+pic
        #if  os.path.isdir(p)==True:
        #with Image.open(p) as img:
         #       width, height = img.size
          #      print(img.shape)
df_train['Size']=df_train['Image'].apply(lambda x: Image.open(path+'train/'+x).size)
9/22: df_train.head()
9/23:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return saliencyMap, np.array(polar_img, dtype="float") / 255.0, np.array(normalized_img, dtype="float") / 255.0
9/24:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return saliencyMap, np.array(polar_img, dtype="float") / 255.0, np.array(normalized_img, dtype="float") / 255.0
9/25:
#saving the transferred images 
for pic in train_df.image
    image_path = TRAIN + pic
    img_salient, img_polar, img_normalized = load_image(image_path)
    cv2.imwrite( path+'salient_train/salient_'+file_names[i]+'.jpg', img_salient*255)
    cv2.imwrite( path+'normalized_train/normalized_'+file_names[i]+'.jpg', img_normalized*255)
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255)
9/26:
#saving the transferred images 
for pic in train_df.image:
    image_path = TRAIN + pic
    img_salient, img_polar, img_normalized = load_image(image_path)
    cv2.imwrite( path+'salient_train/salient_'+file_names[i]+'.jpg', img_salient*255)
    cv2.imwrite( path+'normalized_train/normalized_'+file_names[i]+'.jpg', img_normalized*255)
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255)
9/27:
#saving the transferred images 
for pic in df_train.image:
    image_path = TRAIN + pic
    img_salient, img_polar, img_normalized = load_image(image_path)
    cv2.imwrite( path+'salient_train/salient_'+file_names[i]+'.jpg', img_salient*255)
    cv2.imwrite( path+'normalized_train/normalized_'+file_names[i]+'.jpg', img_normalized*255)
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255)
9/28:
#saving the transferred images 
for pic in df_train.Image:
    image_path = TRAIN + pic
    img_salient, img_polar, img_normalized = load_image(image_path)
    cv2.imwrite( path+'salient_train/salient_'+file_names[i]+'.jpg', img_salient*255)
    cv2.imwrite( path+'normalized_train/normalized_'+file_names[i]+'.jpg', img_normalized*255)
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255)
9/29:
#saving the transferred images 
for pic in df_train.Image:
    image_path = TRAIN + pic
    img_salient, img_polar, img_normalized = load_image(image_path)
    cv2.imwrite( path+'salient_train/salient_'+pic+'.jpg', img_salient*255)
    cv2.imwrite( path+'normalized_train/normalized_'+pic+'.jpg', img_normalized*255)
    cv2.imwrite( path+'polar_train/polar_'+pic+'.jpg', img_polar*255)
9/30:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    #image.convertTo(img_polar, CV_8UC3, 255.0); 
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255);
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    plt.axis("off")
    plt.show()
9/31:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
print(df_train.shape)
train.head(3)
9/32:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
print(df_train.shape)
df_train.head(3)
9/33: df_train=df_train.sample(frac=0.1,random_state=1)
9/34: df_train.drop(Size,axis=1)
9/35: #df_train.drop(Size,axis=1)
9/36: (trainX, testX, trainY, testY) = train_test_split(df_train, df.Id, test_size=0.25)
9/37:
from sklearn import train_test_split
(trainX, testX, trainY, testY) = train_test_split(df_train, df.Id, test_size=0.25)
9/38:
from sklearn.model_selection import train_test_split 
(trainX, testX, trainY, testY) = train_test_split(df_train, df.Id, test_size=0.25)
9/39:
from sklearn.model_selection import train_test_split 
(trainX, testX, trainY, testY) = train_test_split(df_train, df_train.Id, test_size=0.25)
9/40: trainX.head()
9/41: trainY.head()
9/42: trainX.head()
9/43:
from sklearn.model_selection import train_test_split 
(trainX, testX, trainY, testY) = train_test_split(df_train.Image, df_train.Id, test_size=0.25,random_state=1)
9/44: trainX.head()
9/45:
#df_train.drop(Size,axis=1)
#droping the new_whale images
df_train=df_train[df_train.Id != 'new_whale']
9/46:
#df_train.drop(Size,axis=1)
#droping the new_whale images
df_train=df_train[df_train.Id != 'new_whale']
df_train.shape
9/47:
from sklearn.model_selection import train_test_split 
(trainX, testX, trainY, testY) = train_test_split(df_train.Image, df_train.Id, test_size=0.25,random_state=1)
9/48:
from keras.utils import np_utils
#np.utils.to_categorical is used to convert array of labeled data(from 0 to nb_classes-1) to one-hot matrix(vector)
Y_train=np_utils.to_categorical(y_train, num_classes = num_classes)
9/49:
from keras.utils import np_utils
#np.utils.to_categorical is used to convert array of labeled data(from 0 to nb_classes-1) to one-hot matrix(vector)
Y_train=np_utils.to_categorical(trainY, num_classes = num_classes)
9/50: num_classes=id_map.shape[0]-2 #excluding new_whale
9/51:
from keras.utils import np_utils
#np.utils.to_categorical is used to convert array of labeled data(from 0 to nb_classes-1) to one-hot matrix(vector)
Y_train=np_utils.to_categorical(trainY, num_classes = num_classes)
9/52:
from keras.utils import np_utils
#np.utils.to_categorical is used to convert array of labeled data(from 0 to nb_classes-1) to one-hot matrix(vector)
Y_train=np_utils.to_categorical(trainY[:],, num_classes = num_classes)
9/53:
from keras.utils import np_utils
#np.utils.to_categorical is used to convert array of labeled data(from 0 to nb_classes-1) to one-hot matrix(vector)
Y_train=np_utils.to_categorical(trainY[:], num_classes = num_classes)
9/54:
from sklearn.model_selection import train_test_split 
(trainX, testX, trainY, testY) = train_test_split(df_train.Image, df_train.Id, test_size=0.25,shuffle=True,random_state=1)
9/55:
from sklearn.preprocessing import OneHotEncoder
onehotencoder = OneHotEncoder(categorical_features = [0])
Y_train = onehotencoder.fit_transform(trainY).toarray()
9/56:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
ohe = OneHotEncoder(sparse=False ) 
# apply OneHotEncoder on categorical feature columns
y = ohe.fit_transform(trainY) # It returns an numpy array
9/57:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
ohe = OneHotEncoder(sparse=False ) 
# apply OneHotEncoder on categorical feature columns
trainY.shape
y = ohe.fit_transform(trainY) # It returns an numpy array
9/58:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
ohe = OneHotEncoder(sparse=False ) 
# apply OneHotEncoder on categorical feature columns
trainY.shap
y = ohe.fit_transform(trainY) # It returns an numpy array
9/59:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
ohe = OneHotEncoder(sparse=False ) 
# apply OneHotEncoder on categorical feature columns

y = ohe.fit_transform(trainY.reshape(*trainY.shape)) # It returns an numpy array
9/60:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
ohe = OneHotEncoder(sparse=False ) 
# apply OneHotEncoder on categorical feature columns

y = ohe.fit_transform(trainY.reshape(-1,1)) # It returns an numpy array
9/61:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
ohe = OneHotEncoder(sparse=False ) 
# apply OneHotEncoder on categorical feature columns
y = pd.DataFrame(trainY)
y = ohe.fit_transform(trainY.reshape(-1,1)) # It returns an numpy array
9/62:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
ohe = OneHotEncoder(sparse=False ) 
# apply OneHotEncoder on categorical feature columns
y = pd.DataFrame(trainY)
y = ohe.fit_transform(y.reshape(-1,1)) # It returns an numpy array
9/63:
num_classes=id_map.shape[0]-2 #excluding new_whale
df_train[:,0]
9/64:
num_classes=id_map.shape[0]-2 #excluding new_whale
df_train[:,1]
9/65:
num_classes=id_map.shape[0]-2 #excluding new_whale
df_train[,0]
9/66:
num_classes=id_map.shape[0]-2 #excluding new_whale
df_train.shape
9/67:
num_classes=id_map.shape[0]-2 #excluding new_whale
df_train[:,1]
9/68:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
onehotencoder1 = OneHotEncoder(categorical_features = [1] ) 
# apply OneHotEncoder on categorical feature columns
df_train= onehotencoder1.fit_transform(df_train).toarray
9/69:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
onehotencoder1 = OneHotEncoder(["Id"] ) 
# apply OneHotEncoder on categorical feature columns
 XX=onehotencoder1.fit_transform(df_train)
9/70:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
# instantiate OneHotEncoder
onehotencoder1 = OneHotEncoder(["Id"] ) 
# apply OneHotEncoder on categorical feature columns
XX=onehotencoder1.fit_transform(df_train)
9/71:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
values=trainY
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)
print(integer_encoded)
# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
print(onehot_encoded)
9/72:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
values=trainY

# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded =values.reshape(len(values), 1)
Y_train = onehot_encoder.fit_transform(integer_encoded)
9/73:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
values=df_train.Id
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)

# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
Y = onehot_encoder.fit_transform(integer_encoded)
9/74:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
values=df_train.Id
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)

# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
Y = onehot_encoder.fit_transform(integer_encoded)
Y.head()
9/75:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
values=df_train.Id
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)

# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
ohe = onehot_encoder.fit(integer_encoded)
y=ohe.transform
9/76:
# import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
values=trainY
# integer encode
label_encoder = LabelEncoder()
le=label_encoder.fit(trainY)
integer_encoded = le.transform(trainY)
integer_test=le.transform(testY)
# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
ohe = onehot_encoder.fit(integer_encoded)
y_train=ohe.transform(integer_encoded)
y_test = ohe.transform(integer_test)
9/77:
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
values = df_train.Id
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)
print(integer_encoded)
# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
print(onehot_encoded)
# invert first example
inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])
9/78:
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from numpy import argmax
values = df_train.Id
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)
print(integer_encoded)
# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
print(onehot_encoded)
# invert first example
inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])
9/79: inverted
9/80: inverted.head()
9/81: inverted[1]
9/82: inverted[1,:]
9/83: inverted[1,1:2]
9/84: len(inverted)
9/85: dim(inverted)
9/86: inverted
9/87: print(inverted)
9/88: inverted.size()
9/89: inverted.size
9/90:
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from numpy import argmax
values = df_train.Id
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)
print(integer_encoded)
# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
print(onehot_encoded)
# invert first example
inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0:1, :])])
9/91: inverted.size
9/92:
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from numpy import argmax
values = df_train.Id
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)
print(integer_encoded)
# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
print(onehot_encoded)
# invert first example
inverted = label_encoder.inverse_transform([argmax(onehot_encoded[, :])])
9/93:
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from numpy import argmax
values = df_train.Id
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)
print(integer_encoded)
# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
print(onehot_encoded)
# invert first example
inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0:5, :])])
9/94: inverted.size
9/95:
df_train_onehot = df_train.copy()
df_train_onehot = pd.get_dummies(df_train_onehot, columns=['Id'], prefix = ['Id'])
9/96:
df_train_onehot = df_train.copy()
df_train_onehot = pd.get_dummies(df_train_onehot, columns=['Id'], prefix = ['Id'])
df_train_onehot.head()
9/97:
df_train_onehot = df_train.copy()
df_train_onehot = pd.get_dummies(df_train_onehot, columns=['Id'], prefix = ['Id'])
df_train_onehot.shape
9/98:
df_train_onehot = df_train.copy()
df_train_onehot = pd.get_dummies(df_train_onehot, columns=['Id'], prefix = ['Id'])
df_train_onehot.shape
df_train_onehot.head()
9/99: df_train.Id.describe()
9/100:
# Indicates sum of values in our data
train.isnull().sum().sum()
9/101:
# Indicates sum of values in our data
df_train.isnull().sum().sum()
9/102:
df_train.head()
df_train.shape[0]
9/103:
# put labels into y_train variable
y_train = df_train["Id"]
# Drop 'Id' column
X_train = df_train.drop(labels = ["Id"], axis = 1)
9/104:
load_imgfrom sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
9/105:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
9/106:
#df_train_onehot = df_train.copy()
#df_train_onehot = pd.get_dummies(df_train_onehot, columns=['Id'], prefix = ['Id'])
#df_train_onehot.shape
#df_train_onehot.head()
9/107: df_train.Id.describe()
9/108:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
9/109:
import matplotlib.pyplot as plt
from PIL import Image
9/110:
# Indicates number of NAs in our data
df_train.isnull().sum().sum()
9/111:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
9/112: df_train.Id.describe()
9/113:
id_count=df_train.groupby('Id').count().sort_values('Image',ascending = False)
id_count.head(10)
9/114:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
9/115: popular_whale=whale_images('w_23a388d',df_train)
9/116:
for pic in popular_whale[0:5]: 
    p=path+'train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
9/117:
unique_whale_data = df_train.groupby("Id").Image.nunique()
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
9/118: df_train=df_train.sample(frac=0.1,random_state=1)
9/119:
df_train.head()
df_train.shape[0]
9/120:
##df_train.drop(Size,axis=1)
##droping the new_whale images
#df_train=df_train[df_train.Id != 'new_whale']
#df_train.shape
9/121:
id_map = df_train[['Id']].drop_duplicates()
id_map.shape[0]
9/122:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
9/123:
# put labels into y_train variable
y_train = df_train["Id"]
# Drop 'Id' column
X_train = df_train.drop(labels = ["Id"], axis = 1)
9/124:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
9/125:
## Evaluate TEST model class prediction accuracy
#print("[INFO] Evaluating network...")
#predictions = model.predict(testX, batch_size=batch_size)
#print(classification_report(testY.argmax(axis=1),
#                           predictions.argmax(axis=1),
#                           target_names=[str(x) for x in lb.classes_]))
9/126:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu', input_shape = (100,100,3)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
9/127:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
9/128:
# convert to one-hot-encoding(one hot vectors)
# we have 5005 class look at from=> train.Id.describe()

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
9/129:
# convert to one-hot-encoding(one hot vectors)
# we have 5005 class look at from=> train.Id.describe()

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
y_train.shape[1]
9/130:
# put labels into y_train variable
y_train = df_train["Id"]
# Drop 'Id' column
X_train = df_train.drop(labels = ["Id"], axis = 1)
9/131:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
9/132:
# convert to one-hot-encoding(one hot vectors)
# we have 5005 class look at from=> train.Id.describe()

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
y_train.shape[1]
9/133:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
9/134:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
9/135:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
9/136: model.summary()
9/137:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
9/138: model.summary()
9/139:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/140:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/141: model.summary()
9/142:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
9/143:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
9/144:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
9/145:
# convert class vectors to binary class matrices

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
9/146:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
9/147:
# convert class vectors to binary class matrices

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
9/148:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
9/149:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
9/150:
# convert class vectors to binary class matrices

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
9/151:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val=label_encoder.transform(y_val)
9/152:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.fit_transform(y_vel)
9/153:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.fit_transform(y_val)
9/154:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
#y_val = label_encoder.fit_transform(y_val)
9/155:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
9/156:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
9/157:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
#y_val = label_encoder.fit_transform(y_val)
9/158:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
9/159:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
9/160:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.fit_transform(y_val)
9/161:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.fit_transform(y_val)
print(y_train.shape)
9/162:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.fit_transform(y_val)
print(y_train.shape)
print(y_val.shape)
9/163:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
9/164:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
print(y_train.shape)
print(y_val.shape)
9/165:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.fit_transform(y_val)
print(y_train.shape)
print(y_val.shape)
9/166:
# convert class vectors to binary class matrices

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
9/167: y_val = to_categorical(y_val, num_classes = num_classes )
9/168:
batch_size = 128
epochs = 12
9/169:
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
9/170:
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
9/171:
batch_size = 128
epochs = 12
type(x_train)
9/172:
batch_size = 128
epochs = 12
type(x_train.values)
9/173:
model.fit(x_train.values, y_train.values,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
9/174:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
9/175:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
print(y_train.shape)
print(y_val.shape)
9/176:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.fit_transform(y_val)
print(y_train.shape)
print(y_val.shape)
9/177:
# convert class vectors to binary class matrices

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
9/178: y_val = to_categorical(y_val, num_classes = num_classes )
9/179:
## Evaluate TEST model class prediction accuracy
#print("[INFO] Evaluating network...")
#predictions = model.predict(testX, batch_size=batch_size)
#print(classification_report(testY.argmax(axis=1),
#                           predictions.argmax(axis=1),
#                           target_names=[str(x) for x in lb.classes_]))
9/180:
batch_size = 128
epochs = 12
9/181:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/182:
model.fit(x_train.values, y_train.values,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
9/183:
batch_size = 128
epochs = 12
type(x_train)
9/184:
batch_size = 128
epochs = 12
type(x_train)
type(x_train.values)
9/185:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(x_train.values))
9/186:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(x_train.values))
print(type(y_train))
print(type(y_train.values))
9/187:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(x_train.values))
print(type(y_train))
print(type(y_train))
9/188:
model.fit(x_train.values, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
9/189:
model.fit(x_train.values, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val.values, y_val))
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
9/190:
model.fit(x_train.values, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
9/191:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
input_shape=(height,width,channels)
(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu',input_shape = (1,IMG_SIZE, IMG_SIZE)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/192:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
#input_shape=(height,width,channels)
#(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu',input_shape = (1,IMG_SIZE, IMG_SIZE)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/193:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
#input_shape=(height,width,channels)
#(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,1)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/194:
model.fit(x_train.values, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
9/195:
model.fit(x_train.values, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val.values, y_val))
9/196:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(x_train.values))
print(type(y_train))
print(type(y_train))
print(x_vel.shape)
9/197:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(x_train.values))
print(type(y_train))
print(type(y_train))
print(x_val.shape)
9/198:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(x_train.values))
print(type(y_train))
print(type(y_train))
print(y_val.shape)
9/199:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(x_train.values))
print(type(y_train))
print(type(y_train))
print(y_val.shape)
print(y_train.shape)
9/200:
model.fit(x_train.values, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
9/201: np.resize(x_train, (-1, input_shape)
9/202: np.resize(x_train, (-1, input_shape))
9/203: np.resize(x_train, (-1, IMG_SIZE, IMG_SIZE,1))
9/204: np.resize(x_train, (-1, IMG_SIZE, IMG_SIZE))
9/205: np.resize(x_train, (-1, (IMG_SIZE, IMG_SIZE)))
9/206: #np.resize(x_train, (-1, (IMG_SIZE, IMG_SIZE)))
9/207:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
#input_shape=(height,width,channels)
#(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (1902,)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/208:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
#input_shape=(height,width,channels)
#(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/209:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
#input_shape=(height,width,channels)
#(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/210:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
#input_shape=(height,width,channels)
#(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,1)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/211:
model.fit(x_train.values, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val.values, y_val))
9/212:
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
9/213:
X_train=x_train.reshape(-1,IMG_SIZE,IMG_SIZE,1)
model.fit(X_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
9/214:
X_train=x_train.values.reshape(-1,IMG_SIZE,IMG_SIZE,1)
model.fit(X_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
9/215:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
df_train.to_csv(file_name, sep='\t')
9/216:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
df_train.to_csv('partial_df', sep='\t')
9/217:
# convert class vectors to binary class matrices

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
print(y_train.shape)
print(y_val.shape)
9/218:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
9/219:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
print(y_train.shape)
print(y_val.shape)
9/220:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.fit_transform(y_val)
print(y_train.shape)
print(y_val.shape)
9/221:
# convert class vectors to binary class matrices

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
print(y_train.shape)
print(y_val.shape)
9/222:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
df_train.to_csv('partial_df', sep='\t')
9/223: df_train=df_train.sample(frac=0.1,random_state=1)
9/224:
df_train.head()
df_train.shape[0]
9/225:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
9/226:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
print(y_train.shape)
print(y_val.shape)
9/227:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.fit_transform(y_val)
print(y_train.shape)
print(y_val.shape)
9/228:
# convert class vectors to binary class matrices

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = num_classes )
print(y_train.shape)
print(y_val.shape)
9/229:
y_val = to_categorical(y_val, num_classes = num_classes )
print(y_train.shape)
print(y_val.shape)
9/230:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
print(y_train.shape)
print(y_val.shape)
9/231:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.transform(y_val)
print(y_train.shape)
print(y_val.shape)
9/232:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
9/233:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
9/234:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
9/235:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
9/236:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
print(y_train.shape)
print(y_val.shape)
9/237:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
9/238:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
9/239:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
9/240:
#from sklearn.preprocessing import LabelEncoder
#label_encoder = LabelEncoder()
#y_train = label_encoder.fit_transform(y_train)
#y_val = label_encoder.transform(y_val)
#print(y_train.shape)
#print(y_val.shape)
9/241:
# convert class vectors to binary class matrices

#from keras.utils.np_utils import to_categorical
#y_train = to_categorical(y_train, num_classes = num_classes )
#print(y_train.shape)
#print(y_val.shape)
9/242:
#y_val = to_categorical(y_val, num_classes = num_classes )
print(y_train.shape)
print(y_val.shape)
9/243:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(x_train.values))
print(type(y_train))
print(type(y_train))
print(y_val.shape)
print(y_train.shape)
9/244:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
#input_shape=(height,width,channels)
#(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,1)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
9/245: #np.resize(x_train, (-1, (IMG_SIZE, IMG_SIZE)))
9/246:
X_train=x_train.values.reshape(-1,IMG_SIZE,IMG_SIZE,1)
model.fit(X_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
9/247:
#X_train=x_train.values.reshape(-1,IMG_SIZE,IMG_SIZE,1)
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
10/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
10/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
10/3: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
10/4: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
10/5: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
11/1:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv(LABELS)
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
11/2:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
11/3:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv(LABELS)
11/4:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
11/5: id_map = df_train[['Id']].drop_duplicates()
11/6:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv(path+'train.csv')
11/7: id_map = df_train[['Id']].drop_duplicates()
9/248:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
df_train.to_csv('partial_df')
11/8:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv(path+'train.csv')
df_train=df_train.sample(frac=0.1,random_state=1)
11/9: id_map = df_train[['Id']].drop_duplicates()
11/10:
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
11/11:
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train:
    img=train_files+'/'+pic
    train_imgs = train_imgs + img_to_array(load_img(img, target_size=IMG_DIM))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)


validation_imgs=[]
for pic in x_val:
    img=train_files+'/'+pic
    validation_imgs = validation_imgs + img_to_array(load_img(img, target_size=IMG_DIM))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
validation_imgs = np.array(validation_imgs)


#y_train = to_categorical(y_train, num_classes = num_classes )
#train_labels = [fn.split('\\')[1].split('.')[0].strip() for fn in train_files]

#validation_files = glob.glob('validation_data/*')
#validation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in validation_files]
#validation_imgs = np.array(validation_imgs)
#validation_labels = [fn.split('\\')[1].split('.')[0].strip() for fn in validation_files]

#print('Train dataset shape:', train_imgs.shape, 
#      '\tValidation dataset shape:', validation_imgs.shape)
11/12:
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train:
    
    img=train_files+'/'+pic
    print(img)
    #train_imgs = train_imgs + img_to_array(load_img(img, target_size=IMG_DIM))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/13:
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train:
    
    img=train_files+'/'+pic
    original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
    #train_imgs = train_imgs + img_to_array(load_img(img, target_size=IMG_DIM))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/14:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train:
    
    img=train_files+'/'+pic
    original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
    #train_imgs = train_imgs + img_to_array(load_img(img, target_size=IMG_DIM))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/15:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train:
    
    img=train_files+'/'+pic
    original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
    #train_imgs = train_imgs + img_to_array(load_img(img, target_size=IMG_DIM))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)

train_imgs.head()
11/16:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train:
    
    img=train_files+'/'+pic
    original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
    #train_imgs = train_imgs + img_to_array(load_img(img, target_size=IMG_DIM))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)

print(train_imgs)
11/17:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train:
    
    img=train_files+'/'+pic
    original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
    train_imgs = train_imgs + img_to_array(load_img(original_img, target_size=IMG_DIM))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/18:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    return  np.array(normalized_img, dtype="float") / 255.0
11/19:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_DIM, IMG_DIM))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    return  np.array(normalized_img, dtype="float") / 255.0
11/20:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train:
    
    img=train_files+'/'+pic
    #original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
    train_imgs = train_imgs + img_to_array(load_img(img))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/21:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train[1:]:
    
    img=train_files+'/'+pic
    #original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
    train_imgs = train_imgs + img_to_array(load_img(img))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/22:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
11/23:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)

x_train.shape
11/24:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)

x_train[0]
11/25:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)

type(x_train)
11/26:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)

x_train.head()
11/27:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)

x_train.head()
x_train.Image
11/28:
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train.Image:
    img=train_files+'/'+pic
    #original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
    train_imgs = train_imgs + img_to_array(load_img(img))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/29:
#train_files = glob.glob('training_data/*')
train_imgs=[]
for pic in x_train.Image:
    img=train_files+'/'+pic
    #original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
    train_imgs = img_to_array(load_img(img))  
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/30:
#train_files = glob.glob('training_data/*')
#train_imgs=[]
#for pic in x_train.Image:
    #img=train_files+'/'+pic
    #original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
train_imgs = [img_to_array(load_img(train_files+'/'+pic)) for pic in x_train.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/31:
validation_imgs = [img_to_array(load_img(train_files+'/'+pic)) for pic in x_val.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
validation_imgs = np.array(validation_imgs)


#y_train = to_categorical(y_train, num_classes = num_classes )
#train_labels = [fn.split('\\')[1].split('.')[0].strip() for fn in train_files]

#validation_files = glob.glob('validation_data/*')
#validation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in validation_files]
#validation_imgs = np.array(validation_imgs)
#validation_labels = [fn.split('\\')[1].split('.')[0].strip() for fn in validation_files]

#print('Train dataset shape:', train_imgs.shape, 
#      '\tValidation dataset shape:', validation_imgs.shape)
11/32:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv(path+'train.csv')
df_train=df_train.sample(frac=0.1,random_state=1)
11/33: id_map = df_train[['Id']].drop_duplicates()
11/34:
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
11/35:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_DIM, IMG_DIM))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    return  np.array(normalized_img, dtype="float") / 255.0
11/36:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
11/37:
#train_files = glob.glob('training_data/*')
#train_imgs=[]
#for pic in x_train.Image:
    #img=train_files+'/'+pic
    #original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
train_imgs = [img_to_array(load_img(train_files+'/'+pic)) for pic in x_train.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
11/38:
validation_imgs = [img_to_array(load_img(train_files+'/'+pic)) for pic in x_val.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
validation_imgs = np.array(validation_imgs)


#y_train = to_categorical(y_train, num_classes = num_classes )
#train_labels = [fn.split('\\')[1].split('.')[0].strip() for fn in train_files]

#validation_files = glob.glob('validation_data/*')
#validation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in validation_files]
#validation_imgs = np.array(validation_imgs)
#validation_labels = [fn.split('\\')[1].split('.')[0].strip() for fn in validation_files]

#print('Train dataset shape:', train_imgs.shape, 
#      '\tValidation dataset shape:', validation_imgs.shape)
13/1:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv(path+'train.csv')
df_train=df_train.sample(frac=0.1,random_state=1)
13/2: id_map = df_train[['Id']].drop_duplicates()
13/3:
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
13/4:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_DIM, IMG_DIM))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    return  np.array(normalized_img, dtype="float") / 255.0
13/5:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
13/6:
#train_files = glob.glob('training_data/*')
#train_imgs=[]
#for pic in x_train.Image:
    #img=train_files+'/'+pic
    #original_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)
train_imgs = [img_to_array(load_img(train_files+'/'+pic)) for pic in x_train.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
train_imgs = np.array(train_imgs)
12/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
12/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
12/3: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
12/4: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
12/5: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project/convert Data.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/Shahla project')
13/7: y_train.head()
13/8: type(y_train)
13/9:
type(y_train)
y_train.shape
13/10:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv(path+'train.csv')
df_train=df_train.sample(frac=0.1,random_state=1)
13/11: id_map = df_train[['Id']].drop_duplicates()
13/12:
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
13/13:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_DIM, IMG_DIM))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    return  np.array(normalized_img, dtype="float") / 255.0
13/14:
imgs = [img_to_array(load_img(train_files+'/'+pic)) for pic in X.Image ]
imgs = np.array(imgs)
13/15:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(imgs, y, test_size=0.25,shuffle=True,random_state=1)
13/16: x_train.shape
13/17:
print(x_train.shape)
print(y_train.shape)
13/18:
print(x_train.shape)
print(y_train.shape)
print(x_val.shape)
print(y_val.shape)
14/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
14/2:
import matplotlib.pyplot as plt
from PIL import Image
14/3:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
14/4:
# Indicates number of NAs in our data
df_train.isnull().sum().sum()
14/5: df_train.Id.describe()
14/6:
id_count=df_train.groupby('Id').count().sort_values('Image',ascending = False)
id_count.head(10)
14/7:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
14/8: popular_whale=whale_images('w_23a388d',df_train)
14/9:
unique_whale_data = df_train.groupby("Id").Image.nunique()
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
14/10: id_map = df_train[['Id']].drop_duplicates()
14/11: print("Number of distinct Ids:",id_map.shape[0])
14/12: df_train=df_train.sample(frac=0.1,random_state=1)
14/13:
df_train.head()
df_train.shape[0]
14/14:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return  np.array(normalized_img, dtype="float") / 255.0
#for the time being I removed the other return objects: saliencyMap, np.array(polar_img, dtype="float") / 255.0,
14/15:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    #polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    #saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
   # (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return  np.array(normalized_img, dtype="float") / 255.0
#for the time being I removed the other return objects: saliencyMap, np.array(polar_img, dtype="float") / 255.0,
14/16:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
14/17:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
14/18:
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
14/19:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
14/20:
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
14/21:
imgs = [img_to_array(load_img(train_files+'/'+pic)) for pic in X.Image ]
imgs = np.array(imgs)
14/22:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
imgs = [img_to_array(load_img(train_files+'/'+pic)) for pic in X.Image ]
imgs = np.array(imgs)
14/23:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
imgs = [img_to_array(load_img(TRAIN+pic)) for pic in X.Image ]
imgs = np.array(imgs)
15/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
15/2:
import matplotlib.pyplot as plt
from PIL import Image
15/3:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
15/4: df_train=df_train.sample(frac=0.1,random_state=1)
15/5:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    #polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    #saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
   # (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return  np.array(normalized_img, dtype="float") / 255.0
#for the time being I removed the other return objects: saliencyMap, np.array(polar_img, dtype="float") / 255.0,
15/6:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
15/7:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
15/8:
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
15/9:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
imgs = [img_to_array(load_img(TRAIN+pic)) for pic in X.Image ]
imgs = np.array(imgs)
15/10:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
15/11:
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
15/12:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
imgs = [img_to_array(load_img(TRAIN+pic)) for pic in X.Image ]
imgs = np.array(imgs)
15/13:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(imgs, y, test_size=0.25,shuffle=True,random_state=1)
15/14:
#y_val = to_categorical(y_val, num_classes = num_classes )
print(y_train.shape)
print(y_val.shape)
15/15:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(x_train.values))
print(type(y_train))
print(type(y_train))
print(y_val.shape)
print(y_train.shape)
15/16:
batch_size = 128
epochs = 12
print(type(x_train))
print(type(y_train))
print(type(y_train))
print(y_val.shape)
print(y_train.shape)
15/17:
#https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
#input_shape=(height,width,channels)
#(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,1)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
15/18: #np.resize(x_train, (-1, (IMG_SIZE, IMG_SIZE)))
15/19:
#X_train=x_train.values.reshape(-1,IMG_SIZE,IMG_SIZE,1)
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_val, y_val))
15/20:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
imgs = [img_to_array(load_img(TRAIN+pic)) for pic in X.Image ]
imgs = np.array(imgs)
imgs.shape
17/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
17/2:
import matplotlib.pyplot as plt
from PIL import Image
17/3:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
17/4: id_map = df_train[['Id']].drop_duplicates()
17/5: print("Number of distinct Ids:",id_map.shape[0])
17/6: df_train=df_train.sample(frac=0.1,random_state=1)
17/7:
df_train.head()
df_train.shape[0]
17/8:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
17/9:
import matplotlib.pyplot as plt
from PIL import Image
17/10:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
17/11: df_train=df_train.sample(frac=0.1,random_state=1)
17/12:
df_train.head()
df_train.shape[0]
17/13:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
17/14:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
17/15:
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
17/16:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
imgs = [img_to_array(load_img(TRAIN+pic)) for pic in X.Image ]
imgs = np.array(imgs)
17/17: print(imgs.shape)
17/18:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
imgs = [img_to_array(load_img(TRAIN+pic,target_size=IMG_SIZE)) for pic in X.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
imgs = np.array(imgs)
17/19:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
IMG_DIM = (150, 150)

imgs = [img_to_array(load_img(TRAIN+pic,target_size=IMG_DIM)) for pic in X.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
imgs = np.array(imgs)
17/20: print(imgs.shape)
17/21:
imgs_scaled = imgs.astype('float32')
imgs_scaled /= 255
17/22:
print(imgs[0].shape)
array_to_img(imgs[0])
17/23:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(imgs, y, test_size=0.25,shuffle=True,random_state=1)
17/24:
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
17/25:
from keras.models import Sequential # to create a cnn model
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
17/26:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
17/27:
batch_size = 30
epochs = 30
input_shape = (150, 150, 3)
17/28:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
17/29:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
17/30:
#y_val = to_categorical(y_val, num_classes = num_classes )
print(y_train.shape)
print(y_val.shape)
17/31:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
17/32:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
18/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
18/2:
import matplotlib.pyplot as plt
from PIL import Image
18/3:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
18/4: df_train=df_train.sample(frac=0.1,random_state=1)
18/5:
#for pic in df_train['Image']:
        #p=path+'train/'+pic
        #if  os.path.isdir(p)==True:
        #with Image.open(p) as img:
         #       width, height = img.size
          #      print(img.shape)
#df_train['Size']=df_train['Image'].apply(lambda x: Image.open(path+'train/'+x).size)
18/6:
df_train.head()
df_train.shape[0]
18/7:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
18/8:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
18/9:
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
18/10:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
IMG_DIM = (150, 150)

imgs = [img_to_array(load_img(TRAIN+pic,target_size=IMG_DIM)) for pic in X.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
imgs = np.array(imgs)
18/11: print(imgs.shape)
18/12:
imgs_scaled = imgs.astype('float32')
imgs_scaled /= 255
18/13:
print(imgs[0].shape)
array_to_img(imgs[0])
18/14:
batch_size = 30
epochs = 30
input_shape = (150, 150, 3)
18/15:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(imgs, y, test_size=0.25,shuffle=True,random_state=1)
18/16:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
18/17:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
18/18:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
18/19:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
18/20:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
18/21:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
18/22:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
18/23:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
18/24:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
18/25: print(y_train.shape)
18/26:
print(y_train.shape)
print(x_train.shape)
18/27:
print(y_train.shape)
print(x_train.shape)
print(num_classes)
18/28:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(units=num_classes,kernel_initializer=uniform,activation=sigmoid))
#model.add(Dense(1, activation='sigmoid'))
#classifier.add(Dense(units=8,kernel_initializer=uniform,activation=sigmoid))

model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
18/29:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(units=num_classes,activation=sigmoid))
#model.add(Dense(1, activation='sigmoid'))
#classifier.add(Dense(units=8,kernel_initializer=uniform,activation=sigmoid))

model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
18/30:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(num_classes,activation=sigmoid))
#model.add(Dense(1, activation='sigmoid'))
#classifier.add(Dense(units=8,kernel_initializer=uniform,activation=sigmoid))

model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
18/31:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1208,activation=sigmoid))
#model.add(Dense(1, activation='sigmoid'))
#classifier.add(Dense(units=8,kernel_initializer=uniform,activation=sigmoid))

model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
18/32:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(1208, activation='sigmoid'))
#classifier.add(Dense(units=8,kernel_initializer=uniform,activation=sigmoid))

model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
18/33:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
18/34:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
19/1:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
19/2:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
19/3:
import matplotlib.pyplot as plt
from PIL import Image
19/4:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
19/5: df_train=df_train.sample(frac=0.1,random_state=1)
19/6:
#for pic in df_train['Image']:
        #p=path+'train/'+pic
        #if  os.path.isdir(p)==True:
        #with Image.open(p) as img:
         #       width, height = img.size
          #      print(img.shape)
#df_train['Size']=df_train['Image'].apply(lambda x: Image.open(path+'train/'+x).size)
19/7:
df_train.head()
df_train.shape[0]
19/8:
whale_images('w_23a388d',df_train)
for pic in popular_whale[0:5]: 
    p=path+'train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
19/9:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
19/10:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
19/11:
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
19/12:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
IMG_DIM = (150, 150)

imgs = [img_to_array(load_img(TRAIN+pic,target_size=IMG_DIM)) for pic in X.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
imgs = np.array(imgs)
19/13: print(imgs.shape)
19/14:
imgs_scaled = imgs.astype('float32')
imgs_scaled /= 255
19/15:
print(imgs[0].shape)
array_to_img(imgs[0])
19/16:
batch_size = 30
epochs = 30
input_shape = (150, 150, 3)
19/17:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(imgs, y, test_size=0.25,shuffle=True,random_state=1)
19/18:
print(y_train.shape)
print(x_train.shape)
print(num_classes)
19/19:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))
#classifier.add(Dense(units=8,kernel_initializer=uniform,activation=sigmoid))

model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
19/20:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
19/21:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
20/1:
#from sklearn.preprocessing import LabelEncoder
#label_encoder = LabelEncoder()
#y_train = label_encoder.fit_transform(y_train)
#y_val = label_encoder.transform(y_val)
#print(y_train.shape)
#print(y_val.shape)
20/2: model.save('basic_cnn.h5')
20/3:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
20/4:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))
#classifier.add(Dense(units=8,kernel_initializer=uniform,activation=sigmoid))

model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
20/5:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
20/6:
import matplotlib.pyplot as plt
from PIL import Image
20/7:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
20/8: df_train=df_train.sample(frac=0.1,random_state=1)
20/9:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
20/10:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
20/11:
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
20/12:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
IMG_DIM = (150, 150)

imgs = [img_to_array(load_img(TRAIN+pic,target_size=IMG_DIM)) for pic in X.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
imgs = np.array(imgs)
20/13: print(imgs.shape)
20/14:
imgs_scaled = imgs.astype('float32')
imgs_scaled /= 255
20/15:
print(imgs[0].shape)
array_to_img(imgs[0])
20/16:
batch_size = 30
epochs = 30
input_shape = (150, 150, 3)
20/17:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(imgs, y, test_size=0.25,shuffle=True,random_state=1)
20/18:
print(y_train.shape)
print(x_train.shape)
print(num_classes)
20/19:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))
#classifier.add(Dense(units=8,kernel_initializer=uniform,activation=sigmoid))

model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
20/20:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
20/21:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
21/1:
batch_size = 10
epochs = 30
input_shape = (150, 150, 3)
21/2:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(imgs, y, test_size=0.25,shuffle=True,random_state=1)
22/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
22/2:
import matplotlib.pyplot as plt
from PIL import Image
22/3:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
22/4: df_train=df_train.sample(frac=0.1,random_state=1)
22/5:
id_map = df_train[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
22/6:
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)
22/7:
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
22/8:
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
IMG_DIM = (150, 150)

imgs = [img_to_array(load_img(TRAIN+pic,target_size=IMG_DIM)) for pic in X.Image ]
#train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]
imgs = np.array(imgs)
22/9: print(imgs.shape)
22/10:
imgs_scaled = imgs.astype('float32')
imgs_scaled /= 255
22/11:
print(imgs[0].shape)
array_to_img(imgs[0])
22/12:
batch_size = 3#30
epochs = 3 #30
input_shape = (150, 150, 3)
22/13:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(imgs, y, test_size=0.25,shuffle=True,random_state=1)
22/14:
print(y_train.shape)
print(x_train.shape)
print(num_classes)
22/15:
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', 
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))
#classifier.add(Dense(units=8,kernel_initializer=uniform,activation=sigmoid))

model.compile(loss='categorical_crossentropy',
    #loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(),
              metrics=['accuracy'])

model.summary()
22/16:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
22/17:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
24/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/3: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/4: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/5: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/6: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/7: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/8: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/9: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/10: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/11: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
24/12: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
26/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
26/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project')
29/1:
import pandas as pd
import numpy as np
29/2: np.Series(np.random.randn(5),index=['a','b','c','d','e'])
29/3: pd.Series(np.random.randn(5),index=['a','b','c','d','e'])
29/4: s=pd.Series(np.random.randn(5),index=['a','b','c','d','e'])
29/5: s[[0]]
29/6: s[0]
29/7: s[[0,1,2]]
29/8: s[0,1,2]
29/9: np.exp(s)
29/10: s.dtype
29/11: s.shape
29/12: s.array
29/13:
x=s.array
x.dtype
29/14: x['a']
29/15: s['a']
29/16: s['a','b']
29/17: s[['a','b']
29/18: s[['a','b']]
29/19: 'a' in s
29/20: s.get('a')
29/21: s.get('e')
29/22: s.get('f')
29/23: s.get('f')
29/24: s.get('f',np.nan)
29/25: s.get('a','b')
29/26: s.get('a','b')
29/27: s.get('a')
29/28: s.get('b')
29/29: s.get('a')
29/30: s[:3]
29/31: s[-1]
29/32: s
29/33: s[-1]
29/34: s[-5]
29/35: s[:3]
29/36: s[3:]
29/37: s[1]
29/38: s[1:]
29/39: s[:-1]
29/40: s[1:]
29/41: s[1:]*s[:-1]
29/42: n=pd.Series(np.random.randn(5),name='5 standard normal distribution')
29/43: N
29/44: n
29/45: n.rename('normal distribution')
29/46: n
29/47: n1=n.rename('normal distribution')
29/48: n1
29/49: df=pd.DataFrame('one',pd.Series([1,2,3],index=['a','b','c']),'two':pd.Series([3,4,4],index=[a,b,c]))
29/50: df=pd.DataFrame('one':pd.Series([1,2,3],index=['a','b','c']),'two':pd.Series([3,4,4],index=[a,b,c]))
29/51: df=pd.DataFrame('one': pd.Series([1,2,3],index=['a','b','c']),'two':pd.Series([3,4,4],index=[a,b,c]))
29/52: df=pd.DataFrame({'one': pd.Series([1,2,3],index=['a','b','c']),'two':pd.Series([3,4,4],index=[a,b,c])})
29/53: d={'one': pd.Series([1,2,3],index=['a','b','c']),'two':pd.Series([3,4,4],index=['a','b','c'])}
29/54: d.dtype
29/55: type(d)
29/56: pd.DataFrame(d)
29/57: df['a']
29/58: df=pd.DataFrame(d)
29/59: df['a']
29/60: df['a']
29/61:
df=pd.DataFrame(d)
df
29/62: df[['a']]
29/63: df['one']
29/64:
x=df['one']
type(x)
29/65:
x=df['one']
x.dtype
29/66: df[0]
29/67: df['three']=df['one']*df['two']
29/68: df
29/69: del df['one']
29/70: df
29/71: df.pop['three']
29/72: df['two']
29/73: df[1]
29/74: df.loc['a']
29/75: df.iloc[0]
29/76: df[1:2]
29/77: df[1:3]
29/78: df[1]
29/79: df[0]
29/80: df[0:2]
29/81: df[0:3]
29/82: list(ABC)
29/83: list('ABC')
29/84: list('I am a strudeint')
29/85: pd
30/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
30/2:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
30/3:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
#print(os.listdir(path))

# Any results you write to the current directory are saved as output
32/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
#print(os.listdir(path))

# Any results you write to the current directory are saved as output
32/2:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
32/3:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
#print(os.listdir(path))

# Any results you write to the current directory are saved as output
32/4:
import matplotlib.pyplot as plt
from PIL import Image
32/5:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv(path+'train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
32/6:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
TRAIN = path+'train/'
TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(TRAIN)
import os
#print(os.listdir(path))

# Any results you write to the current directory are saved as output
32/7: pwd
32/8:
print("The tranining data has:",len(os.listdir(path+'train')),"images")
print("The test data has:",len(os.listdir(path+'test')),"images")
df_train = pd.read_csv('train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
32/9:
import os
print(os.getcwd() + "\n")
32/10: df_train = pd.read_csv('train.csv')
32/11:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
#path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
#TRAIN = path+'train/'
#TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir(pwd)
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
32/12:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
#path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
#TRAIN = path+'train/'
#TEST = path+'test/'
LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir()
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
32/13:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
#path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
#TRAIN = path+'train/'
#TEST = path+'test/'
#LABELS = path+'/train.csv'
IMG_SIZE = 128
file_names = os.listdir()
import os
print(os.listdir(path))

# Any results you write to the current directory are saved as output
32/14:
print("The tranining data has:",len(os.listdir('train')),"images")
print("The test data has:",len(os.listdir('test')),"images")
df_train = pd.read_csv('train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
32/15:
print("The tranining data has:",len(os.listdir('train')),"images")
print("The test data has:",len(os.listdir('test')),"images")
df_train = pd.read_csv('train.csv')
df_original=df_train
print(df_train.shape)
df_train.head(3)
32/16:
id_count=df_train.groupby('Id').count().sort_values('Image',ascending = False)
id_count.head(10)
32/17:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
id_count1.head(10)
32/18:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
id_count1<10
32/19:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
sum(id_count1<3)
32/20:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
type(id_count1)
32/21:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
id_count1.columns
32/22:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
id_count1.Imgage.count(True)
32/23:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
id_count1.Image.count(True)
32/24:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1)
32/25:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<3)
32/26:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<1)
32/27:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
32/28:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(1<id_count1<3)
32/29:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(id_count1<3 && id_count1>1)
32/30:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(id_count1<3 & id_count1>1)
32/31:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(id_count1<3 and id_count1>1)
32/32:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
32/33:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum((id_count1<3) & (id_count1>1)
32/34:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum((id_count1<3) & (id_count1>1))
32/35:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(id_count1=2)
np.sum((id_count1<3) & (id_count1>1))
32/36:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(id_count1==2)
np.sum((id_count1<3) & (id_count1>1))
32/37:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(id_count1==2)
#np.sum((id_count1<3) & (id_count1>1))
32/38:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(id_count1==2)
np.sum(id_count1==3)
#np.sum((id_count1<3) & (id_count1>1))
32/39:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(id_count1==2)
np.sum(id_count1<3)
#np.sum((id_count1<3) & (id_count1>1))
32/40:
id_count1=df_train.groupby('Id').count().sort_values('Image',ascending = True)
np.sum(id_count1<2)
np.sum(id_count1==2)
np.sum(id_count1<4)
#np.sum((id_count1<3) & (id_count1>1))
32/41:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
32/42: popular_whale=whale_images('w_23a388d',df_train)
32/43:
for pic in popular_whale[0:5]: 
    p=path+'train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
32/44:
for pic in popular_whale[0:5]: 
    p='train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
32/45: df_train.groupby('Id').head()
32/46:
unique_whale_data = df_train.groupby("Id").Image.nunique()
pd.value_counts(unique_whale_data).plot.bar(x='Pictures', y='Whales')
32/47:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar().savefig()
32/48:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar().save()
32/49:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar().save('fig.png')
32/50:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar().savefig('fig.png')
32/51:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar().figure.savefig('fig.png')
32/52:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar(x='Id').figure.savefig('fig.png')
32/53:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar(x='Id').figure.savefig('fig.png')
32/54:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar(x='Id').figure(figsize=(3.841, 7.195), dpi=100).savefig('fig.png')
32/55:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar(x='Id').figure().savefig('fig.png')
32/56:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar(x='Id').figure.savefig('fig.png')
32/57:
sorted_unique = unique_whale_data.sort_values(ascending=True)
sorted_unique.iloc[1:30].plot.bar(x='Id').figure.savefig('fig.png')
32/58:
sorted_unique = unique_whale_data.sort_values(ascending=False)
sorted_unique.iloc[1:30].plot.bar(x='Id').figure.savefig('fig.png')
32/59:
sorted_unique = unique_whale_data.sort_values(ascending=True)
sorted_unique.iloc[1:30].plot.bar(x='Id')
32/60:
color = ('b','g','r')
id_count.plot.hist(range = (0,20),color=color)
#id_count.plot.bar()
32/61:
color = ('b','g','r')
id_count.plot.hist(range = (0,20),color='g')
#id_count.plot.bar()
32/62:
color = ('b','g','r')
id_count.plot.hist(range = (0,20),color='r')
#id_count.plot.bar()
32/63:
color = ('b','g','r')
id_count.plot.hist(range = (0,20),color='b')
#id_count.plot.bar()
32/64:
color = ('b','g','r')
id_count.plot.hist(range = (0,20),color='g')
#id_count.plot.bar()
32/65:
color = ('b','g','r')
id_count.plot.hist(range = (0,10),color='g')
#id_count.plot.bar()
32/66:
color = ('b','g','r')
id_count.plot.hist(range = (0,3),color='g')
#id_count.plot.bar()
32/67:
color = ('b','g','r')
id_count.plot.hist(range = (0,20),color='g')
#id_count.plot.bar()
32/68:
def transform_image_ang(img,ang_range):
    # Rotation

    ang_rot = np.random.uniform(ang_range)-ang_range/2
    rows,cols,ch = img.shape    
    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)

    img = cv2.warpAffine(img,Rot_M,(cols,rows))
    
    return img

def transform_image_shear(img,shear_range):
  
    rows,cols,ch = img.shape
    # Shear
    pts1 = np.float32([[5,5],[20,5],[5,20]])

    pt1 = 5+shear_range*np.random.uniform()-shear_range/2
    pt2 = 20+shear_range*np.random.uniform()-shear_range/2

    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])

    shear_M = cv2.getAffineTransform(pts1,pts2)

    img = cv2.warpAffine(img,shear_M,(cols,rows))
    
    return img
  

  
def transform_image_trans(img,trans_range):
    rows,cols,ch = img.shape
    # Translation
    tr_x = trans_range*np.random.uniform()-trans_range/2
    tr_y = trans_range*np.random.uniform()-trans_range/2
    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])


    img = cv2.warpAffine(img,Trans_M,(cols,rows))
    
    return img

  
#def load_image(fn):
 #   original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
  #  scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
   # normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    #polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    #saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
   # (success, saliencyMap) = saliency.computeSaliency(normalized_img)
   # return  np.array(normalized_img, dtype="float") / 255.0
#for the time being I removed the other return objects: saliencyMap, np.array(polar_img, dtype="float") / 255.0,
32/69:
def transform_image_ang(img,ang_range):
    # Rotation

    ang_rot = np.random.uniform(ang_range)-ang_range/2
    rows,cols,ch = img.shape    
    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)

    img = cv2.warpAffine(img,Rot_M,(cols,rows))
    
    return img

def transform_image_shear(img,shear_range):
  
    rows,cols,ch = img.shape
    # Shear
    pts1 = np.float32([[5,5],[20,5],[5,20]])

    pt1 = 5+shear_range*np.random.uniform()-shear_range/2
    pt2 = 20+shear_range*np.random.uniform()-shear_range/2

    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])

    shear_M = cv2.getAffineTransform(pts1,pts2)

    img = cv2.warpAffine(img,shear_M,(cols,rows))
    
    return img
  

  
def transform_image_trans(img,trans_range):
    rows,cols,ch = img.shape
    # Translation
    tr_x = trans_range*np.random.uniform()-trans_range/2
    tr_y = trans_range*np.random.uniform()-trans_range/2
    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])


    img = cv2.warpAffine(img,Trans_M,(cols,rows))
    
    return img

  
#def load_image(fn):
 #   original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
  #  scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
   # normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    #polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    #saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
   # (success, saliencyMap) = saliency.computeSaliency(normalized_img)
   # return  np.array(normalized_img, dtype="float") / 255.0
#for the time being I removed the other return objects: saliencyMap, np.array(polar_img, dtype="float") / 255.0,
32/70:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    img_angle = transform_image_ang(img_normalized, 35)
      img_shear = transform_image_shear(img_normalized, 10)
      img_translate = transform_image_trans(img_normalized, 7)

      salient_angle = transform_image_ang(img_salient, 35)
      salient_shear = transform_image_shear(img_salient, 10)
      salient_translate = transform_image_trans(img_salient, 7)

      polar_angle = transform_image_ang(img_polar, 35)
      polar_shear = transform_image_shear(img_polar, 10)
      polar_translate = transform_image_trans(img_polar, 7)
    #image.convertTo(img_polar, CV_8UC3, 255.0); 
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255);
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    #plt.imshow(img_salient, cmap='gray')
    plt.imshow(   polar_translate, cmap='gray')
    polar_translate
    plt.axis("off")
    plt.show()
32/71:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    img_angle = transform_image_ang(img_normalized, 35)
    img_shear = transform_image_shear(img_normalized, 10)
    img_translate = transform_image_trans(img_normalized, 7)

    salient_angle = transform_image_ang(img_salient, 35)
    salient_shear = transform_image_shear(img_salient, 10)
    salient_translate = transform_image_trans(img_salient, 7)

    polar_angle = transform_image_ang(img_polar, 35)
    polar_shear = transform_image_shear(img_polar, 10)
    polar_translate = transform_image_trans(img_polar, 7)
    #image.convertTo(img_polar, CV_8UC3, 255.0); 
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255);
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    #plt.imshow(img_salient, cmap='gray')
    plt.imshow(polar_translate, cmap='gray')
    polar_translate
    plt.axis("off")
    plt.show()
32/72:
def transform_image_ang(img,ang_range):
    # Rotation

    ang_rot = np.random.uniform(ang_range)-ang_range/2
    rows,cols,ch = img.shape    
    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)

    img = cv2.warpAffine(img,Rot_M,(cols,rows))
    
    return img

def transform_image_shear(img,shear_range):
  
    rows,cols,ch = img.shape
    # Shear
    pts1 = np.float32([[5,5],[20,5],[5,20]])

    pt1 = 5+shear_range*np.random.uniform()-shear_range/2
    pt2 = 20+shear_range*np.random.uniform()-shear_range/2

    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])

    shear_M = cv2.getAffineTransform(pts1,pts2)

    img = cv2.warpAffine(img,shear_M,(cols,rows))
    
    return img
  

  
def transform_image_trans(img,trans_range):
    rows,cols,ch = img.shape
    # Translation
    tr_x = trans_range*np.random.uniform()-trans_range/2
    tr_y = trans_range*np.random.uniform()-trans_range/2
    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])


    img = cv2.warpAffine(img,Trans_M,(cols,rows))
    
    return img

  
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return  np.array(normalized_img, dtype="float") / 255.0
#for the time being I removed the other return objects: saliencyMap, np.array(polar_img, dtype="float") / 255.0,
32/73:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    img_angle = transform_image_ang(img_normalized, 35)
    img_shear = transform_image_shear(img_normalized, 10)
    img_translate = transform_image_trans(img_normalized, 7)

    salient_angle = transform_image_ang(img_salient, 35)
    salient_shear = transform_image_shear(img_salient, 10)
    salient_translate = transform_image_trans(img_salient, 7)

    polar_angle = transform_image_ang(img_polar, 35)
    polar_shear = transform_image_shear(img_polar, 10)
    polar_translate = transform_image_trans(img_polar, 7)
    #image.convertTo(img_polar, CV_8UC3, 255.0); 
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255);
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    #plt.imshow(img_salient, cmap='gray')
    plt.imshow(polar_translate, cmap='gray')
    polar_translate
    plt.axis("off")
    plt.show()
32/74:

for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
    img_angle = transform_image_ang(img_normalized, 35)
    img_shear = transform_image_shear(img_normalized, 10)
    img_translate = transform_image_trans(img_normalized, 7)

    salient_angle = transform_image_ang(img_salient, 35)
    salient_shear = transform_image_shear(img_salient, 10)
    salient_translate = transform_image_trans(img_salient, 7)

    polar_angle = transform_image_ang(img_polar, 35)
    polar_shear = transform_image_shear(img_polar, 10)
    polar_translate = transform_image_trans(img_polar, 7)
    #image.convertTo(img_polar, CV_8UC3, 255.0); 
    cv2.imwrite( path+'polar_train/polar_'+file_names[i]+'.jpg', img_polar*255);
    #img_polar.save(path+'polar_train/polar_'+file_names[i]+'.jpg', 'JPEG' )

    #imgplot = plt.imshow(img_salient, cmap='gray')
    #plt.show()

    #print(img_salient.shape)

    rcParams['figure.figsize'] = 14, 8
    plt.gray()
    fig = plt.figure()
    ax = fig.add_subplot(3, 3, 1)
    ax.set_title('Original')
    plt.imshow(img_normalized, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 2)
    ax.set_title('LogPolar')
    plt.imshow(img_polar, cmap='gray')
    plt.axis("off")
    ax = fig.add_subplot(3, 3, 3)
    ax.set_title('Salient')
    plt.imshow(img_salient, cmap='gray')
    #plt.imshow(polar_translate, cmap='gray')
    polar_translate
    plt.axis("off")
    plt.show()
32/75:
for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
32/76:
def transform_image_ang(img,ang_range):
    # Rotation

    ang_rot = np.random.uniform(ang_range)-ang_range/2
    rows,cols,ch = img.shape    
    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)

    img = cv2.warpAffine(img,Rot_M,(cols,rows))
    
    return img

def transform_image_shear(img,shear_range):
  
    rows,cols,ch = img.shape
    # Shear
    pts1 = np.float32([[5,5],[20,5],[5,20]])

    pt1 = 5+shear_range*np.random.uniform()-shear_range/2
    pt2 = 20+shear_range*np.random.uniform()-shear_range/2

    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])

    shear_M = cv2.getAffineTransform(pts1,pts2)

    img = cv2.warpAffine(img,shear_M,(cols,rows))
    
    return img
  

  
def transform_image_trans(img,trans_range):
    rows,cols,ch = img.shape
    # Translation
    tr_x = trans_range*np.random.uniform()-trans_range/2
    tr_y = trans_range*np.random.uniform()-trans_range/2
    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])


    img = cv2.warpAffine(img,Trans_M,(cols,rows))
    
    return img

IMG_SIZE = 128  
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return  np.array(normalized_img, dtype="float") / 255.0
#for the time being I removed the other return objects: saliencyMap, np.array(polar_img, dtype="float") / 255.0,
32/77:
for i in range(0,5):
    image_path = TRAIN + file_names[i]

    img_salient, img_polar, img_normalized = load_image(image_path)
34/1:
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
from pylab import rcParams
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import os
IMG_SIZE = 128
import matplotlib.pyplot as plt
from PIL import Image
34/2: df_train = pd.read_csv('train.csv')
34/3: df_original=df_train
34/4:
def transform_image_ang(img,ang_range):
    # Rotation

    ang_rot = np.random.uniform(ang_range)-ang_range/2
    rows,cols,ch = img.shape    
    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)

    img = cv2.warpAffine(img,Rot_M,(cols,rows))
    
    return img

def transform_image_shear(img,shear_range):
  
    rows,cols,ch = img.shape
    # Shear
    pts1 = np.float32([[5,5],[20,5],[5,20]])

    pt1 = 5+shear_range*np.random.uniform()-shear_range/2
    pt2 = 20+shear_range*np.random.uniform()-shear_range/2

    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])

    shear_M = cv2.getAffineTransform(pts1,pts2)

    img = cv2.warpAffine(img,shear_M,(cols,rows))
    
    return img
  

  
def transform_image_trans(img,trans_range):
    rows,cols,ch = img.shape
    # Translation
    tr_x = trans_range*np.random.uniform()-trans_range/2
    tr_y = trans_range*np.random.uniform()-trans_range/2
    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])


    img = cv2.warpAffine(img,Trans_M,(cols,rows))
    
    return img

IMG_SIZE = 128  
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    return  np.array(normalized_img, dtype="float") / 255.0
34/5:
def transform_image_ang(img,ang_range):
    # Rotation

    ang_rot = np.random.uniform(ang_range)-ang_range/2
    rows,cols,ch = img.shape    
    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)

    img = cv2.warpAffine(img,Rot_M,(cols,rows))
    
    return img

def transform_image_shear(img,shear_range):
  
    rows,cols,ch = img.shape
    # Shear
    pts1 = np.float32([[5,5],[20,5],[5,20]])

    pt1 = 5+shear_range*np.random.uniform()-shear_range/2
    pt2 = 20+shear_range*np.random.uniform()-shear_range/2

    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])

    shear_M = cv2.getAffineTransform(pts1,pts2)

    img = cv2.warpAffine(img,shear_M,(cols,rows))
    
    return img
  

  
def transform_image_trans(img,trans_range):
    rows,cols,ch = img.shape
    # Translation
    tr_x = trans_range*np.random.uniform()-trans_range/2
    tr_y = trans_range*np.random.uniform()-trans_range/2
    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])


    img = cv2.warpAffine(img,Trans_M,(cols,rows))
    
    return img

IMG_SIZE = 128  
def load_image(fn, action):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    
    scaled_original = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    
    polar_img = cv2.logPolar(normalized_img, (normalized_img.shape[0]/2, normalized_img.shape[1]/2), 25, cv2.WARP_FILL_OUTLIERS)
    
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    (success, saliencyMap) = saliency.computeSaliency(normalized_img)
    
    if action == 'original':
      return np.array(normalized_img, dtype="float") / 255.0
    elif action == 'salient':
      return saliencyMap
    elif action == 'polar':
      return np.array(polar_img, dtype="float") / 255.0
    
    
    return saliencyMap, np.array(polar_img, dtype="float") / 255.0, np.array(normalized_img, dtype="float") / 255.0
34/6:
  img_salient, img_polar, img_normalized = load_image(filename, 'all')
  
  x_normalized = image.img_to_array(img_normalized)
  x_salient = image.img_to_array(img_salient)
  x_polar = image.img_to_array(img_polar)
  
  
  X_train[index1] = x_normalized
  X_train_logpolar[index1] = x_polar
  X_train_salient[index1] = x_salient
34/7:
def whale_images(whale_id,df):
    return df[df['Id']==whale_id]['Image']
34/8: popular_whale=whale_images('w_23a388d',df_train)
34/9:
  for pic in popular_whale[0:5]: 
    p='train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
  img_salient, img_polar, img_normalized = load_image(p, 'all')
  
  x_normalized = image.img_to_array(img_normalized)
  x_salient = image.img_to_array(img_salient)
  x_polar = image.img_to_array(img_polar)
  
  
  X_train[index1] = x_normalized
  X_train_logpolar[index1] = x_polar
  X_train_salient[index1] = x_salient
34/10:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
for pic in popular_whale[0:5]: 
    p='train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
  img_salient, img_polar, img_normalized = load_image(p, 'all')
  
  x_normalized = image.img_to_array(img_normalized)
  x_salient = image.img_to_array(img_salient)
  x_polar = image.img_to_array(img_polar)
  
  
  X_train[index1] = x_normalized
  X_train_logpolar[index1] = x_polar
  X_train_salient[index1] = x_salient
34/12:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
for pic in popular_whale[0:5]: 
    p='train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
    img_salient, img_polar, img_normalized = load_image(p, 'all')
  
    x_normalized = image.img_to_array(img_normalized)
    x_salient = image.img_to_array(img_salient)
    x_polar = image.img_to_array(img_polar)
34/13:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
for pic in popular_whale[0:5]: 
    p='train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
    img_salient, img_polar, img_normalized = load_image(p, 'all')
  
    x_normalized = image.img_to_array(img_normalized)
    ax.imshow(x_normalized)
    x_salient = image.img_to_array(img_salient)
    x_polar = image.img_to_array(img_polar)
34/14:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
for pic in popular_whale[0:5]: 
    p='train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
    img_salient, img_polar, img_normalized = load_image(p, 'all')
  
    x_normalized = image.img_to_array(img_normalized)
    x_salient = image.img_to_array(img_salient)
    x_polar = image.img_to_array(img_polar)
    
    img_angle = transform_image_ang(x_normalized, 35)
    img_shear = transform_image_shear(x_normalized, 10)
    img_translate = transform_image_trans(x_normalized, 7)

    salient_angle = transform_image_ang(x_salient, 35)
    salient_shear = transform_image_shear(x_salient, 10)
    salient_translate = transform_image_trans(x_salient, 7)

    polar_angle = transform_image_ang(x_polar, 35)
    polar_shear = transform_image_shear(x_polar, 10)
    polar_translate = transform_image_trans(x_polar, 7)
34/15:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
for pic in popular_whale[0:5]: 
    p='train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
    img_salient, img_polar, img_normalized = load_image(p, 'all')
  
    x_normalized = image.img_to_array(img_normalized)
    x_salient = image.img_to_array(img_salient)
    x_polar = image.img_to_array(img_polar)
    
    img_angle = transform_image_ang(x_normalized, 35)
    img_shear = transform_image_shear(x_normalized, 10)
    img_translate = transform_image_trans(x_normalized, 7)
    plt.imshow(img_angle)

    salient_angle = transform_image_ang(x_salient, 35)
    salient_shear = transform_image_shear(x_salient, 10)
    salient_translate = transform_image_trans(x_salient, 7)

    polar_angle = transform_image_ang(x_polar, 35)
    polar_shear = transform_image_shear(x_polar, 10)
    polar_translate = transform_image_trans(x_polar, 7)
34/16:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
for pic in popular_whale[0:5]: 
    p='train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            #ax.imshow(img)
    img_salient, img_polar, img_normalized = load_image(p, 'all')
  
    x_normalized = image.img_to_array(img_normalized)
    x_salient = image.img_to_array(img_salient)
    x_polar = image.img_to_array(img_polar)
    
    img_angle = transform_image_ang(x_normalized, 35)
    img_shear = transform_image_shear(x_normalized, 10)
    img_translate = transform_image_trans(x_normalized, 7)
    plt.imshow(img_angle)

    salient_angle = transform_image_ang(x_salient, 35)
    salient_shear = transform_image_shear(x_salient, 10)
    salient_translate = transform_image_trans(x_salient, 7)

    polar_angle = transform_image_ang(x_polar, 35)
    polar_shear = transform_image_shear(x_polar, 10)
    polar_translate = transform_image_trans(x_polar, 7)
34/17:
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
for pic in popular_whale[0:5]: 
    p='train/'+pic
    with Image.open(p) as img:
            fig, ax = plt.subplots()
            ax.imshow(img)
    img_salient, img_polar, img_normalized = load_image(p, 'all')
  
    x_normalized = image.img_to_array(img_normalized)
    x_salient = image.img_to_array(img_salient)
    x_polar = image.img_to_array(img_polar)
    
    img_angle = transform_image_ang(x_normalized, 35)
    img_shear = transform_image_shear(x_normalized, 10)
    img_translate = transform_image_trans(x_normalized, 7)
    #plt.imshow(img_angle)

    salient_angle = transform_image_ang(x_salient, 35)
    salient_shear = transform_image_shear(x_salient, 10)
    salient_translate = transform_image_trans(x_salient, 7)

    polar_angle = transform_image_ang(x_polar, 35)
    polar_shear = transform_image_shear(x_polar, 10)
    polar_translate = transform_image_trans(x_polar, 7)
36/1:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv(path+'train.csv')
df_train=df_train.sample(frac=0.1,random_state=1)
36/2:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv('train.csv')
df_train=df_train.sample(frac=0.1,random_state=1)
36/3:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv('train.csv')
df_train=df_train.sample(frac=0.1,random_state=1)
df_train.head(3)
36/4: id_map = df_train[['Id']].drop_duplicates()
36/5:
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
36/6:
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
36/7:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_DIM, IMG_DIM))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    return  np.array(normalized_img, dtype="float") / 255.0
36/8:
imgs = [img_to_array(load_img(train_files+'/'+pic)) for pic in X.Image ]
imgs = np.array(imgs)
36/9:
imgs = [img_to_array(load_img('train'+'/'+pic)) for pic in X.Image ]
imgs = np.array(imgs)
36/10:
import cv2
(x_train, x_val, y_train, y_val) = train_test_split(imgs, y, test_size=0.25,shuffle=True,random_state=1)
36/11:
print(x_train.shape)
print(y_train.shape)
print(x_val.shape)
print(y_val.shape)
36/12:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
#input_shape=(height,width,channels)
#(1,IMG_SIZE, IMG_SIZE)

model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,1)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
36/13:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
IMG_SIZE = 128
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,1)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
36/14:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
IMG_SIZE = 128
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,1)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
36/15: model.summary()
36/16:
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, 
                 show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))
36/17:
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
36/18:
batch_size = 3#30
epochs = 3 #30
input_shape = (150, 150, 3)
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
36/19:
batch_size = 5
epochs = 5
input_shape = (150, 150, 3)
37/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project')
37/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/basic cnn.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project')
36/20:
from keras.models import Sequential # to create a cnn model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import keras
model = Sequential()
IMG_SIZE = 128
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', 
                 activation = 'relu',input_shape = (IMG_SIZE, IMG_SIZE,3)))
model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))
model.add(Dropout(0.25))

# fully connected
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dense(y_train.shape[1], activation = "softmax"))
#
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
36/21:
batch_size = 3#30
epochs = 3 #30
input_shape = (150, 150, 3)
history = model.fit(x=x_train, y=y_train,
                    validation_data=(x_val, y_val),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1)
39/1:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv('train.csv')
df_train=df_train.sample(frac=0.1,random_state=1)
df_train.head(3)
39/2: id_map = df_train[['Id']].drop_duplicates()
39/3:
num_classes =id_map.shape[0]
# put labels into y_train variable
y = df_train["Id"]
# Drop 'Id' column
X = df_train.drop(labels = ["Id"], axis = 1)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
39/4:
def load_image(fn):
    original_img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)
    scaled_original = cv2.resize(original_img, (IMG_DIM, IMG_DIM))
    normalized_img = cv2.normalize(scaled_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    return  np.array(normalized_img, dtype="float") / 255.0
39/5:
imgs = [img_to_array(load_img('train'+'/'+pic)) for pic in X.Image ]
imgs = np.array(imgs)
39/6: y.head(3)
39/7: y[0:3]
39/8:
#def fit(self, classifier_template, df_train, df_val, imgs_folder,
#           batch_size=32, nbr_epochs=30,
#          model_save_file=None,
#         history_save_file=None):
batch_size=32 
nbr_epochs=30
train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train'#imgs_folder, 
                x_col="Imge", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
39/9:
#def fit(self, classifier_template, df_train, df_val, imgs_folder,
#           batch_size=32, nbr_epochs=30,
#          model_save_file=None,
#         history_save_file=None):
batch_size=32 
nbr_epochs=30
train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train'#imgs_folder, 
                x_col="Image", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
39/10:
#def fit(self, classifier_template, df_train, df_val, imgs_folder,
#           batch_size=32, nbr_epochs=30,
#          model_save_file=None,
#         history_save_file=None):
batch_size=32 
nbr_epochs=30
train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train'#imgs_folder, 
                x_col='Image', y_col='Id',
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
39/11:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 21 09:51:46 2019

@author: shahla
"""

#import glob
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split 
from keras.utils.np_utils import to_categorical

IMG_DIM = (128, 128)
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group Project/'
train_files = path+'train'
#change the following to the whole date set by chaging partial_df to train.csv
LABELS = path+'partial_df'
df_train=pd.read_csv('train.csv')
df_train=df_train.sample(frac=0.1,random_state=1)
df_train.head(3)
39/12:
#def fit(self, classifier_template, df_train, df_val, imgs_folder,
#           batch_size=32, nbr_epochs=30,
#          model_save_file=None,
#         history_save_file=None):
batch_size=32 
nbr_epochs=30
train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train'#imgs_folder, 
                x_col='Image', y_col='Id',
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
39/13:
#def fit(self, classifier_template, df_train, df_val, imgs_folder,
#           batch_size=32, nbr_epochs=30,
#          model_save_file=None,
#         history_save_file=None):
batch_size=32 
nbr_epochs=30
train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train',#imgs_folder
                x_col='Image', y_col='Id',
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
39/14:
#def fit(self, classifier_template, df_train, df_val, imgs_folder,
#           batch_size=32, nbr_epochs=30,
#          model_save_file=None,
#         history_save_file=None):
batch_size=32 
nbr_epochs=30
train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train',#imgs_folder
                x_col='Image', y_col='Id',
                #target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
39/15:
df_val=df_train
validation_steps = math.ceil(df_val.shape[0] / batch_size)
validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory=imgs_folder, x_col="img", y_col="classname",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9})
            #early = EarlyStopping(monitor="val_loss", mode="min", patience=6)
            #callbacks_list = [early]
            callbacks_list = []
            best_model_callback = ModelCheckpoint(model_save_file, monitor='val_loss',
                                                      verbose=1, save_best_only=True)
            callbacks_list.append(best_model_callback)
39/16:
df_val=df_train
validation_steps = math.ceil(df_val.shape[0] / batch_size)
validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory=imgs_folder, x_col="img", y_col="classname",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9})
            #early = EarlyStopping(monitor="val_loss", mode="min", patience=6)
            #callbacks_list = [early]
callbacks_list = []
best_model_callback = ModelCheckpoint(model_save_file, monitor='val_loss',
                                                      verbose=1, save_best_only=True)
callbacks_list.append(best_model_callback)
39/17:
import math
df_val=df_train
validation_steps = math.ceil(df_val.shape[0] / batch_size)
validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory=imgs_folder, x_col="img", y_col="classname",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9})
            #early = EarlyStopping(monitor="val_loss", mode="min", patience=6)
            #callbacks_list = [early]
callbacks_list = []
best_model_callback = ModelCheckpoint(model_save_file, monitor='val_loss',
                                                      verbose=1, save_best_only=True)
callbacks_list.append(best_model_callback)
39/18:
import math
df_val=df_train
validation_steps = math.ceil(df_val.shape[0] / batch_size)
validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory='train',#imgs_folder
                x_col="Image", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9})
            #early = EarlyStopping(monitor="val_loss", mode="min", patience=6)
            #callbacks_list = [early]
callbacks_list = []
best_model_callback = ModelCheckpoint(model_save_file, monitor='val_loss',
                                                      verbose=1, save_best_only=True)
callbacks_list.append(best_model_callback)
39/19:
import math
df_val=df_train
validation_steps = math.ceil(df_val.shape[0] / batch_size)
validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory='train',#imgs_folder
                x_col="Image", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9})
            #early = EarlyStopping(monitor="val_loss", mode="min", patience=6)
            #callbacks_list = [early]
39/20:
import math
df_val=df_train
validation_steps = math.ceil(df_val.shape[0] / batch_size)
validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory='train',#imgs_folder
                x_col="Image", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9}
                )
39/21:
import math
df_val=df_train
validation_steps = math.ceil(df_val.shape[0] / batch_size)
validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory='train',#imgs_folder
                x_col="Image", y_col="Id",
                #target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9}
                )
39/22:
imgs_folder='train'
def fit(self, classifier_template, df_train, df_val, imgs_folder,
        batch_size=32, nbr_epochs=30,
        model_save_file=None,
        history_save_file=None):
        batch_size=32 
        nbr_epochs=30
        train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train',#imgs_folder
                x_col='Image', y_col='Id',
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
39/23:
import math
imgs_folder='train'
def fit(self, classifier_template, df_train, df_val, imgs_folder,
        batch_size=32, nbr_epochs=30,
        model_save_file=None,
        history_save_file=None):
        batch_size=32 
        nbr_epochs=30
        train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train',#imgs_folder
                x_col='Image', y_col='Id',
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
        if df_val is not None
                validation_steps = math.ceil(df_val.shape[0] / batch_size)
                validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory='train',#imgs_folder
                x_col="Image", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9}
                )
39/24:
import math
imgs_folder='train'
def fit(self, classifier_template, df_train, df_val, imgs_folder,
        batch_size=32, nbr_epochs=30,
        model_save_file=None,
        history_save_file=None):
        batch_size=32 
        nbr_epochs=30
        train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train',#imgs_folder
                x_col='Image', y_col='Id',
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
        if df_val is not None:
                validation_steps = math.ceil(df_val.shape[0] / batch_size)
                validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory='train',#imgs_folder
                x_col="Image", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9}
                )
        else:
            validation_generator = None
            validation_steps = None
            callbacks_list = []
        model = classifier_template.create_model()
        history = model.fit_generator(
            train_generator,
            steps_per_epoch=math.ceil(df_train.shape[0] / batch_size),
            nb_epoch=nbr_epochs,
            validation_data=validation_generator,
            validation_steps=validation_steps,
            callbacks=callbacks_list)

        model.save(model_save_file)
        if history_save_file is not None:
            with open(history_save_file, 'wb') as handle:
                pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)
        return model
39/25:
import math
imgs_folder='train'
def fit(self, classifier_template, df_train, df_val, imgs_folder,
        batch_size=32, nbr_epochs=30,
        model_save_file=None,
        history_save_file=None):
        batch_size=32 
        nbr_epochs=30
        train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory='train',#imgs_folder
                x_col='Image', y_col='Id',
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )
        if df_val is not None:
                validation_steps = math.ceil(df_val.shape[0] / batch_size)
                validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory='train',#imgs_folder
                x_col="Image", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9}
                )
        else:
            validation_generator = None
            validation_steps = None
            callbacks_list = []
        model = classifier_template.create_model()
        history = model.fit_generator(
            train_generator,
            steps_per_epoch=math.ceil(df_train.shape[0] / batch_size),
            nb_epoch=nbr_epochs,
            validation_data=validation_generator,
            validation_steps=validation_steps,
            callbacks=callbacks_list)

        model.save(model_save_file)
        if history_save_file is not None:
            with open(history_save_file, 'wb') as handle:
                pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)
        return model
42/1: pwd
42/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/classifier.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project')
42/3: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/classifier.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project')
42/4: runfile('/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/classifier.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project')
41/1:
import os

from keras.applications import densenet
from keras.models import Model
from keras.layers import Activation, Dense
from keras import regularizers
from keras.optimizers import SGD



def DenseNetTemplate(
        learning_rate,
        model_name,
        img_width ,
        img_height,
        num_classes):
      base_model = densenet.DenseNet121(input_shape=(img_height, img_width, 3),
                                     weights='imagenet',
                                     include_top=False,
                                     pooling='avg')
      for layer in base_model.layers:
        layer.trainable = True

      x = base_model.output
      x = Dense(1000, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)
      x = Activation('relu')(x)
      x = Dense(500, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)
      x = Activation('relu')(x)
      predictions = Dense(num_classes, activation='softmax')(x)
      model = Model(inputs=base_model.input, outputs=predictions)
      optimizer = SGD(lr=learning_rate, momentum=0.9, decay=0.0, nesterov=True)
      model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
      return model
41/2:
import os

from keras.applications import densenet
from keras.models import Model
from keras.layers import Activation, Dense
from keras import regularizers
from keras.optimizers import SGD



def DenseNetTemplate(
        learning_rate,
        model_name,
        img_width ,
        img_height,
        num_classes):
      base_model = densenet.DenseNet121(input_shape=(img_height, img_width, 3),
                                     weights='imagenet',
                                     include_top=False,
                                     pooling='avg')
      for layer in base_model.layers:
        layer.trainable = True

      x = base_model.output
      x = Dense(1000, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)
      x = Activation('relu')(x)
      x = Dense(500, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)
      x = Activation('relu')(x)
      predictions = Dense(num_classes, activation='softmax')(x)
      model = Model(inputs=base_model.input, outputs=predictions)
      optimizer = SGD(lr=learning_rate, momentum=0.9, decay=0.0, nesterov=True)
      model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
      return model
41/3:
def fit(classifier_template, df_train, df_val,
                     imgs_folder,
                     batch_size, 
                     nbr_epochs, model_save_file):
     train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory=imgs_folder, x_col="Image", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )

     if df_val is not None:
            validation_steps = math.ceil(df_val.shape[0] / batch_size)
            validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory=imgs_folder, x_col="Image", y_col="Id",
                target_size=(classifier_template.img_width, classifier_template.img_height),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9}
                )
            #early = EarlyStopping(monitor="val_loss", mode="min", patience=6)
            #callbacks_list = [early]
            callbacks_list = []
            best_model_callback = ModelCheckpoint(model_save_file, monitor='val_loss',
                                                      verbose=1, save_best_only=True)
            callbacks_list.append(best_model_callback)
     else:
            validation_generator = None
            validation_steps = None
            callbacks_list = []
     model = classifier_template.create_model()
     history = model.fit_generator(
            train_generator,
            steps_per_epoch=math.ceil(df_train.shape[0] / batch_size),
            nb_epoch=nbr_epochs,
            validation_data=validation_generator,
            validation_steps=validation_steps,
            callbacks=callbacks_list)

     model.save(model_save_file)
     return model
41/4:
def cross_validate_fit( classifier_template, saved_folder=None):
        # df_train must contains two columns: Images and Ids
        if saved_folder is None:
            print("Model saved path is not provided.")
            raise ValueError
        if os.path.isdir(saved_folder) is False:
            os.mkdir(saved_folder)
        if os.path.isdir(saved_folder+"/"+classifier_template.model_name) is False:
            os.mkdir(saved_folder+"/"+classifier_template.model_name)

        
        y = df["Id"]
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            if i<=2:
                continue
            print("CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            best_model_file = saved_folder+"/"+classifier_template.model_name + "/bestmodel.hdf5.cv" + str(i)
            fit(classifier_template, df_train, df_val,
                     ############################################
                     path+"train/",
                     ##########################################################
                     batch_size=32, 
                     nbr_epochs=30, model_save_file=best_model_file)
41/5:
def eval(model, df, img_folder, classifier_template):
        batch_size=32
        validation_steps = math.ceil(df.shape[0] / batch_size)
        validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
            df,
            directory=img_folder, x_col="Image", y_col="Id",
            target_size=(classifier_template.img_width, classifier_template.img_height),
            batch_size=32,
            shuffle=True,
            class_mode='categorical')
        eval_res = model.evaluate_generator(validation_generator, steps=validation_steps, verbose=1)
        res = {}
        res["loss"] = eval_res[0]
        res["acc"] = eval_res[1]
        return res
41/6:
def cross_validate_eval( saved_folder, classifier_template):
        if saved_folder is None:
            print("Need to provide the folder to the saved models.")
            raise ValueError
        # use the models generated from cross validation
        scores = {}
        scores["train_loss"] = []
        scores["train_acc"] = []
        scores["val_loss"] = []
        scores["val_acc"] = []
        y = df["Id"]
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            print("Evaluating CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            model_save_file = saved_folder+"/"+ classifier_template.model_name + "/bestmodel.hdf5.cv" + str(i)
            model = load_model(model_save_file)
            res_train = eval(model, df_train, "/train/", classifier_template)
            res_val = eval(model, df_val, "/train/", classifier_template)
            scores["train_loss"].append(res_train["loss"])
            scores["train_acc"].append(res_train["acc"])
            scores["val_loss"].append(res_val["loss"])
            scores["val_acc"].append(res_val["acc"])
        df_scores = pd.DataFrame(scores)
        df_scores.index.name = "CV round"
        df_scores = df_scores.T
        df_scores["mean"] = df_scores.mean(axis=1)
        df_scores["std"] = df_scores.std(axis=1)
        with open(saved_folder + "/" + classifier_template.model_name + "/cv_score.pickle", 'wb') as handle:
            pickle.dump(df_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)
        with open(saved_folder + "/" + classifier_template.model_name + "/cv_score.txt", 'w') as handle:
            handle.write(saved_folder + "\n")
            handle.write(str(df_scores))
            handle.write("\n")
        return df_scores
41/7: pwd
41/8:
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/'
df = pd.read_csv(path+'train.csv')
batch_size=32
41/9:
import pandas as pd
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/'
df = pd.read_csv(path+'train.csv')
batch_size=32
41/10:
model=DenseNetTemplate(
        learning_rate=0.001,
        model_name = "DenseNet",
        img_width = 299,
        img_height = 299,
        num_classes = 5005)
41/11:
df = df.iloc[list(range(0, df.shape[0], 200))].reset_index(drop=True)
cross_validate_fit(model, "/saved_models")
41/12:
df = df.iloc[list(range(0, df.shape[0], 200))].reset_index(drop=True)
cross_validate_fit(model, "/saved_models")
41/13:
def cross_validate_fit( classifier_template, saved_folder=None):
        # df_train must contains two columns: Images and Ids
        #if saved_folder is None:
         #   print("Model saved path is not provided.")
          #  raise ValueError
        #if os.path.isdir(saved_folder) is False:
         #   os.mkdir(saved_folder)
        #if os.path.isdir(saved_folder+"/"+classifier_template.model_name) is False:
         #   os.mkdir(saved_folder+"/"+classifier_template.model_name)

        
        y = df["Id"]
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            if i<=2:
                continue
            print("CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            best_model_file = saved_folder+"/"+classifier_template.model_name + "/bestmodel.hdf5.cv" + str(i)
            fit(classifier_template, df_train, df_val,
                     ############################################
                     path+"train/",
                     ##########################################################
                     batch_size=32, 
                     nbr_epochs=30, model_save_file=best_model_file)
41/14:
df = df.iloc[list(range(0, df.shape[0], 200))].reset_index(drop=True)
cross_validate_fit(model, "/saved_models")
41/15:
from sklearn.model_selection import StratifiedKFold
df = df.iloc[list(range(0, df.shape[0], 200))].reset_index(drop=True)
cross_validate_fit(model, "/saved_models")
41/16:
from sklearn.model_selection import StratifiedKFold
df = df.iloc[list(range(0, df.shape[0], 400))].reset_index(drop=True)
cross_validate_fit(model, "/saved_models")
41/17:
y = df["Id"]
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            if i<=2:
                continue
41/18: dim(y)
41/19: len(y)
41/20: y[0:3]
41/21: y[0:7]
41/22: df.head(3)
41/23:
from sklearn.model_selection import StratifiedKFold
df_train=df_train.sample(frac=0.1,random_state=1)
cross_validate_fit(model, "/saved_models")
41/24:
from sklearn.model_selection import StratifiedKFold
df=df.sample(frac=0.1,random_state=1)
cross_validate_fit(model, "/saved_models")
41/25:
from sklearn.model_selection import StratifiedKFold
df=df.sample(frac=0.1,random_state=1)
41/26:
import pandas as pd
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/'
df = pd.read_csv(path+'train.csv')
batch_size=32
41/27:
from sklearn.model_selection import StratifiedKFold
df=df.sample(frac=0.1,random_state=1)
41/28: cross_validate_fit(model, "/saved_models")
41/29:
def cross_validate_eval( saved_folder, classifier_template):
        if saved_folder is None:
            print("Need to provide the folder to the saved models.")
            raise ValueError
        # use the models generated from cross validation
        scores = {}
        scores["train_loss"] = []
        scores["train_acc"] = []
        scores["val_loss"] = []
        scores["val_acc"] = []
        y = df["Id"]
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            print("Evaluating CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            model_save_file = saved_folder+"/"+ classifier_template.model_name + "/bestmodel.hdf5.cv" + str(i)
            model = load_model(model_save_file)
            res_train = eval(model, df_train, "/train/", classifier_template)
            res_val = eval(model, df_val, "/train/", classifier_template)
            scores["train_loss"].append(res_train["loss"])
            scores["train_acc"].append(res_train["acc"])
            scores["val_loss"].append(res_val["loss"])
            scores["val_acc"].append(res_val["acc"])
        df_scores = pd.DataFrame(scores)
        df_scores.index.name = "CV round"
        df_scores = df_scores.T
        df_scores["mean"] = df_scores.mean(axis=1)
        df_scores["std"] = df_scores.std(axis=1)
        with open(saved_folder + "/" + classifier_template.DenseNet + "/cv_score.pickle", 'wb') as handle:
            pickle.dump(df_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)
        with open(saved_folder + "/" + classifier_template.DenseNet + "/cv_score.txt", 'w') as handle:
            handle.write(saved_folder + "\n")
            handle.write(str(df_scores))
            handle.write("\n")
        return df_scores
41/30:
import pandas as pd
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/'
df = pd.read_csv(path+'train.csv')
batch_size=32
41/31:
from sklearn.model_selection import StratifiedKFold
df=df.sample(frac=0.1,random_state=1)
41/32: cross_validate_fit(model, "/saved_models")
41/33:
def cross_validate_fit( classifier_template, saved_folder=None):
        # df_train must contains two columns: Images and Ids
        #if saved_folder is None:
         #   print("Model saved path is not provided.")
          #  raise ValueError
        #if os.path.isdir(saved_folder) is False:
         #   os.mkdir(saved_folder)
        #if os.path.isdir(saved_folder+"/"+classifier_template.model_name) is False:
         #   os.mkdir(saved_folder+"/"+classifier_template.model_name)

        
        y = df["Id"]
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            if i<=2:
                continue
            print("CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            best_model_file = saved_folder+"/"+classifier_template.DenseNet + "/bestmodel.hdf5.cv" + str(i)
            fit(classifier_template, df_train, df_val,
                     ############################################
                     path+"train/",
                     ##########################################################
                     batch_size=32, 
                     nbr_epochs=30, model_save_file=best_model_file)
41/34: cross_validate_fit(model, "/saved_models")
41/35:
def cross_validate_eval( saved_folder, classifier_template):
        if saved_folder is None:
            print("Need to provide the folder to the saved models.")
            raise ValueError
        # use the models generated from cross validation
        scores = {}
        scores["train_loss"] = []
        scores["train_acc"] = []
        scores["val_loss"] = []
        scores["val_acc"] = []
        y = df["Id"]
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            print("Evaluating CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            model_save_file = saved_folder+"/DenseNet" + "/bestmodel.hdf5.cv" + str(i)
            model = load_model(model_save_file)
            res_train = eval(model, df_train, "/train/", classifier_template)
            res_val = eval(model, df_val, "/train/", classifier_template)
            scores["train_loss"].append(res_train["loss"])
            scores["train_acc"].append(res_train["acc"])
            scores["val_loss"].append(res_val["loss"])
            scores["val_acc"].append(res_val["acc"])
        df_scores = pd.DataFrame(scores)
        df_scores.index.name = "CV round"
        df_scores = df_scores.T
        df_scores["mean"] = df_scores.mean(axis=1)
        df_scores["std"] = df_scores.std(axis=1)
        with open(saved_folder + "/DenseNet" + "/cv_score.pickle", 'wb') as handle:
            pickle.dump(df_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)
        with open(saved_folder + "/DenseNet" + "/cv_score.txt", 'w') as handle:
            handle.write(saved_folder + "\n")
            handle.write(str(df_scores))
            handle.write("\n")
        return df_scores
41/36: cross_validate_fit(model, "/saved_models")
41/37:
def cross_validate_fit( classifier_template, saved_folder=None):
        # df_train must contains two columns: Images and Ids
        #if saved_folder is None:
         #   print("Model saved path is not provided.")
          #  raise ValueError
        #if os.path.isdir(saved_folder) is False:
         #   os.mkdir(saved_folder)
        #if os.path.isdir(saved_folder+"/"+classifier_template.model_name) is False:
         #   os.mkdir(saved_folder+"/"+classifier_template.model_name)

        
        y = df["Id"]
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            if i<=2:
                continue
            print("CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            best_model_file = saved_folder+"/DenseNet" + "/bestmodel.hdf5.cv" + str(i)
            fit(classifier_template, df_train, df_val,
                     ############################################
                     path+"train/",
                     ##########################################################
                     batch_size=32, 
                     nbr_epochs=30, model_save_file=best_model_file)
41/38: cross_validate_fit(model, "/saved_models")
41/39:
from keras.preprocessing.image import ImageDataGenerator
cross_validate_fit(model, "/saved_models")
41/40:
import os

from keras.applications import densenet
from keras.models import Model
from keras.layers import Activation, Dense
from keras import regularizers
from keras.optimizers import SGD



def DenseNetTemplate(
        learning_rate,
        model_name,
        img_width ,
        img_height,
        num_classes):
      base_model = densenet.DenseNet121(input_shape=(img_height, img_width, 3),
                                     weights='imagenet',
                                     include_top=False,
                                     pooling='avg')
      for layer in base_model.layers:
        layer.trainable = True

      x = base_model.output
      x = Dense(1000, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)
      x = Activation('relu')(x)
      x = Dense(500, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)
      x = Activation('relu')(x)
      predictions = Dense(num_classes, activation='softmax')(x)
      model = Model(inputs=base_model.input, outputs=predictions)
      optimizer = SGD(lr=learning_rate, momentum=0.9, decay=0.0, nesterov=True)
      model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
      return model
41/41:
def fit(classifier_template, df_train, df_val,
                     imgs_folder,
                     batch_size, 
                     nbr_epochs, model_save_file):
     train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory=imgs_folder, x_col="Image", y_col="Id",
                target_size=(299,299),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )

     if df_val is not None:
            validation_steps = math.ceil(df_val.shape[0] / batch_size)
            validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory=imgs_folder, x_col="Image", y_col="Id",
                target_size=(299,299),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9}
                )
            #early = EarlyStopping(monitor="val_loss", mode="min", patience=6)
            #callbacks_list = [early]
            callbacks_list = []
            best_model_callback = ModelCheckpoint(model_save_file, monitor='val_loss',
                                                      verbose=1, save_best_only=True)
            callbacks_list.append(best_model_callback)
     else:
            validation_generator = None
            validation_steps = None
            callbacks_list = []
     model = classifier_template.create_model()
     history = model.fit_generator(
            train_generator,
            steps_per_epoch=math.ceil(df_train.shape[0] / batch_size),
            nb_epoch=nbr_epochs,
            validation_data=validation_generator,
            validation_steps=validation_steps,
            callbacks=callbacks_list)

     model.save(model_save_file)
     return model
41/42:
def cross_validate_fit( classifier_template, saved_folder=None):
        # df_train must contains two columns: Images and Ids
        #if saved_folder is None:
         #   print("Model saved path is not provided.")
          #  raise ValueError
        #if os.path.isdir(saved_folder) is False:
         #   os.mkdir(saved_folder)
        #if os.path.isdir(saved_folder+"/"+classifier_template.model_name) is False:
         #   os.mkdir(saved_folder+"/"+classifier_template.model_name)

        
        y = df["Id"]
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            if i<=2:
                continue
            print("CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            best_model_file = saved_folder+"/DenseNet" + "/bestmodel.hdf5.cv" + str(i)
            fit(classifier_template, df_train, df_val,
                     ############################################
                     path+"train/",
                     ##########################################################
                     batch_size=32, 
                     nbr_epochs=30, model_save_file=best_model_file)
41/43:
def eval(model, df, img_folder, classifier_template):
        batch_size=32
        validation_steps = math.ceil(df.shape[0] / batch_size)
        validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
            df,
            directory=img_folder, x_col="Image", y_col="Id",
            target_size=(299,299),
            batch_size=32,
            shuffle=True,
            class_mode='categorical')
        eval_res = model.evaluate_generator(validation_generator, steps=validation_steps, verbose=1)
        res = {}
        res["loss"] = eval_res[0]
        res["acc"] = eval_res[1]
        return res
41/44:
def cross_validate_eval( saved_folder, classifier_template):
        if saved_folder is None:
            print("Need to provide the folder to the saved models.")
            raise ValueError
        # use the models generated from cross validation
        scores = {}
        scores["train_loss"] = []
        scores["train_acc"] = []
        scores["val_loss"] = []
        scores["val_acc"] = []
        y = df["Id"]
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            print("Evaluating CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            model_save_file = saved_folder+"/DenseNet" + "/bestmodel.hdf5.cv" + str(i)
            model = load_model(model_save_file)
            res_train = eval(model, df_train, "/train/", classifier_template)
            res_val = eval(model, df_val, "/train/", classifier_template)
            scores["train_loss"].append(res_train["loss"])
            scores["train_acc"].append(res_train["acc"])
            scores["val_loss"].append(res_val["loss"])
            scores["val_acc"].append(res_val["acc"])
        df_scores = pd.DataFrame(scores)
        df_scores.index.name = "CV round"
        df_scores = df_scores.T
        df_scores["mean"] = df_scores.mean(axis=1)
        df_scores["std"] = df_scores.std(axis=1)
        with open(saved_folder + "/DenseNet" + "/cv_score.pickle", 'wb') as handle:
            pickle.dump(df_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)
        with open(saved_folder + "/DenseNet" + "/cv_score.txt", 'w') as handle:
            handle.write(saved_folder + "\n")
            handle.write(str(df_scores))
            handle.write("\n")
        return df_scores
41/45:
import pandas as pd
path='/Users/shahla/Dropbox/MLCourse/ML1020/Group_Project/'
df = pd.read_csv(path+'train.csv')
batch_size=32
41/46:
from sklearn.model_selection import StratifiedKFold
df=df.sample(frac=0.1,random_state=1)
41/47:
from keras.preprocessing.image import ImageDataGenerator
cross_validate_fit(model, "/saved_models")
41/48:
import math
from keras.preprocessing.image import ImageDataGenerator
cross_validate_fit(model, "/saved_models")
41/49:
import math
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.preprocessing.image import ImageDataGenerator
cross_validate_fit(model, "/saved_models")
41/50:
def fit(classifier_template, df_train, df_val,
                     imgs_folder,
                     batch_size, 
                     nbr_epochs, model_save_file):
     train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory=imgs_folder, x_col="Image", y_col="Id",
                target_size=(299,299),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                #classes={"c0":0,"c1":1,"c2":2,"c3":3,"c4":4,"c5":5,"c6":6,"c7":7,"c8":8,"c9":9}
                )

     if df_val is not None:
            validation_steps = math.ceil(df_val.shape[0] / batch_size)
            validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory=imgs_folder, x_col="Image", y_col="Id",
                target_size=(299,299),
                batch_size=batch_size,
                shuffle=True,
                # save_to_dir = '../data/aug_output/',
                # save_prefix = 'aug',
                # classes = class_names,
                class_mode='categorical',
                #classes={"c0": 0, "c1": 1, "c2": 2, "c3": 3, "c4": 4, "c5": 5, "c6": 6, "c7": 7, "c8": 8, "c9": 9}
                )
            #early = EarlyStopping(monitor="val_loss", mode="min", patience=6)
            #callbacks_list = [early]
            callbacks_list = []
            best_model_callback = ModelCheckpoint(model_save_file, monitor='val_loss',
                                                      verbose=1, save_best_only=True)
            callbacks_list.append(best_model_callback)
     else:
            validation_generator = None
            validation_steps = None
            callbacks_list = []
     model = classifier_template
     history = model.fit_generator(
            train_generator,
            steps_per_epoch=math.ceil(df_train.shape[0] / batch_size),
            nb_epoch=nbr_epochs,
            validation_data=validation_generator,
            validation_steps=validation_steps,
            callbacks=callbacks_list)

     model.save(model_save_file)
     return model
41/51:
import math
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.preprocessing.image import ImageDataGenerator
cross_validate_fit(model, "/saved_models")
41/52:
model=DenseNetTemplate(
        learning_rate=0.001,
        model_name = "DenseNet",
        img_width = 299,
        img_height = 299,
    ###change num_classes later
        num_classes = 224)
41/53:
import math
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.preprocessing.image import ImageDataGenerator
cross_validate_fit(model, "/saved_models")
50/1:
import matplotlib.pyplot as plt
from PIL import Image
import pandas as pd
import numpy as np
50/2:
from keras.applications import densenet
from keras.models import Model
from keras.layers import Activation, Dense
from keras import regularizers
from keras.optimizers import SGD
import pandas as pd
import numpy as np
import os
import math

from keras.models import load_model
from sklearn.model_selection import StratifiedKFold

from keras.applications.inception_v3 import InceptionV3
from keras.layers import Flatten, Dense, AveragePooling2D
from keras.models import Model
from keras.optimizers import RMSprop, SGD
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.preprocessing.image import ImageDataGenerator

import argparse
import pickle
import glob
50/3: path='/home/andyshengyizhu/'
50/4:
train = pd.read_csv('/home/andyshengyizhu/train.csv')
df=train
50/5:
train = pd.read_csv('train.csv')
df=train
50/6:
def DenseNetTemplate(
        learning_rate,
        model_name,
        img_width ,
        img_height,
        num_classes):
      base_model = densenet.DenseNet121(input_shape=(img_height, img_width, 3),
                                     weights='imagenet',
                                     include_top=False,
                                     pooling='avg')
      for layer in base_model.layers:
        layer.trainable = True

      x = base_model.output
      x = Dense(1000, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)
      x = Activation('relu')(x)
      x = Dense(500, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)
      x = Activation('relu')(x)
      predictions = Dense(num_classes, activation='softmax')(x)
      model = Model(inputs=base_model.input, outputs=predictions)
      optimizer = SGD(lr=learning_rate, momentum=0.9, decay=0.0, nesterov=True)
      model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
      return model
50/7:
def fit(classifier_template, df_train, df_val,
                     imgs_folder,
                     batch_size, 
                     nbr_epochs, model_save_file):
     train_generator = ImageDataGenerator(
                rescale=1. / 255,
                shear_range=0.1,
                zoom_range=0.1,
                rotation_range=10.,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True).flow_from_dataframe(
                df_train,
                directory=imgs_folder, x_col="Image", y_col="Id",
                target_size=(img_width, img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                )

     if df_val is not None:
            validation_steps = math.ceil(df_val.shape[0] / batch_size)
            validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
                df_val,
                directory=imgs_folder, x_col="Image", y_col="Id",
                target_size=(img_width, img_height),
                batch_size=batch_size,
                shuffle=True,
                class_mode='categorical',
                )
            #early = EarlyStopping(monitor="val_loss", mode="min", patience=6)
            #callbacks_list = [early]
            callbacks_list = []
            best_model_callback = ModelCheckpoint(model_save_file, monitor='val_loss',
                                                      verbose=1, save_best_only=True)
            callbacks_list.append(best_model_callback)
     else:
            validation_generator = None
            validation_steps = None
            callbacks_list = []
     model = classifier_template
     history = model.fit_generator(
            train_generator,
            steps_per_epoch=math.ceil(df_train.shape[0] / batch_size),
            nb_epoch=nbr_epochs,
            validation_data=validation_generator,
            validation_steps=validation_steps,
            callbacks=callbacks_list)

     model.save(model_save_file)
     return model
50/8:
def cross_validate_fit( classifier_template, saved_folder=None):
        # df_train must contains two columns: Images and Ids
        #if saved_folder is None:
         #   print("Model saved path is not provided.")
          #  raise ValueError
        #if os.path.isdir(saved_folder) is False:
        #    os.mkdir(saved_folder)
        #if os.path.isdir(saved_folder+"/"+"DenseNet") is False:
        #    os.mkdir(saved_folder+"/"+"DenseNet")

        
        
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            if i<=2:
                continue
            print("CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            best_model_file = saved_folder+"/"+"DenseNet" + "/bestmodel.hdf5.cv" + str(i)
            fit(classifier_template, df_train, df_val,
                     ############################################
                     path+"train/",
                     ##########################################################
                     batch_size=32, 
                     nbr_epochs=5, model_save_file=best_model_file)
50/9:
def eval(model, df, img_folder, classifier_template):
        batch_size=32
        validation_steps = math.ceil(df.shape[0] / batch_size)
        validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_dataframe(
            df,
            directory=img_folder, x_col="Image", y_col="Id",
            target_size=(img_width, img_height),
            batch_size=32,
            shuffle=True,
            class_mode='categorical')
        eval_res = model.evaluate_generator(validation_generator, steps=validation_steps, verbose=1)
        res = {}
        res["loss"] = eval_res[0]
        res["acc"] = eval_res[1]
        return res
50/10:
def cross_validate_eval( saved_folder, classifier_template):
        if saved_folder is None:
            print("Need to provide the folder to the saved models.")
            raise ValueError
        # use the models generated from cross validation
        scores = {}
        scores["train_loss"] = []
        scores["train_acc"] = []
        scores["val_loss"] = []
        scores["val_acc"] = []
        
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        for i, (train_index, val_index) in enumerate(skf.split(df, y)):
            print("Evaluating CV round %d..." % i)
            df_train = df.iloc[train_index].reset_index(drop=True)
            df_val = df.iloc[val_index].reset_index(drop=True)
            model_save_file = saved_folder+"/"+ "DenseNet" + "/bestmodel.hdf5.cv" + str(i)
            model = load_model(model_save_file)
            res_train = eval(model, df_train, "/train/", classifier_template)
            res_val = eval(model, df_val, "/train/", classifier_template)
            scores["train_loss"].append(res_train["loss"])
            scores["train_acc"].append(res_train["acc"])
            scores["val_loss"].append(res_val["loss"])
            scores["val_acc"].append(res_val["acc"])
        df_scores = pd.DataFrame(scores)
        df_scores.index.name = "CV round"
        df_scores = df_scores.T
        df_scores["mean"] = df_scores.mean(axis=1)
        df_scores["std"] = df_scores.std(axis=1)
        with open(saved_folder + "/" + "DenseNet" + "/cv_score.pickle", 'wb') as handle:
            pickle.dump(df_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)
        with open(saved_folder + "/" + "DenseNet" + "/cv_score.txt", 'w') as handle:
            handle.write(saved_folder + "\n")
            handle.write(str(df_scores))
            handle.write("\n")
        return df_scores
50/11:
img_height = 299
img_width = 299
batch_size=32
50/12: df = df.iloc[list(range(0, df.shape[0], 200))].reset_index(drop=True)
50/13: y = df['Id']
50/14:
id_map = df[['Id']].drop_duplicates()
num_classes =id_map.shape[0]
50/15:
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y, num_classes = num_classes )
50/16:
model=DenseNetTemplate(
        learning_rate=0.001,
        model_name = "DenseNet",
        img_width = 299,
        img_height = 299,
        num_classes = num_classes)
50/17: cross_validate_fit(model, "/saved_models")
51/1:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
50/18:
y = df['Id']
X=df['Image']
50/19:
from sklearn.model_selection import train_test_split 
(x_train, x_val, y_train, y_val) = train_test_split(X, y, test_size=0.25,shuffle=True,random_state=1)
52/1: pwd
52/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/3: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/4: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/5: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/6: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/7: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/8: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/9: pwd
52/10: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/11: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/12: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/13: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/14: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
52/15: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
55/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/1.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
56/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir("../input"))
path_to_data='../input/'

# Any results you write to the current directory are saved as output.
56/2: pwd
56/3:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory


path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project

# Any results you write to the current directory are saved as output.
56/4:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory


path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'

# Any results you write to the current directory are saved as output.
56/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
56/6:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
56/7:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
56/8: csv_filepath = 'path_to_data' + '*.h5'
56/9:
input_files = sorted(glob.glob(csv_filepath))
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
56/10: df = pd.read_hdf(path_to_data+'2.h5')
56/11: print("We have ",df.shape[1],"columns and", df.shape[0],"rows.")
56/12:
list_columns=list(df)
list_columns
56/13: 'price' in list_columns
56/14:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    [["price","mp"]]
56/15: ts.head
56/16:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    [["price","mp"]]
56/17: ts.head
56/18: ts
56/19:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    [["price","mp"]]
56/20:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    [["price","mp"]]
    type(ts)
56/21:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    [["price","mp"]]
    print(type(ts))
56/22:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    [["price","mp"]]
    print(type(ts))
56/23:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    [["price","mp"]]
    print('s')
56/24:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)[["price","mp"]]
    print('s')
56/25:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)[["price","mp"]]
    
    
print('s')
56/26:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)[["price","mp"]]
    print('s')
56/27: ts
56/28:
def feature_space(input_files_train_contract, lookback_points=30, lookforward_points=5):
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    [["price","mp"]]
    if ts.shape[0] > 300:
        for i in range(0, lookback_points):
            ts["Lookback%s" % str(i+1)] = ts["price"].shift(i+1)
        for i in range(0, lookforward_points):
            ts["Lookforward%s" % str(i+1)] = ts["mp"].shift(-(i+1))
        ts.dropna(inplace=True)
        ts["Lookback0"] = ts["mp"].pct_change()*100.0
        for i in range(0, lookback_points):
            ts["Lookback%s" % str(i+1)] = ts["Lookback%s" % str(i+1)].pct_change()*100.0
        for i in range(0, lookforward_points):
            ts["Lookforward%s" % str(i+1)] = ts[
                "Lookforward%s" % str(i+1)
            ].pct_change()*100.0
        ts["UpDown"] = np.sign(ts.price.pct_change())
        ts.dropna(inplace=True)
        ts["UpDown"] = ts["UpDown"].astype(int)
        ts["UpDown"].replace(to_replace=0, value=-1, inplace=True)
    return ts
56/29:
for input_files_train_contract in input_files_train_contracts:
    ts = feature_space(input_files_train_contract)
    print("Preprocessing data...")
    X = ts[
        [
            "Lookback%s" % str(i)
            for i in range(0, 5)
        ]
    ]
    y = ts["UpDown"]
56/30:
print("Preprocessing data...")
X = ts[
    [
        "Lookback%s" % str(i)
        for i in range(0, 5)
    ]
]
y = ts["UpDown"]
56/31:
for input_files_train_contract in input_files_train_contracts:
    ts = feature_space(input_files_train_contract)
    print("Preprocessing data...")
    X = ts[
        [
            "Lookback%s" % str(i)
            for i in range(0, 5)
        ]
    ]
    y = ts["UpDown"]
56/32:
for input_files_train_contract in input_files_train_contracts:
    #ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)[["price","mp"]]
    print('s')
56/33: type(input_files_train_contracts)
56/34: input_files_train_contracts
56/35: input_files
56/36:
input_files = glob.glob(csv_filepath)
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
56/37: input_files
56/38: len(df)
56/39: input_files=df
56/40:
#input_files = sorted(glob.glob(csv_filepath))
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
56/41: input_files
56/42: input_files_train_contracts
56/43:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)[["price","mp"]]
56/44:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)[["price","mp"]]
    print('s')
56/45:
for input_files_train_contract in input_files_train_contracts:
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    print('s')
56/46:
def feature_space(input_files_train_contract, lookback_points=30, lookforward_points=5):
    ts = pd.read_hdf(input_files_train_contract, key='df', header=True, index_col="date_timeExg", parse_dates=True)
    [["price","mp"]]
    if ts.shape[0] > 300:
        for i in range(0, lookback_points):
            ts["Lookback%s" % str(i+1)] = ts["price"].shift(i+1)
        for i in range(0, lookforward_points):
            ts["Lookforward%s" % str(i+1)] = ts["mp"].shift(-(i+1))
        ts.dropna(inplace=True)
        ts["Lookback0"] = ts["mp"].pct_change()*100.0
        for i in range(0, lookback_points):
            ts["Lookback%s" % str(i+1)] = ts["Lookback%s" % str(i+1)].pct_change()*100.0
        for i in range(0, lookforward_points):
            ts["Lookforward%s" % str(i+1)] = ts[
                "Lookforward%s" % str(i+1)
            ].pct_change()*100.0
        ts["UpDown"] = np.sign(ts.price.pct_change())
        ts.dropna(inplace=True)
        ts["UpDown"] = ts["UpDown"].astype(int)
        ts["UpDown"].replace(to_replace=0, value=-1, inplace=True)
    return ts
56/47:
for input_files_train_contract in input_files_train_contracts:
    ts = feature_space(input_files_train_contract)
    print("Preprocessing data...")
    X = ts[
        [
            "Lookback%s" % str(i)
            for i in range(0, 5)
        ]
    ]
    y = ts["UpDown"]
56/48: y=input_files['price']
56/49: X=input_files.drop('price',1)
56/50: X.head
56/51:
# Use the training-testing split with 70% of data in the
# training data with the remaining 30% of data in the testing
print("Creating train/test split of data...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)
56/52:
# Use the training-testing split with 70% of data in the
# training data with the remaining 30% of data in the testing
print("Creating train/test split of data...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)
56/53:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory


path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'

# Any results you write to the current directory are saved as output.
56/54:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
56/55:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
56/56: csv_filepath = 'path_to_data' + '*.h5'
56/57:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
56/58:
#input_files = sorted(glob.glob(csv_filepath))
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
56/59: input_files
56/60: print("We have ",df.shape[1],"columns and", df.shape[0],"rows.")
56/61:
list_columns=list(df)
list_columns
57/1:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['price'].plot(grid=True)

# Show the plot
plt.show()
57/2:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
57/3:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
57/4: csv_filepath = 'path_to_data' + '*.h5'
57/5:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
57/6:
#input_files = sorted(glob.glob(csv_filepath))
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
57/7: input_files
57/8:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['price'].plot(grid=True)

# Show the plot
plt.show()
57/9: input_files.index
57/10: min(input_files.index)
57/11: min(input_files.index)
57/12: max(input_files.index)
57/13:
# Import matplotlib
import matplotlib.pyplot as plt

# Plot the distribution of `
input_files[['price']].hist(bins=50)

# Show the plot
plt.show()

# Pull up summary statistics
print(input_files[['price']].describe())
57/14:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['forward_price_sec_1'].plot(grid=True)

# Show the plot
plt.show()
57/15:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['ret_sec_1'].plot(grid=True)

# Show the plot
plt.show()
57/16:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['forward_price_sec_3'].plot(grid=True)

# Show the plot
plt.show()
57/17:

type(input_files[sig_label_sec_1])
57/18:

type(input_files['sig_label_sec_1'])
57/19:

dtype(input_files['sig_label_sec_1'])
57/20:
# Import matplotlib
import matplotlib.pyplot as plt

# Plot the distribution of `
input_files[['sig_label_sec_1']].hist(bins=50)

# Show the plot
plt.show()

# Pull up summary statistics
print(input_files[['sig_label_sec_1']].describe())
57/21: print(input_files[['sig_label_sec_1'].dtypes)
57/22: print(input_files['sig_label_sec_1'].dtypes)
58/1:
print(input_files['sig_label_sec_1'].dtypes)
input_files.sig_label_sec_1 = input_files.sig_label_sec_1.astype('category')
58/2: csv_filepath = 'path_to_data' + '*.h5'
58/3:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
58/4:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
58/5: csv_filepath = 'path_to_data' + '*.h5'
58/6:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
58/7:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
58/8:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
58/9: csv_filepath = 'path_to_data' + '*.h5'
58/10:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
58/11:
#input_files = sorted(glob.glob(csv_filepath))
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
58/12:
print(input_files['sig_label_sec_1'].dtypes)
input_files.sig_label_sec_1 = input_files.sig_label_sec_1.astype('category')
58/13: print(input_files['sig_label_sec_1'].dtypes)
58/14:
# Import matplotlib
import matplotlib.pyplot as plt

# Plot the distribution of `
input_files[['sig_label_sec_1']].hist(bins=50)

# Show the plot
plt.show()

# Pull up summary statistics
print(input_files[['sig_label_sec_1']].describe())
58/15:

# Pull up summary statistics
print(input_files[['sig_label_sec_1']].describe())
58/16:
print(input_files['sig_label_sec_1'].dtypes)
input_files['sig_label_sec_1']
58/17: X=input_files.drop('price',1)
58/18:
X=input_files[['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']]
58/19:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

        inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

        p = ["40", "30", "20"]

        signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

        ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
        inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

        lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
58/20:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
58/21: lookbacks
58/22:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
58/23: input_files_train_contracts.head
58/24: input_files_train_contracts.head(10)
58/25: input_files_test_contracts.head(10)
58/26: input_files_train_contracts.tail(10)
58/27:
print(input_files['sig_label_sec_1'].dtypes)
input_files['sig_label_sec_1'].head
58/28:
print(input_files['sig_label_sec_1'].dtypes)
input_files['sig_label_sec_1'].head(10)
58/29: y_train=input_files_train_contracts['sig_label_sec_1']
58/30:
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']]
58/31:
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
58/32:
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
58/33: y_train=input_files_train_contracts['sig_label_sec_1']
58/34: y_test=input_files_test_contracts['sig_label_sec_1']
58/35: print("Creating train/test split of data...")
58/36: X_train.columns
58/37: X_train.head(4)
59/1: dtype(lookbacks)
59/2: type(lookbacks)
59/3:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
59/4: type(lookbacks)
59/5:
type(lookbacks)
print("-"*10)
59/6:
type(lookbacks)
print("-"*10)
lookbacks['BBI:3']
59/7:
type(lookbacks)
print("-"*10)
lookbacks['BBI_3']
59/8:
type(lookbacks)
print("-"*10)
lookbacks[BBI_3]
59/9:
type(lookbacks)
print("-"*10)
lookbacks[BBI_3]
59/10:
print(lookbacks)
print("-"*10)
59/11: (lookbacks)
59/12: lookbacks[100ms]
59/13: lookbacks
59/14: lookbacks['100ms']
59/15:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
59/16:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]
59/17:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222}
59/18:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222}
released
59/19:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]=1222,1012
released
59/20:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
released
59/21:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
59/22:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
released.get('iphone')
59/23:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
released.get('iphone',none)
59/24:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
released.get('iphone','none')
59/25:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
released.get('iphone','none')
released.items()
59/26:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
released.get('iphone','none')
for key, value in released.items():
    print(key,value)
59/27:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
released.get('iphone','none')
for key in released:
    print(count.get(key))
59/28:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
released.get('iphone','none')
for key in released:
    print(len(released[key]))
59/29:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
released.get('iphone','none')
count = {}
for element in released:
    count[element] = count.get(element, 0) + 1
print(count)
59/30:
released = {
        "iphone" : 2007,
        "iphone 3G" : 2008,
        "iphone 3GS" : 2009,
        "iphone 4" : 2010,
        "iphone 4S" : 2011,
        "iphone 5" : 2012
    }
released["iphone"]={1222,1012}
for key in released:
    print(released[key])
released.get('iphone','none')
released.values()
59/31: lookbacks.keys()
59/32: lookbacks['100ms']
59/33: lookbacks['300ms']
59/34:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
59/35:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
59/36: csv_filepath = 'path_to_data' + '*.h5'
59/37:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
59/38:
#input_files = sorted(glob.glob(csv_filepath))
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
59/39: input_files
59/40: print("We have ",df.shape[1],"columns and", df.shape[0],"rows.")
59/41:
list_columns=list(df)
list_columns
59/42: list_columns.dtypes()
59/43: list_columns.dtypes
59/44: type(list_columns)
59/45: df.dtypes
59/46:
g = df.columns.to_series().groupby(df.dtypes).groups
g
59/47:
mylist = list(df.select_dtypes(include=['bool']).columns)
mylist
59/48:
mylist = list(df.select_dtypes(include=['int8']).columns)
mylist
59/49:
sig = list(df.select_dtypes(include=['int8']).columns)
sig
59/50: range(0,len(sig))
59/51: print(range(0,len(sig)))
59/52:
for x in range(2, 6):
  print(x)
59/53:
for x in range(0, 6):
  print(x)
59/54:
for x in range(0, len(sig)-1):
  print(x)
59/55:
for i in range(0, len(sig)-1):
    input_files.sig[i]= input_files.sig[i].astype('category')
59/56: sig[0]
59/57:
for i in range(0, len(sig)-1):
    input_files[sig[i]]= input_files[sig[i]].astype('category')
59/58: input_files1[sig]= input_files[sig].astype('category')
59/59:
input_files1=input_files
input_files1[sig]= input_files[sig].astype('category')
59/60:

input_files[sig]= input_files[sig].astype('category')
59/61:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
59/62:
#input_files = sorted(glob.glob(csv_filepath))
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
59/63: input_files.info()
59/64:
list_columns=list(df)
list_columns
59/65:

input_files_train_contracts[sig]= input_files_train_contracts[sig].astype('category')
59/66:

input_files[sig]= input_files[sig].astype('category')
59/67: input_files[sig].dtypes
59/68: input_files.dtypes
59/69:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
59/70:
print(input_files['sig_label_sec_1'].dtypes)
input_files['sig_label_sec_1'].head(10)
59/71:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
59/72: input_files_train_contracts.tail(10)
59/73:
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
59/74:
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
59/75: y_train=input_files_train_contracts['sig_label_sec_1']
59/76: y_test=input_files_test_contracts['sig_label_sec_1']
59/77: xx=pd.Categorical(X_train,[0,1,2])
59/78:
xx=pd.Categorical(X_train,[0,1,2])
xx.dtaype()
59/79: type(xx)
59/80:
type(xx)
xx.describe()
59/81: xx=pd.Categorical(X_train,[0,1,2])
59/82:
type(xx)
xx.describe()
59/83: xx.head()
59/84: xx
59/85:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
59/86:

input_files[sig] = input_files[sig].astype('category', ordered=True, categories=[0, 1, 2])
59/87:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
59/88:
print(input_files['sig_label_sec_1'].dtypes)
input_files['sig_label_sec_1'].head(10)
59/89:

# Pull up summary statistics
print(input_files[['sig_label_sec_1']].describe())
59/90: input_files[sig_label_sec_15].describe()
59/91: input_files['sig_label_sec_15'].describe()
59/92: input_files['sig_label_sec_15']
59/93:
ser = pd.Series([1,-1,1,1,-1], dtype='int32')
ser
59/94:
ser = pd.Series([1,-1,1,1,-1], dtype='category')
ser
59/95:
ser.astype('category', ordered=True, categories=[2, 1])
ser
59/96:
ser.astype('category', ordered=True, categories=[3, 1])
ser
59/97:
input_files['sig_label_sec_15']
type(input_files)
59/98:
input_files['sig_label_sec_15']
dtype(input_files)
59/99:
input_files['sig_label_sec_15']
input_files.dtypes
59/100:
input_files['sig_label_sec_15']
dtype(input_files)
59/101:
input_files['sig_label_sec_15']
type(input_files)
59/102: input_files['sig_label_sec_15'].cat.codes
59/103:
ser = pd.Series([1,-1,1,1,-1], dtype='category')
ser
59/104:
ser.astype('category', ordered=True, categories=[0,-1, 1])
ser
59/105:
train = pd.Series(list('abbaa'))
test = pd.Series(list('abcd'))
categories = np.union1d(train, test)
train = train.astype('category', categories=categories)
test = test.astype('category', categories=categories)
59/106: test.describe()
59/107: train.describe()
59/108: categories
59/109:
ser = pd.Series([1,-1,1,1,-1], dtype='category')
ser
59/110:
ser.astype('category', ordered=True, categories=['0',''-1', '1'])
ser
59/111:
ser.astype('category', ordered=True, categories=[0,-1, 1])
ser
59/112:
ser.astype('category', ordered=True, categories=[0,-1, 2])
ser
59/113:
ser.astype('category', ordered=True, categories=[0,1, 2])
ser
59/114:
train = pd.Series(list('abbaa'))
test = pd.Series(list('abcd'))
categories = np.union1d(train, test)
train = train.astype('category', categories=categories)
59/115:
train = pd.Series(list('abbaa'))
test = pd.Series(list('abcd'))
categories = np.union1d(train, test)
train = train.astype('category', categories=categories)
train
59/116: train = train.astype('category', categories=[ab,c])
59/117: train = train.astype('category', categories=[a,b,c])
59/118: train = train.astype('category', categories=['a',b,c])
59/119: train = train.astype('category', categories=['a','b','c'])
59/120:
train = train.astype('category', categories=['a','b','c'])
train
59/121:
train = train.astype('category', categories=['f','b','c'])
train
59/122:
ser.astype('category', categories=[0,1, 2])
ser
59/123:
ser.astype('category', ordered=True, categories=[0,-1, 1])
ser.cat.codes
59/124:
ser.astype('category', ordered=True, categories=[0,-1, 2])
ser.cat.codes
59/125:
ser.astype('category', ordered=True, categories=[0, 2])
ser.cat.codes
59/126:
train = pd.Series(list('abbaa'))
test = pd.Series(list('abcd'))
categories = np.union1d(train, test)
train = train.astype('category', categories=categories)
train
59/127:
train = train.astype('category', categories=['f','b','c'])
train
59/128:
#Cast a pandas object to a specified dtype 
input_files[sig] = input_files[sig].astype('category', ordered=True, categories=[-1, 0, 1])
59/129: input_files['sig_label_sec_15'].cat.codes
59/130:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
pd.get_dummies(test)
59/131:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
59/132:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
59/133: #input_files = sorted(glob.glob(csv_filepath))
59/134: input_files.info()
59/135:
list_columns=list(input_files)
list_columns
59/136:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
59/137:
#Cast a pandas object to a specified dtype 
categories=[-1, 0, 1]
input_files[sig] = input_files[sig].astype('category', ordered=True, categories = categories)
59/138:
# -1 replaced with 0
# 0 replaced with 1
# 1 replaced with 2
#input_files['sig_label_sec_15'].cat.codes
59/139:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
59/140:
# -1 replaced with 0
# 0 replaced with 1
# 1 replaced with 2
input_files1=input_files
input_files1['sig_label_sec_15']=input_files1['sig_label_sec_15'].cat.codes
59/141:
# -1 replaced with 0
# 0 replaced with 1
# 1 replaced with 2
input_files1=input_files
input_files1['sig_label_sec_15']=input_files1['sig_label_sec_15'].cat.codes
input_files1['sig_label_sec_15'].describe()
59/142:
# -1 replaced with 0
# 0 replaced with 1
# 1 replaced with 2
input_files1=input_files
input_files1['sig_label_sec_15']=input_files1['sig_label_sec_15'].cat.codes
input_files1['sig_label_sec_15']
59/143:
# -1 replaced with 0
# 0 replaced with 1
# 1 replaced with 2
input_files1=input_files
input_files1['sig_label_sec_15']=input_files1['sig_label_sec_15'].cat.codes
59/144:
#Cast a pandas object to a specified dtype 
categories=[-1, 0, 1]
input_files[sig] = input_files[sig].astype('category', ordered=True, categories = categories)
59/145:
# -1 replaced with 0
# 0 replaced with 1
# 1 replaced with 2
input_files1=input_files
input_files1['sig_label_sec_15']=input_files1['sig_label_sec_15'].cat.codes
59/146: input_files1['sig_label_sec_15']
59/147: input_files1['sig_label_sec_15'][0:3]
59/148: input_files['sig_label_sec_15'][0:3]
59/149: input_files['sig_label_sec_15'][0:7]
59/150: input_files['sig_label_sec_15'][0:30]
59/151: input_files1['sig_label_sec_15'][0:30]
59/152: input_files['sig_label_sec_15'][30:60]
59/153: input_files1['sig_label_sec_15'][30:60]
59/154:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df
59/155:
#Cast a pandas object to a specified dtype 
categories=[-1, 0, 1]
input_files[sig] = input_files[sig].astype('category', ordered=True, categories = categories)
59/156: input_files['sig_label_sec_15'][30:60]
59/157:
# -1 replaced with 0
# 0 replaced with 1
# 1 replaced with 2
input_files1=input_files
input_files1['sig_label_sec_15']=input_files1['sig_label_sec_15'].cat.codes
59/158: input_files1['sig_label_sec_15'][30:60]
59/159: input_files['sig_label_sec_15'][30:60]
59/160:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
59/161:
#Cast a pandas object to a specified dtype 
categories=[-1, 0, 1]
input_files[sig] = input_files[sig].astype('category', ordered=True, categories = categories)
59/162: input_files['sig_label_sec_15'][30:60]
59/163:
# -1 replaced with 0
# 0 replaced with 1
# 1 replaced with 2
input_files1=input_files.copy()
input_files1['sig_label_sec_15']=input_files1['sig_label_sec_15'].cat.codes
59/164: input_files1['sig_label_sec_15'][30:60]
59/165: input_files['sig_label_sec_15'][30:60]
59/166:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files1=input_files.copy()
input_files1[sig]=input_files1[sig].cat.codes
59/167:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files1=input_files.copy()
input_files1[sig]=input_files1[sig].apply(lambda x: x.cat.codes)
59/168: input_files1['sig_label_sec_15'][30:60]
59/169: input_files['sig_label_sec_15'][30:60]
59/170:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2

input_files[sig]=input_files[sig].apply(lambda x: x.cat.codes)
59/171:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
59/172:
print(input_files['sig_label_sec_1'].dtypes)
input_files['sig_label_sec_1'].head(10)
59/173:

# Pull up summary statistics
print(input_files[['sig_label_sec_1']].describe())
59/174:

# Pull up summary statistics
print(type(input_files[['sig_label_sec_1']]))
59/175:

# Pull up summary statistics
print(input_files[['sig_label_sec_1']].describe())
59/176: input_files.info()
59/177: input_files.describe()
59/178: input_files.dtypes()
59/179: type(input_files())
59/180: type(input_files)
59/181: input_files.dtypes
59/182:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
59/183:
#Cast a pandas object to a specified dtype 
categories=[-1, 0, 1]
input_files[sig] = input_files[sig].astype('category', ordered=True, categories = categories)
59/184: input_files[sig]=input_files[sig].apply(lambda x: x.cat.rename_categories([0 , 1, 2]))
59/185: sig_label_sec_1.dtypes
59/186: input_files[sig_label_sec_1].dtypes
59/187: input_files['sig_label_sec_1'].dtypes
59/188:
input_files['sig_label_sec_1'].dtypes
input_files['sig_label_sec_1'].cat.rename_categories([0,1,2])
59/189:
input_files['sig_label_sec_1'].dtypes
input_files['sig_label_sec_1'].cat.rename_categories([0,1])
59/190:
input_files['sig_label_sec_1'].dtypes
input_files['sig_label_sec_1'].add.rename_categories([0])
input_files['sig_label_sec_1'].cat.rename_categories([0,1,2])
59/191:
input_files['sig_label_sec_1'].dtypes
input_files['sig_label_sec_1'].cat.add_categories([0])
input_files['sig_label_sec_1'].cat.rename_categories([0,1,2])
59/192:
input_files['sig_label_sec_1'].dtypes
input_files['sig_label_sec_1'].cat.add_categories([0])
input_files['sig_label_sec_1'].cat.rename_categories([0,2])
59/193:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
59/194:
#Cast a pandas object to a specified dtype 
categories=[-1, 0, 1]
input_files[sig] = input_files[sig].astype('category', categories = categories)
59/195:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2

input_files[sig]=input_files[sig].apply(lambda x: x.cat.rename_categories([0,2])) # there is no zero in the input_files
input_files[sig]=input_files[sig].apply(lambda x: x.cat.add_categories([0]))
59/196: input_files['sig_BBI_2_100ms'].info()
59/197: input_files['sig_BBI_2_100ms'].info
59/198: input_files['sig_BBI_2_100ms'].dtypes
59/199: input_files[sig]=input_files[sig].apply(lambda x: x.cat.add_categories([0,1,2]))
59/200: x=input_files['sig_BBI_2_3s'].replace([-1,0,1],[0,1,2])
59/201: x=input_files['sig_BBI_2_3s'].replace([-1,0,1],[0,1,2])
59/202:
x=input_files['sig_BBI_2_3s'].replace([-1,0,1],[0,1,2])
x[20:40]
59/203:
x=input_files['sig_BBI_2_3s'].replace([-1,0,1],[0,1,2])
print(x[20:40])
print(input_files['sig_BBI_2_3s'][20:40])
59/204:
x=input_files['sig_BBI_2_3s'].replace([-1,0,1],[0,1,2])
print(x[20:40])
print(input_files['sig_BBI_2_3s'][20:40])
print(x.dtypes)
59/205:
x=input_files['sig_BBI_2_3s'].replace([-1,0,1],[0,1,2])
#print(x[20:40])
#print(input_files['sig_BBI_2_3s'][20:40])
print(x.dtypes)
59/206: input_files[sig] = input_files[sig].apply(lambda x: x.replace(([-1,0,1],[0,1,2])))
59/207: input_files[sig] = input_files[sig].apply(lambda x: x.replace(([-1],[0])))
64/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
import json

# Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
    json.dump(credentials, file)
64/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'learn python',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
64/3:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'learn python',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
64/4:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(5)
64/5:
from twython import TwythonStreamer  
import csv

# Filter out unwanted data
def process_tweet(tweet):  
    d = {}
    d['hashtags'] = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]
    d['text'] = tweet['text']
    d['user'] = tweet['user']['screen_name']
    d['user_loc'] = tweet['user']['location']
    return d


# Create a class that inherits TwythonStreamer
class MyStreamer(TwythonStreamer):     

    # Received data
    def on_success(self, data):

        # Only collect tweets in English
        if data['lang'] == 'en':
            tweet_data = process_tweet(data)
            self.save_to_csv(tweet_data)

    # Problem with the API
    def on_error(self, status_code, data):
        print(status_code, data)
        self.disconnect()

    # Save each tweet to csv file
    def save_to_csv(self, tweet):
        with open(r'saved_tweets.csv', 'a') as file:
            writer = csv.writer(file)
            writer.writerow(list(tweet.values()))
64/6:
stream = MyStreamer(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'],  
                    creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])
# Start the stream
stream.statuses.filter(track='python')
64/7:
from geopy.geocoders import Nominatim  
import gmplot

geolocator = Nominatim()

# Go through all tweets and add locations to 'coordinates' dictionary
coordinates = {'latitude': [], 'longitude': []}  
for count, user_loc in enumerate(tweets.location):  
    try:
        location = geolocator.geocode(user_loc)

        # If coordinates are found for location
        if location:
            coordinates['latitude'].append(location.latitude)
            coordinates['longitude'].append(location.longitude)

    # If too many connection requests
    except:
        pass

# Instantiate and center a GoogleMapPlotter object to show our map
gmap = gmplot.GoogleMapPlotter(30, 0, 3)

# Insert points on the map passing a list of latitudes and longitudes
gmap.heatmap(coordinates['latitude'], coordinates['longitude'], radius=20)

# Save the map to html file
gmap.draw("python_heatmap.html")
64/8:
from twython import TwythonStreamer  
import csv

# Filter out unwanted data
def process_tweet(tweet):  
    d = {}
    d['hashtags'] = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]
    d['text'] = tweet['text']
    d['user'] = tweet['user']['screen_name']
    d['user_loc'] = tweet['user']['location']
    return d


# Create a class that inherits TwythonStreamer
class MyStreamer(TwythonStreamer):     

    # Received data
    def on_success(self, data):

        # Only collect tweets in English
        if data['lang'] == 'en':
            tweet_data = process_tweet(data)
            self.save_to_csv(tweet_data)

    # Problem with the API
    def on_error(self, status_code, data):
        print(status_code, data)
        self.disconnect()

    # Save each tweet to csv file
    def save_to_csv(self, tweet):
        with open(r'saved_tweets.csv', 'a') as file:
            writer = csv.writer(file)
            writer.writerow(list(tweet.values()))
64/9:
stream = MyStreamer(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'],  
                    creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])
# Start the stream
stream.statuses.filter(track='python')
64/10:
stream = MyStreamer(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'],  
                    creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])
# Start the stream
stream.statuses.filter(track='python')
65/1:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
65/2:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
65/3: csv_filepath = 'path_to_data' + '*.h5'
65/4:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
65/5: #input_files = sorted(glob.glob(csv_filepath))
65/6:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
65/7: input_files[sig] = input_files[sig].apply(lambda x: x+1)
65/8:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
65/9:
x=input_files['sig_BBI_2_3s']
print(x.dtypes)
65/10:
x=input_files['sig_BBI_2_3s']
print(x[30:40])
65/11: input_files['sig_label_sec_1'].dtypes
65/12: input_files.dtypes
65/13: input_files["forward_price_sec_3 "].dtypes
65/14: type(input_files["forward_price_sec_3 "])
65/15: type(input_files["forward_price_sec_3"])
65/16: (input_files["forward_price_sec_3"].dtypes)
65/17: input_files["forward_price_sec_3"][10:30]
65/18:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['ret_sec_1'].plot(grid=True)

# Show the plot
plt.show()
65/19:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
65/20:

# Pull up summary statistics
print(input_files[['sig_label_sec_1']].describe())
65/21:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
65/22:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
65/23: input_files_train_contracts.tail(10)
65/24: input_files_test_contracts.head(10)
65/25:
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
65/26:
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
65/27: y_train=input_files_train_contracts['sig_label_sec_1']
65/28: y_test=input_files_test_contracts['sig_label_sec_1']
65/29: y_test=input_files_test_contracts['sig_label_sec_1']
67/1:
df = pd.read_hdf(path_to_data+'2.h5')
#input_files=df.copy()
67/2:
import pandas as pd
import sklearn
from sklearn.linear_model import LogisticRegression
67/3:
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
csv_filepath = 'path_to_data' + '*.h5'
67/4:
df = pd.read_hdf(path_to_data+'2.h5')
#input_files=df.copy()
67/5:
#Here we only look at a small percentage of data (20%) in our local machine
index = ceil(len(input_files)*0.2)
input_files = df[index]
67/6:
import pandas as pd
import sklearn
from math import ceil
from sklearn.linear_model import LogisticRegression
67/7:
#Here we only look at a small percentage of data (20%) in our local machine

index = ceil(len(input_files)*0.2)
input_files = df[index]
67/8:
#Here we only look at a small percentage of data (20%) in our local machine

index = ceil(len(df)*0.2)
input_files = df[index]
67/9:
#Here we only look at a small percentage of data (20%) in our local machine

index = ceil(len(df)*0.2)
input_files = df[:index]
67/10: input_files.info()
67/11:

sig_lookback_periods = [ "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s", "12s", "18s", "21s" ]

inv_sig_lookback_periods = [
        "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
        "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
    ]

p = ["40", "30", "20"]

signals = {
        "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
        "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
        "CCI": {"lookback_periods": sig_lookback_periods},
        "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
        "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "CMO": {"lookback_periods": sig_lookback_periods},
        "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
        "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
        "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
      }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                 **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}

#logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')

def x_subset(df):
    x=[]
    for n in ti:
        x.append(df.filter(like=('sig_'+n+'_')))
    inv_x=[]
    for n in inv_ti:
        inv_x.append(df.filter(like=n))
    return x,inv_x



class ModelManager:
    def __init__(self,raw_data=pd.DataFrame()):
        self.raw_data = raw_data
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.models=None
        
    def x_subset(self) ->None:
        self.train=self.raw_data[0:round(.7*len(self.raw_data))]
        self.test=self.raw_data[round(.7*len(self.raw_data))+1:len(self.raw_data)]
        for n in ti:
            self.X_train.append(self.train.filter(like=('sig_'+n+'_')))
        for n in inv_ti:
            self.X_train_inv.append(self.train.filter(like=n))
        for n in ti:
            self.X_test.append(self.test.filter(like=('sig_'+n+'_')))
        for n in inv_ti:
            self.X_test_inv.append(self.test.filter(like=n))    
        self.y_train=self.raw_data['sig_label_sec_1'][0:round(.7*len(self.raw_data))]
        self.y_test =self.raw_data['sig_label_sec_1'][round(.7*len(self.raw_data))+1:len(self.raw_data)]
    
        return self.X_train, self.X_train_inv,self.X_test,self.X_test_inv,self.y_train, self.y_test
    
    def build_models(self):
        self.models.append(LogisticRegression)
        return self.models
    def train_models(self):
        
        
    
r=ModelManager(df)    
x=r.x_subset()
x()
67/12:

sig_lookback_periods = [ "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s", "12s", "18s", "21s" ]

inv_sig_lookback_periods = [
        "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
        "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
    ]

p = ["40", "30", "20"]

signals = {
        "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
        "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
        "CCI": {"lookback_periods": sig_lookback_periods},
        "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
        "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "CMO": {"lookback_periods": sig_lookback_periods},
        "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
        "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
        "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
      }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                 **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}

#logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')

def x_subset(input_files):
    x=[]
    for n in ti:
        x.append(input_files.filter(like=('sig_'+n+'_')))
    inv_x=[]
    for n in inv_ti:
        inv_x.append(input_files.filter(like=n))
    return x,inv_x



class ModelManager:
    def __init__(self,raw_data=pd.DataFrame()):
        self.raw_data = raw_data
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.models=None
        
    def x_subset(self) ->None:
        self.train=self.raw_data[0:round(.7*len(self.raw_data))]
        self.test=self.raw_data[round(.7*len(self.raw_data))+1:len(self.raw_data)]
        for n in ti:
            self.X_train.append(self.train.filter(like=('sig_'+n+'_')))
        for n in inv_ti:
            self.X_train_inv.append(self.train.filter(like=n))
        for n in ti:
            self.X_test.append(self.test.filter(like=('sig_'+n+'_')))
        for n in inv_ti:
            self.X_test_inv.append(self.test.filter(like=n))    
        self.y_train=self.raw_data['sig_label_sec_1'][0:round(.7*len(self.raw_data))]
        self.y_test =self.raw_data['sig_label_sec_1'][round(.7*len(self.raw_data))+1:len(self.raw_data)]
    
        return self.X_train, self.X_train_inv,self.X_test,self.X_test_inv,self.y_train, self.y_test
    
    def build_models(self):
        self.models.append(LogisticRegression)
        return self.models
    def train_models(self):
        
        
    
r=ModelManager(input_files)    
x=r.x_subset()
x()
66/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Test.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
67/13:

sig_lookback_periods = [ "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s", "12s", "18s", "21s" ]

inv_sig_lookback_periods = [
        "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
        "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
    ]

p = ["40", "30", "20"]

signals = {
        "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
        "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
        "CCI": {"lookback_periods": sig_lookback_periods},
        "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
        "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "CMO": {"lookback_periods": sig_lookback_periods},
        "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
        "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
        "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
      }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                 **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}

#logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')

def x_subset(input_files):
    x=[]
    for n in ti:
        x.append(input_files.filter(like=('sig_'+n+'_')))
    inv_x=[]
    for n in inv_ti:
        inv_x.append(input_files.filter(like=n))
    return x,inv_x



class ModelManager:
    def __init__(self,raw_data=pd.DataFrame()):
        self.raw_data = raw_data
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.models=None
        
    def x_subset(self) ->None:
        self.train=self.raw_data[0:round(.7*len(self.raw_data))]
        self.test=self.raw_data[round(.7*len(self.raw_data))+1:len(self.raw_data)]
        for n in ti:
            self.X_train.append(self.train.filter(like=('sig_'+n+'_')))
        for n in inv_ti:
            self.X_train_inv.append(self.train.filter(like=n))
        for n in ti:
            self.X_test.append(self.test.filter(like=('sig_'+n+'_')))
        for n in inv_ti:
            self.X_test_inv.append(self.test.filter(like=n))    
        self.y_train=self.raw_data['sig_label_sec_1'][0:round(.7*len(self.raw_data))]
        self.y_test =self.raw_data['sig_label_sec_1'][round(.7*len(self.raw_data))+1:len(self.raw_data)]
    
        return self.X_train, self.X_train_inv,self.X_test,self.X_test_inv,self.y_train, self.y_test
    
    def build_models(self):
        self.models.append(LogisticRegression)
        return self.models
    def train_models(self):
67/14:

sig_lookback_periods = [ "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s", "12s", "18s", "21s" ]

inv_sig_lookback_periods = [
        "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
        "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
    ]

p = ["40", "30", "20"]

signals = {
        "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
        "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
        "CCI": {"lookback_periods": sig_lookback_periods},
        "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
        "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "CMO": {"lookback_periods": sig_lookback_periods},
        "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
        "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
        "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
      }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                 **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}

#logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')

def x_subset(input_files):
    x=[]
    for n in ti:
        x.append(input_files.filter(like=('sig_'+n+'_')))
    inv_x=[]
    for n in inv_ti:
        inv_x.append(input_files.filter(like=n))
    return x,inv_x



class ModelManager:
    def __init__(self,raw_data=pd.DataFrame()):
        self.raw_data = raw_data
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.models=None
        
    def x_subset(self) ->None:
        self.train=self.raw_data[0:round(.7*len(self.raw_data))]
        self.test=self.raw_data[round(.7*len(self.raw_data))+1:len(self.raw_data)]
        for n in ti:
            self.X_train.append(self.train.filter(like=('sig_'+n+'_')))
        for n in inv_ti:
            self.X_train_inv.append(self.train.filter(like=n))
        for n in ti:
            self.X_test.append(self.test.filter(like=('sig_'+n+'_')))
        for n in inv_ti:
            self.X_test_inv.append(self.test.filter(like=n))    
        self.y_train=self.raw_data['sig_label_sec_1'][0:round(.7*len(self.raw_data))]
        self.y_test =self.raw_data['sig_label_sec_1'][round(.7*len(self.raw_data))+1:len(self.raw_data)]
    
        return self.X_train, self.X_train_inv,self.X_test,self.X_test_inv,self.y_train, self.y_test
    
    def build_models(self):
        self.models.append(LogisticRegression)
        return self.models
67/15:
#Here we only look at a small percentage of data (20%) in our local machine

index = ceil(len(df)*0.2)
input_files = df[:index]
67/16: input_files.info()
67/17:

sig_lookback_periods = [ "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s", "12s", "18s", "21s" ]

inv_sig_lookback_periods = [
        "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
        "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
    ]

p = ["40", "30", "20"]

signals = {
        "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
        "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
        "CCI": {"lookback_periods": sig_lookback_periods},
        "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
        "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "CMO": {"lookback_periods": sig_lookback_periods},
        "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
        "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
        "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
      }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                 **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}

#logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')
71/1:

sig_lookback_periods = [ "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s", "12s", "18s", "21s" ]

inv_sig_lookback_periods = [
        "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
        "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
    ]

p = ["40", "30", "20"]

signals = {
        "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
        "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
        "CCI": {"lookback_periods": sig_lookback_periods},
        "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
        "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "CMO": {"lookback_periods": sig_lookback_periods},
        "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
        "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
        "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
      }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                 **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}

#logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')
71/2:
#Shahla's
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
##Andy's
#path_to_data='D:/MLCourses/ML1030/2.h5/reduced.csv'
csv_filepath = 'path_to_data' + '*.h5'
71/3:
df = pd.read_hdf(path_to_data+'2.h5')
#input_files=df.copy()
71/4:
import pandas as pd
import sklearn
from math import ceil
from sklearn.linear_model import LogisticRegression
71/5:
#Shahla's
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
##Andy's
#path_to_data='D:/MLCourses/ML1030/2.h5/reduced.csv'
csv_filepath = 'path_to_data' + '*.h5'
71/6:
df = pd.read_hdf(path_to_data+'2.h5')
#input_files=df.copy()
71/7:
#Here we only look at a small percentage of data (20%) in our local machine

index = ceil(len(df)*0.2)
input_files = df[:index]
71/8: input_files.info()
71/9:

sig_lookback_periods = [ "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s", "12s", "18s", "21s" ]

inv_sig_lookback_periods = [
        "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
        "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
    ]

p = ["40", "30", "20"]

signals = {
        "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
        "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
        "CCI": {"lookback_periods": sig_lookback_periods},
        "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
        "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "CMO": {"lookback_periods": sig_lookback_periods},
        "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
        "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
        "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
      }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                 **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}

#logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')
71/10:
def x_subset(input_files):
    x=[]
    for n in ti:
        x.append(input_files.filter(like=('sig_'+n+'_')))
    inv_x=[]
    for n in inv_ti:
        inv_x.append(input_files.filter(like=n))
    return x,inv_x
71/11: x_subset(input_files)
71/12: input_files.filter(like=('sig_'+'CCI'+'_')
71/13: input_files.filter(like=('sig_'+'CCI'+'_'))
71/14:
#Here we only look at a small percentage of data (20%) in our local machine

index = ceil(len(df)*0.2)
input_files = df[:index]
input_files.to_csv(reduced)
71/15:
#Here we only look at a small percentage of data (20%) in our local machine

index = ceil(len(df)*0.2)
input_files = df[:index]
input_files.to_csv('reduced.csv')
71/16:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor
    data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    yaml.dump(data,file_descriptor)
if --name-- = "--main--":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print data.get("items")
    for item_name, item_value in items.iteritems()
    print(item_name, item_value)
    filepath = "test2.yaml"
    data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
    yaml_dump(filepath2,data2)
            }
71/17:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
    yaml.dump(data,file_descriptor)
if --name-- = "--main--":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print data.get("items")
    for item_name, item_value in items.iteritems():
    print(item_name, item_value)
    filepath = "test2.yaml"
    data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]
 
   inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

   p = ["40", "30", "20"]

   signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

   ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
   inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

   lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
            }
    yaml_dump(filepath2,data2)
71/18:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
    yaml.dump(data,file_descriptor)
if --name-- = "--main--":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print data.get("items")
    for item_name, item_value in items.iteritems():
    print(item_name, item_value)
    filepath = "test2.yaml"
    data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]
 
   inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

   p = ["40", "30", "20"]

   signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

   ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
   inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

   lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
            }
71/19:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if --name-- = "--main--":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print data.get("items")
    for item_name, item_value in items.iteritems():
    print(item_name, item_value)
    filepath = "test2.yaml"
    data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]
 
   inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

   p = ["40", "30", "20"]

   signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

   ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
   inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

   lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
            }
71/20:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ = "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print data.get("items")
    for item_name, item_value in items.iteritems():
    print(item_name, item_value)
    filepath = "test2.yaml"
    data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]
 
   inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

   p = ["40", "30", "20"]

   signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

   ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
   inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

   lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
            }
71/21:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print data.get("items")
    for item_name, item_value in items.iteritems():
    print(item_name, item_value)
    filepath = "test2.yaml"
    data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]
 
   inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

   p = ["40", "30", "20"]

   signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

   ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
   inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

   lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
            }
71/22:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print(data.get("items"))
    for item_name, item_value in items.iteritems():
    print(item_name, item_value)
    filepath = "test2.yaml"
    data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]
 
   inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

   p = ["40", "30", "20"]

   signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

   ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
   inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

   lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
            }
71/23:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print(data.get("items"))
    for item_name, item_value in items.iteritems():
        print(item_name, item_value)
    filepath = "test2.yaml"
    data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]
 
   inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

   p = ["40", "30", "20"]

   signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

   ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
   inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

   lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
            }
71/24:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print(data.get("items"))
    for item_name, item_value in items.iteritems():
        print(item_name, item_value)
    filepath = "test2.yaml"
 data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]
 
   inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

   p = ["40", "30", "20"]

   signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

   ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
   inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

   lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
            }
71/26:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print(data.get("items"))
    for item_name, item_value in items.iteritems():
        print(item_name, item_value)
    filepath = "test2.yaml"
    data2 = {sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]
 
   inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

   p = ["40", "30", "20"]

   signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

   ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
   inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

   lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
            }
71/27:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    print(data.get("items"))
    for item_name, item_value in items.iteritems():
        print(item_name, item_value)
    filepath = "test2.yaml"
71/28:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    items = data.get("items")
    for item_name, item_value in items.iteritems():
        print(item_name, item_value)
    filepath = "test2.yaml"
71/29: items
71/30: print(items)
71/31:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    items = data.get("PATH")
    for item_name, item_value in items.iteritems():
        print(item_name, item_value)
    filepath = "test2.yaml"
71/32:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="test3.yaml"
    data = yaml_loader(filepath)
    items = data.get("signals")
    for item_name, item_value in items.iteritems():
        print(item_name, item_value)
    filepath = "test2.yaml"
71/33:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="test3.yaml"
    data = yaml_loader(filepath)
    items = data.get("signals")
    print(items)
    filepath = "test2.yaml"
71/34:
import yaml
def yaml_loader(filepath):
    """loads a yaml file"""
    with open(filepath, "r") as file_descriptor:
        data = yaml.load(file_descriptor)
    return data
def yaml_dump(filepath,data):
    """dumps data to a yaml file"""
    with open(filepath, "w") as file_descriptor:
        yaml.dump(data,file_descriptor)
if __name__ == "__main__":
    filepath="testv.yaml"
    data = yaml_loader(filepath)
    items = data.get("signals")
    print(items)
    filepath = "test2.yaml"
71/35: print(items)
72/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
72/2: lookbacks['100ms']
72/3: lookbacks['300ms']
72/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
72/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
72/6: csv_filepath = 'path_to_data' + '*.h5'
72/7:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
72/8: #input_files = sorted(glob.glob(csv_filepath))
72/9: input_files.info()
72/10:
list_columns=list(input_files)
list_columns
72/11:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
72/12:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
72/13:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
72/14:
#I only work with 20 percent of the data for the time being
index = ceil(len(df)*0.2)
input_files = df[:index]
72/15:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
72/16: input_files_train_contracts.tail(10)
72/17: input_files_test_contracts.head(10)
72/18:
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
72/19:
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
72/20: y_train=input_files_train_contracts['sig_label_sec_1']
72/21: y_test=input_files_test_contracts['sig_label_sec_1']
72/22: df.corr()
80/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
80/2: lookbacks['100ms']
80/3: lookbacks['300ms']
80/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
80/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
80/6: csv_filepath = 'path_to_data' + '*.h5'
80/7:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
80/8: #input_files = sorted(glob.glob(csv_filepath))
80/9: input_files.info()
80/10: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
80/11:
list_columns=list(input_files)
list_columns
80/12: input_files.dtypes
80/13:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
80/14:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
80/15:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
80/16:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
80/17:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
80/18: min(input_files.index)
80/19: max(input_files.index)
80/20:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['price'].plot(grid=True)

# Show the plot
plt.show()
80/21:
# Import matplotlib
import matplotlib.pyplot as plt

# Plot the distribution of `
input_files[['price']].hist(bins=50)

# Show the plot
plt.show()

# Pull up summary statistics
print(input_files[['price']].describe())
80/22:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['forward_price_sec_1'].plot(grid=True)

# Show the plot
plt.show()
80/23:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['forward_price_sec_3'].plot(grid=True)

# Show the plot
plt.show()
80/24:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['ret_sec_1'].plot(grid=True)

# Show the plot
plt.show()
80/25:

# Pull up summary statistics
print(input_files[['sig_label_sec_1']].describe())
80/26:
#I only work with 20 percent of the data for the time being
index = ceil(len(df)*0.2)
input_files = df[:index]
80/27: print("Creating train/test split of data...")
80/28:
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
80/29:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
80/30: input_files_train_contracts.tail(10)
80/31: input_files_test_contracts.head(10)
80/32:
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
80/33: input_files[sig_MOVING_AVG].corr()
80/34:
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
80/35: y_train=input_files_train_contracts['sig_label_sec_1']
80/36: y_test=input_files_test_contracts['sig_label_sec_1']
80/37:
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train,y_train)
predictions = lm.predict(X_test)
80/38:
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train,y_train)
y_pred = lm.predict(X_test)
80/39:
from sklearn.metrics import confusion_matrix
confusion_matrix(y_pred,y_test)
80/40: dtype(y_pred)
80/41: y_pred.dtypes()
80/42: types(y_pred)
80/43: type(y_pred)
80/44: y_pred.type()
80/45: y_pred.type
80/46: dtype(y_pred)
80/47: type(y_pred)
80/48: y_pred.dtype
80/49: y_test.dtype
80/50:
from sklearn.linear_model import LogisticRegression
lm = LogisticRegression()
lm.fit(X_train,y_train)
y_pred = lm.predict(X_test)
80/51:
score = lm.score(x_test, y_test)
print(score)
80/52:
score = lm.score(y_test, y_test)
print(score)
80/53:
score = lm.score(y_test, y_pred)
print(score)
81/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
81/2: lookbacks['100ms']
81/3: lookbacks['300ms']
81/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
81/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
81/6: csv_filepath = 'path_to_data' + '*.h5'
81/7:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
81/8: #input_files = sorted(glob.glob(csv_filepath))
81/9: input_files.info()
81/10: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
81/11:
list_columns=list(input_files)
list_columns
81/12: input_files.dtypes
81/13:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
81/14: input_files['ret_sec_1']
81/15:
#input_files = sorted(glob.glob(csv_filepath))
input_files.head()
81/16:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head()
83/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
83/2: lookbacks['100ms']
83/3: lookbacks['300ms']
83/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
83/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
83/6: csv_filepath = 'path_to_data' + '*.h5'
83/7:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
83/8:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head()
83/9: input_files.info()
83/10: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
83/11:
list_columns=list(input_files)
list_columns
83/12: input_files.dtypes
83/13:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
83/14:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
83/15: input_files['ret_sec_1']
83/16:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
83/17:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
83/18:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
85/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
85/2: lookbacks['100ms']
85/3: lookbacks['300ms']
85/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
85/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
85/6: csv_filepath = 'path_to_data' + '*.h5'
85/7:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
85/8:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head()
85/9: input_files.info()
85/10: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
85/11:
list_columns=list(input_files)
list_columns
85/12: input_files.dtypes
85/13:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
85/14:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
85/15: input_files['ret_sec_1']
85/16:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
85/17:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
85/18:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
86/1:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
86/2: csv_filepath = 'path_to_data' + '*.h5'
86/3:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
86/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
86/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
86/6: csv_filepath = 'path_to_data' + '*.h5'
86/7:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
86/8:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head()
86/9: input_files.info()
86/10: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
86/11:
list_columns=list(input_files)
list_columns
86/12: input_files.dtypes
86/13:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
86/14:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
86/15: input_files['ret_sec_1']
86/16:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
86/17:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
86/18:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
86/19: min(input_files.index)
86/20: max(input_files.index)
89/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
89/2: lookbacks['100ms']
89/3: lookbacks['300ms']
89/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
89/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
89/6: csv_filepath = 'path_to_data' + '*.h5'
89/7:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
89/8:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head()
89/9: input_files.info()
89/10: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
89/11:
list_columns=list(input_files)
list_columns
89/12: input_files.dtypes
89/13:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
89/14:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
89/15: input_files['ret_sec_1']
89/16:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
89/17:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
89/18:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
91/1:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
91/2:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
91/3: csv_filepath = 'path_to_data' + '*.h5'
91/4:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
91/5:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head()
91/6: input_files.info()
91/7: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
91/8:
list_columns=list(input_files)
list_columns
91/9: input_files.dtypes
91/10:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
91/11:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
91/12: input_files['ret_sec_1']
91/13:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
91/14:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
91/15:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
91/16: min(input_files.index)
93/1:
from hyperopt import hp
# Discrete uniform distribution
num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}
num_leaves
96/1:
from __future__ import print_function
import logging
import numpy as np
import sys

from optparse import OptionParser
from configparser import ConfigParser
import time
from pymongo import MongoClient
from twython import Twython, TwythonRateLimitError
from dateutil import parser
import datetime as dt
from datetime import datetime
96/2:
def is_recent(twt):
    '''Checks that the tweet is more recent than n_days'''
    return parser.parse(twt['created_at']).replace(tzinfo=None) > \                    (dt.datetime.today() -  dt.timedelta(days=n_days))
96/3:
def is_recent(twt):
    '''Checks that the tweet is more recent than n_days'''
    return parser.parse(twt['created_at']).replace(tzinfo=None) > \
   (dt.datetime.today() -  dt.timedelta(days=n_days))
96/4:
def followers_status(screen_name):
    followers = db.followers.find_one({"screen_name": screen_name})
    print("We have %s follower IDs for %s" %
                (len(followers['ids']), screen_name))
96/5:

def wait_for_awhile():
    reset = int(twitter.get_lastfunction_header('x-rate-limit-reset'))
    wait = max(reset - time.time(), 0) + 10
    print("Rate limit exceeded waiting: %sm %0.0fs"%
            (int(int( wait)/60),wait % 60 ))
    time.sleep(wait)
96/6:
print(__doc__)

# Display progress logs on stdout
logging.basicConfig(level=logging.INFO,
                    format='>>> %(asctime)s %(levelname)s %(message)s')
96/7:
# ---------------------------------------------------------
#  parse commandline arguments
# ---------------------------------------------------------

op = OptionParser()
op.add_option("-s", "--screen_name",
              dest="screen_name", type="string",
              help="Screen name of the main account")

op.add_option("-f", "--followers",
              action="store_true", dest="followers", default=False,
              help="Extracts IDs of screen_name followers from Twitter")

op.add_option("-t", "--timelines",
              action="store_true", dest="timelines", default=False,
              help="Extracts timelines of the followers from Twitter")

op.add_option("-d", "--dbname", dest="dbname", default='twitter',
              help="Name of the MongDB database")

op.add_option("-n", "--n_followers", dest="n_followers", default='5000',
              help="Number of follower IDs; 5000 at a time")


# Initialize
(opts, args) = op.parse_args()
print(opts)
96/8:
# ---------------------------------------------------------
#  parse commandline arguments
# ---------------------------------------------------------

op = OptionParser()
op.add_option("-s", "--screen_name",
              dest="screen_name", type="string",
              help="Screen name of the main account")

op.add_option("-f", "--followers",
              action="store_true", dest="followers", default=False,
              help="Extracts IDs of screen_name followers from Twitter")

op.add_option("-t", "--timelines",
              action="store_true", dest="timelines", default=False,
              help="Extracts timelines of the followers from Twitter")

op.add_option("-d", "--dbname", dest="dbname", default='twitter',
              help="Name of the MongDB database")

op.add_option("-n", "--n_followers", dest="n_followers", default='5000',
              help="Number of follower IDs; 5000 at a time")


# Initialize
(opts, args) = op.parse_args()
print(opts)
screen_name  = opts.screen_name.lower()     # The main twitter account
n_days       = 180          # Only tweets more recent than n_days are kept
n_followers  = int(opts.n_followers)
96/9:
# ---------------------------------------------------------
#  parse commandline arguments
# ---------------------------------------------------------

op = OptionParser()
op.add_option("-s", "--screen_name",
              dest="screen_name", type="string",
              help="Screen name of the main account")

op.add_option("-f", "--followers",
              action="store_true", dest="followers", default=False,
              help="Extracts IDs of screen_name followers from Twitter")

op.add_option("-t", "--timelines",
              action="store_true", dest="timelines", default=False,
              help="Extracts timelines of the followers from Twitter")

op.add_option("-d", "--dbname", dest="dbname", default='twitter',
              help="Name of the MongDB database")

op.add_option("-n", "--n_followers", dest="n_followers", default='5000',
              help="Number of follower IDs; 5000 at a time")


# Initialize
(opts, args) = op.parse_args()
print(opts)


screen_name  = opts.screen_name.lower()     # The main twitter account
n_days       = 180          # Only tweets more recent than n_days are kept
n_followers  = int(opts.n_followers)
96/10:
# ---------------------------------------------------------
#  parse commandline arguments
# ---------------------------------------------------------

op = OptionParser()
op.add_option("-s", "--screen_name",
              dest="screen_name", type="string",
              help="Screen name of the main account")

op.add_option("-f", "--followers",
              action="store_true", dest="followers", default=False,
              help="Extracts IDs of screen_name followers from Twitter")

op.add_option("-t", "--timelines",
              action="store_true", dest="timelines", default=False,
              help="Extracts timelines of the followers from Twitter")

op.add_option("-d", "--dbname", dest="dbname", default='twitter',
              help="Name of the MongDB database")

op.add_option("-n", "--n_followers", dest="n_followers", default='5000',
              help="Number of follower IDs; 5000 at a time")


# Initialize
(opts, args) = op.parse_args()
print(opts)


#screen_name  = opts.screen_name.lower()     # The main twitter account
n_days       = 180          # Only tweets more recent than n_days are kept
n_followers  = int(opts.n_followers)
96/11:
# ---------------------------------------------------------
#  Twitter Connection: credentials stored in twitter.cfg
# ---------------------------------------------------------
config = ConfigParser()
config.read('twitter.cfg')
# for py27 change config to get_config
APP_KEY       = config['credentials']['app_key']
APP_SECRET    = config['credentials']['app_secret']
twitter       = Twython(APP_KEY, APP_SECRET, oauth_version=2)
ACCESS_TOKEN  = twitter.obtain_access_token()
twitter       = Twython(APP_KEY, access_token=ACCESS_TOKEN)

#  MongoDB connection
client      = MongoClient()
db          = client[opts.dbname]
96/12:
# ---------------------------------------------------------
#  Twitter Connection: credentials stored in twitter.cfg
# ---------------------------------------------------------
config = ConfigParser()
config.read('twitter_credentials.json')
# for py27 change config to get_config
APP_KEY       = config['credentials']['app_key']
APP_SECRET    = config['credentials']['app_secret']
twitter       = Twython(APP_KEY, APP_SECRET, oauth_version=2)
ACCESS_TOKEN  = twitter.obtain_access_token()
twitter       = Twython(APP_KEY, access_token=ACCESS_TOKEN)

#  MongoDB connection
client      = MongoClient()
db          = client[opts.dbname]
96/13:
# ---------------------------------------------------------
#  Twitter Connection: credentials stored in twitter.cfg
# ---------------------------------------------------------




with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
twitter = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])



#  MongoDB connection
client      = MongoClient()
db          = client[opts.dbname]
96/14:
# ---------------------------------------------------------
#  Twitter Connection: credentials stored in twitter.cfg
# ---------------------------------------------------------


import json

with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
twitter = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])



#  MongoDB connection
client      = MongoClient()
db          = client[opts.dbname]
96/15:
# -----------------------------------------------------------
#  Follower IDs
# -----------------------------------------------------------

if opts.followers:
    followers_query_size = min(n_followers,5000) # Twitter default
    print("Retrieving %s followers" % n_followers)
    # ------------------------------------------------------------------
    #  1) get follower ids
    #  see https://dev.twitter.com/rest/reference/get/followers/ids
    # ------------------------------------------------------------------

    # Initialize the database followers record for that screen_name
    res = db.followers.find_one( {"screen_name": screen_name})
    if res is None:
        db.followers.insert_one( {"screen_name": screen_name, "ids": []} )

    # cursor is used to navigate a twitter collection
    # https://dev.twitter.com/overview/api/cursoring
    next_cursor     = -1
    follower_ids    = list()
    ids_count       = 0
    while (next_cursor != 0) and ( ids_count < n_followers):
        try:
            print("Followers %s to %s: cursor: %s" %
                    (ids_count, ids_count + followers_query_size, next_cursor))
            result = twitter.get_followers_ids(screen_name = screen_name,
                                            count = followers_query_size,
                                            cursor = next_cursor)

            follower_ids = follower_ids + result['ids']
            next_cursor = result['next_cursor']
            ids_count += len(result['ids'])
            # make sure the list only has unique IDs and sort
            follower_ids = list(set(follower_ids))
            follower_ids.sort()
            print("Retrieved %s follower IDs from twitter" % len(follower_ids))
            # store what we've got so far
            # insert follower_ids in the followers collection
            res = db.followers.update_one(
                    {"screen_name": screen_name},
                    { '$set': {"ids": follower_ids} }
                )
            if res.matched_count == 0:
                print("Unable to update IDs for: ",screen_name)
            elif res.modified_count == 0:
                print("%s IDs not modified"% screen_name)
            else:
                print("%s now has %s IDs " %  (screen_name, str(len(follower_ids)))  )

            followers_status(screen_name)
        except TwythonRateLimitError as e:
            # Wait if we hit the Rate limit
            followers_status(screen_name)
            wait_for_awhile()
        except:
            print(" FAILED: Unexpected error:", sys.exc_info()[0])
            pass

    # followers_status(screen_name)
98/1: from hyperopt import fmin
98/2: fmin.get_params()
98/3:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
98/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
98/5:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
98/6:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
list_columns=list(input_files)
98/7:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------

#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/8:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/9:
print("Creating train/test split of data...")
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']

#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/10:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from sklearn.model_selection import TimeSeriesSplit


import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
98/11:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
99/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/RandomForest.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
99/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/RandomForest.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project')
98/12:
# File to save first results
import csv
out_file = 'RF_trials.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write the headers to the file
writer.writerow(['loss', 'params', 'iteration'])
of_connection.close()
98/13:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new best:', best, params)
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/14:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from sklearn.model_selection import TimeSeriesSplit


import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
98/15:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new best:', best, params)
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/16:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.001)
input_files = df[:index]
98/17:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
my_cv = TimeSeriesSplit(n_splits=10)
98/18:
# File to save first results
import csv
out_file = 'RF_trials.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write the headers to the file
writer.writerow(['loss', 'params', 'iteration'])
of_connection.close()
98/19:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new best:', best, params)
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/20:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from sklearn.model_selection import TimeSeriesSplit


import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
98/21:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new best:', best, params)
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/22:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit


import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
98/23:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new best:', best, params)
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/24: best
98/25:
# File to save first results
import csv
out_file = '/Users/shahla/Dropbox/MLCourse/ML1030/Project/RF_trials.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write the headers to the file
writer.writerow(['loss', 'params', 'iteration'])
of_connection.close()
98/26:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new best:', best, params)
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/27:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/28: best
98/29:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=20, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/30: clf = RandomForestClassifier(best)
98/31:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit


import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
98/32:

y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/33:
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/34: clf = RandomForestClassifier(random_state=50, **best)
98/35:
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/36:
clf = RandomForestClassifier(random_state=50, **best)
clf.get_params()
98/37:
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/38: clf.fit(X_train,y_train)
98/39:
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/40:
clf = RandomForestClassifier(random_state=50, **best)
clf.get_params()
98/41:
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/42:
clf = RandomForestClassifier( **best)
clf.get_params()
98/43:
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/44:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.001)
input_files = df[:index]
98/45:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
my_cv = TimeSeriesSplit(n_splits=10)
98/46:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=20, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/47: best
98/48:
clf = RandomForestClassifier( **best)
clf.get_params()
98/49:
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/50:
for a in range(1,20):
    print(a)
98/51:
for a in range(1,20):
    print(type(a)== int)
98/52:
best
#beccause the following error, I read the results from the csv file
results = pd.read_csv('RF_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = True, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
98/53:
best_bayes_model =  RandomForestClassifier(random_state = 50, bootstrap=True,
                                          best_bayes_params
                                          )
best_bayes_model.fit(X_train, y_train)
98/54:
best_bayes_model =  RandomForestClassifier(best_bayes_params, random_state = 50, bootstrap=True,
                                          
                                          )
best_bayes_model.fit(X_train, y_train)
98/55:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50, bootstrap=True,
                                          
                                          )
best_bayes_model.fit(X_train, y_train)
98/56:
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/57:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/58: best['max_depth']
98/59: best_bayes_params
98/60:

#beccause the following error, I read the results from the csv file
results = pd.read_csv('RF_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = False, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
98/61: best_bayes_params
98/62:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50, bootstrap=True,
                                          
                                          )
best_bayes_model.fit(X_train, y_train)
98/63:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
98/64: results.head()
98/65:

#beccause the following error, I read the results from the csv file
results = pd.read_csv('RF_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = True, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
98/66: results.head()
98/67:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"])}
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=100, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/68:
# File to save first results
import csv
out_file = '/Users/shahla/Dropbox/MLCourse/ML1030/Project/RF_trials.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write the headers to the file
writer.writerow(['loss', 'params', 'iteration'])
of_connection.close()
98/69:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
    'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"]),
    'bootstrap': hp.choice('bootstrap', [True, False])
     }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=100, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
98/70:

#beccause the following error, I read the results from the csv file
#https://github.com/hyperopt/hyperopt/issues/431
results = pd.read_csv(path_to_data+'RF_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = True, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
98/71:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50)
best_bayes_model.fit(X_train, y_train)
98/72:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
101/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
101/2:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
101/3:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df1 = pd.read_hdf(path_to_data+'2.h5')
input_files=df1.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
101/4: df1 = pd.read_hdf(path_to_data+'1.h5')
101/5:
f = [df1, df2 ]
df=pd.concat(df1,df2)
101/6:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df2 = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df2.copy()
##For my local computer
df2 = pd.read_hdf(path_to_data+'2.h5')
input_files=df2.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
101/7:
f = [df1, df2 ]
df=pd.concat(df1,df2)
101/8: df.head()
101/9: df_1.head()
101/10: df1.head()
101/11: df2.head()
101/12:
f = [df1, df2 ]
df=df1.append(df2)
101/13: df.head()
101/14: df2.tail()
101/15: df.tail()
101/16:  df.to_hdf('merged-data.h5', key='df', mode='w')
101/17: df.isnull().any().any()
101/18:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
101/19: df = pd.read_hdf(path_to_data+'merged-data.h5')
101/20: df.head()
101/21: df.tail()
101/22: input_files=df.copy()
101/23: clf.get_params()
101/24:
clf = RandomForestClassifier()
clf.get_params()
101/25: df = pd.read_hdf(path_to_data+'1.h5')
101/26: df.sig_label_sec_1==0
101/27: df = df.drop(df[df.sig_label_sec_1 ==0].index)
101/28: df.sig_label_sec_1==0
101/29: sum(df.sig_label_sec_1==0)
101/30: df['sig_label_sec_1'].value_counts()
101/31: df = pd.read_hdf(path_to_data+'1.h5')
101/32: df['sig_label_sec_1'].value_counts()
107/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
107/2:
#---------------------------------------------------------------------------------
##for Kaggle
import os
print(os.listdir("../input"))
path_to_data='../input/'

##for my computer
#path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
df = pd.read_hdf(path_to_data+'merged/merged-data.h5')

##For my local computer
#df = pd.read_hdf(path_to_data+'merged-data.h5')
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
107/3:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'merged/merged-data.h5')

##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
109/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
109/2:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'merged/merged-data.h5')

##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
#-----------------------------------------------
#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#--------------------------------------------

input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
109/3:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.0001)
input_files = df[:index]
109/4:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
my_cv = TimeSeriesSplit(n_splits=10)
109/5: df['sig_label_sec_1'].value_counts()
109/6:
#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
clf=XGBClassifier()
clf.get_params()
109/7:
# File to save first results
import csv
out_file =path_to_data+'XGB_trials.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write the headers to the file
writer.writerow(['loss', 'params', 'iteration'])
of_connection.close()
109/8:

#---------------------------------------------------------------------------------
#---------------------------XGB Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = XGBClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()

param_space = {
        'max_depth':  hp.choice('max_depth', np.arange(4, 17, dtype=int)),
        'min_child_weight': hp.choice('min_child_weight',np.arange(1,11,dtype=int )),
        'subsample': hp.uniform ('x_subsample', 0.5, 1),
        'colsample_bytree' : hp.uniform ('colsample_bytree', 0.5, 1),
        'gamma' : hp.uniform ('x_gamma', 0.1,0.5),
        'colsample_bytree' : hp.uniform ('x_colsample_bytree', 0.7,1),
        'reg_lambda' : hp.uniform ('x_reg_lambda', 0,1),
        'learning_rate' : hp.uniform('learning_rate', .01,.4)
    }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------XGB Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
110/1:

#beccause the following error, I read the results from the csv file
#https://github.com/hyperopt/hyperopt/issues/431
results = pd.read_csv(path_to_data+'XGB_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = True, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
110/2:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
110/3:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
110/4:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'merged/merged-data.h5')

##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
#-----------------------------------------------
#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#--------------------------------------------

input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
110/5:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.0001)
input_files = df[:index]
110/6:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
my_cv = TimeSeriesSplit(n_splits=10)
110/7: df['sig_label_sec_1'].value_counts()
110/8:
#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
clf=XGBClassifier()
clf.get_params()
110/9:
# File to save first results
import csv
out_file =path_to_data+'XGB_trials.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write the headers to the file
writer.writerow(['loss', 'params', 'iteration'])
of_connection.close()
114/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json

# Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
    json.dump(credentials, file)
114/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'learn python',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
114/3:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'adventure travel',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
114/4:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(5)
115/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json

## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
115/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'adventure travel',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/3:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(5)
115/4:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'adventure travel machine learning',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/5:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(5)
115/6:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'machine learning',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/7:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(5)
115/8:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'machine learning lightgbm',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/9:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(5)
115/10:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'job resume machine learning data scientist',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/11:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(5)
115/12:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'machine learning data scientist',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/13:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(5)
115/14:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'data scientist',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/15:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(5)
115/16:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)
115/17:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'black box',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/18:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)
115/19:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'Cohen',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/20:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)
115/21:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results. 
query = {'q': 'Michael Cohen',  
        'result_type': 'popular',
        'count': 10,
        'lang': 'en',
        }
115/22:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)
115/23:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)  
df.text[0]
115/24:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)  
df.text[0][0:100]
115/25:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)  
df.text[0][0:100]
df.text[0][101:200]
115/26:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])

# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)  
df.text[1][0:100]
#df.text[1][101:200]
115/27:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
115/28:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'Michael Cohen',  
        'result_type': 'popular'#mixed recent popular,
        'count': 10,
        'lang': 'en',
        }
115/29:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'Michael Cohen',  
        'result_type': 'popular',#mixed recent popular
        'count': 10,
        'lang': 'en',
        }
115/30:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
115/31:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)
115/32:
# Create a class that inherits TwythonStreamer
class MyStreamer(TwythonStreamer):     

    # Received data
    def on_success(self, data):

        # Only collect tweets in English
        if data['lang'] == 'en':
            tweet_data = process_tweet(data)
            self.save_to_csv(tweet_data)

    # Problem with the API
    def on_error(self, status_code, data):
        print(status_code, data)
        self.disconnect()

    # Save each tweet to csv file
    def save_to_csv(self, tweet):
        with open(r'saved_tweets.csv', 'a') as file:
            writer = csv.writer(file)
            writer.writerow(list(tweet.values()))
115/33:
from twython import TwythonStreamer  
import csv

# Filter out unwanted data
def process_tweet(tweet):  
    d = {}
    d['hashtags'] = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]
    d['text'] = tweet['text']
    d['user'] = tweet['user']['screen_name']
    d['user_loc'] = tweet['user']['location']
    return d
115/34:
# Create a class that inherits TwythonStreamer
class MyStreamer(TwythonStreamer):     

    # Received data
    def on_success(self, data):

        # Only collect tweets in English
        if data['lang'] == 'en':
            tweet_data = process_tweet(data)
            self.save_to_csv(tweet_data)

    # Problem with the API
    def on_error(self, status_code, data):
        print(status_code, data)
        self.disconnect()

    # Save each tweet to csv file
    def save_to_csv(self, tweet):
        with open(r'saved_tweets.csv', 'a') as file:
            writer = csv.writer(file)
            writer.writerow(list(tweet.values()))
115/35:
stream = MyStreamer(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'],  
                    creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])
# Start the stream
stream.statuses.filter(track='Michael Cohen')
115/36:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

#python3 -m spacy download en

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
118/1:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

#python3 -m spacy download en

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
119/1:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

#python3 -m spacy download en

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
120/1:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
120/2:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
120/3:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
121/1:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
121/2:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

python3 -m spacy download en

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
122/1:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
116/1:
wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I)
    doc = doc.lower()
    doc = doc.strip()
    # tokenize document
    tokens = wpt.tokenize(doc)
    # filter stopwords out of document
    filtered_tokens = [token for token in tokens if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(normalize_document)
122/2:
wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I)
    doc = doc.lower()
    doc = doc.strip()
    # tokenize document
    tokens = wpt.tokenize(doc)
    # filter stopwords out of document
    filtered_tokens = [token for token in tokens if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(normalize_document)
122/3:
norm_corpus = normalize_corpus(corpus)
norm_corpus
122/4:
corpus = ['The sky is blue and beautiful.',
          'Love this blue and beautiful sky!',
          'The quick brown fox jumps over the lazy dog.',
          'The brown fox is quick and the blue dog is lazy!',
          'The sky is very blue and the sky is very beautiful today',
          'The dog is lazy but the brown fox is quick!'    
]
norm_corpus = normalize_corpus(corpus)
norm_corpus
122/5: df.text
122/6:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
122/7:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'Michael Cohen',  
        'result_type': 'popular',#mixed recent popular
        'count': 10,
        'lang': 'en',
        }
122/8:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
122/9:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)
122/10: df.text
122/11: normalize_corpus(df.text)
122/12:
from text_normalizer import normalize_corpus
normalize_corpus(df.text)
122/13:
from text_normalizer import normalize_corpus
normalize_corpus(df.text)
122/14:
from text_normalizer import normalize_corpus
normalize_corpus(df.text)
122/15:
from text_normalizer import normalize_corpus
normalize_corpus(df.text)
122/16: df.text[0]
122/17:
pd.set_option('display.max_colwidth', -1)
df.text[0]
122/18:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:40]
122/19:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:60]
122/20:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:70]
122/21:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:100]
122/22:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:100]
df.text[101:200]
122/23:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:100]
122/24: df.text[101:200]
122/25: df.text[101:130]
122/26: df.text[101:120]
122/27: df.text[0][101:120]
122/28: df.text[0][101:130]
122/29: df.text[0][101:200]
122/30: df.text[0][101:300]
122/31:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
122/32: normalize_corpus(df.text)
122/33:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
122/34: x
122/35: x[0]
122/36: df.text[0][100:300]
122/37: df.text[0]
122/38: df.text[1]
122/39: df.text[2]
122/40: df.text[4]
122/41: df.text[5]
122/42: df.text[6]
122/43: df.text[9]
122/44: normalize_corpus(df.text)
122/45:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
122/46:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:100]
122/47: df.text[0][100:300]
122/48: x[0]
122/49: df.text[2]
122/50:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
122/51: x[0]
122/52:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
122/53: x[0]
123/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json

## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
123/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
123/3:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'Michael Cohen',  
        'result_type': 'popular',#mixed recent popular
        'count': 10,
        'lang': 'en',
        }
123/4:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
123/5:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)
123/6:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en #install it in the terminal

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
123/7:
wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I)
    doc = doc.lower()
    doc = doc.strip()
    # tokenize document
    tokens = wpt.tokenize(doc)
    # filter stopwords out of document
    filtered_tokens = [token for token in tokens if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(normalize_document)
123/8:
corpus = ['The sky is blue and beautiful.',
          'Love this blue and beautiful sky!',
          'The quick brown fox jumps over the lazy dog.',
          'The brown fox is quick and the blue dog is lazy!',
          'The sky is very blue and the sky is very beautiful today',
          'The dog is lazy but the brown fox is quick!'    
]
norm_corpus = normalize_corpus(corpus)
norm_corpus
123/9: df.text[2]
123/10: normalize_corpus(df.text)
123/11:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
123/12:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
123/13:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
123/14:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
124/1:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:100]
124/2: df.text[0][100:300]
124/3:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json

## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
124/4:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
124/5:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'Michael Cohen',  
        'result_type': 'popular',#mixed recent popular
        'count': 10,
        'lang': 'en',
        }
124/6:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
124/7:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.head(10)
124/8:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en #install it in the terminal

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
124/9:
wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I)
    doc = doc.lower()
    doc = doc.strip()
    # tokenize document
    tokens = wpt.tokenize(doc)
    # filter stopwords out of document
    filtered_tokens = [token for token in tokens if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(normalize_document)
124/10:
corpus = ['The sky is blue and beautiful.',
          'Love this blue and beautiful sky!',
          'The quick brown fox jumps over the lazy dog.',
          'The brown fox is quick and the blue dog is lazy!',
          'The sky is very blue and the sky is very beautiful today',
          'The dog is lazy but the brown fox is quick!'    
]
norm_corpus = normalize_corpus(corpus)
norm_corpus
124/11: df.text[2]
124/12: normalize_corpus(df.text)
124/13:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
124/14:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:100]
124/15: df.text[0][100:300]
124/16: x[0]
124/17: df.text[0]
124/18: df.text[2]
124/19: df.text[2]
124/20: df.text[9]
124/21: x[9]
124/22: x[7]
124/23: x[66]
124/24: x[10]
124/25: x[9]
124/26:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape()
124/27:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
124/28:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'Michael Cohen',  
        'result_type': 'popular',#mixed recent popular
        'count': 100,
        'lang': 'en',
        }
124/29:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
124/30:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
124/31:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
124/32:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'Michael Cohen',  
        'result_type': 'popular',#mixed recent popular
        'count': 100,
        'lang': 'en',
        }
124/33:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
124/34:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
124/35:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'python',  
        'result_type': 'popular',#mixed recent popular
        'count': 100,
        'lang': 'en',
        }
124/36:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
124/37:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
124/38:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
124/39:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
124/40:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'food',  
        'result_type': 'popular',#mixed recent popular
        'count': 100,
        'lang': 'en',
        }
124/41:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
124/42:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
124/43:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'food',  
        'result_type': 'popular',#mixed recent popular
        'count': 200,
        'lang': 'en',
        }
124/44:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
124/45:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
124/46:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'trump',  
        'result_type': 'popular',#mixed recent popular
        'count': 200,
        'lang': 'en',
        }
124/47:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
124/48:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
125/1:
import pandas as pd
import numpy as np
import yaml 
import itertools
125/2:
#PATH is path to yaml file
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 
        self.y_train=data['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   
        self.data.append(self.y_train)

        return self.data   
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('C:/MLClass/ML1030/testv.yaml')   
x=r.filter_data()
y=r.load_data()
tp = TimeSeriesSplit(n_splits=10)   

model=r.build_model()

#tuning knn
k_range = range(1, 20)
scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors = k)
    nested_score = cross_val_score(knn, X=x[1][1], y=x[2],cv=tp,scoring='accuracy')
    scores.append(nested_score.mean())

knn = KNeighborsClassifier()


log = LogisticRegression(random_state=1)
log = LogisticRegression(random_state=1,solver='lbfgs',penalty='l2',C= 0.00045749926937954655,multi_class='multinomial')
nested_score = cross_val_score(log, X=x[0], y=x[12],cv=tp,scoring='accuracy')
nested_score.mean()


lda = LinearDiscriminantAnalysis()
gnb = GaussianNB()


clf = DecisionTreeClassifier()


svm = SVC()
mlp=MLPClassifier()

#adaboost tuning
ab=AdaBoostClassifier()


ada_p_dist={'learning_rate':[0.25,0.5,0.75,1.],
            'n_estimators':[1,2,3,4,5,6,7,8,9,10]
            }

best_ada = RandomizedSearchCV(ab, ada_p_dist, random_state=1, n_iter=100, cv=10, verbose=0, n_jobs=-1)
best_ada = best_ada.fit(x[1][1], x[2])
print('Best learning_rate:', best_ada.best_estimator_.get_params()['learning_rate'])
print('Best n_estimators:', best_ada.best_estimator_.get_params()['n_estimators'])

ada=AdaBoostClassifier(learning_rate=0.75,n_estimators=4)
nested_score = cross_val_score(ada, X=x[1][1], y=x[2],cv=tp,scoring='accuracy')
nested_score.mean()


#tuning log regression
penalty = ['l1', 'l2']
# Create regularization hyperparameter distribution using uniform distribution
C = uniform(loc=0, scale=4)

# Create hyperparameter options
hyperparameters = dict(C=C, penalty=penalty)

clf = RandomizedSearchCV(log, hyperparameters, random_state=1, n_iter=100, cv=10, verbose=0, n_jobs=-1)
best_model = clf.fit(x[1][1], x[2])

print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])
print('Best C:', best_model.best_estimator_.get_params()['C'])


nested_score = cross_val_score(model[0], X=x[1][1], y=x[2],cv=tp,scoring='accuracy')
nested_score.mean()

#tune random forest
rf=RandomForestClassifier()
random_grid={'bootstrap': [True],
 'max_depth': [20],
 'max_features': ['auto'],
 'min_samples_leaf': [41],
 'min_samples_split': [2],
 'n_estimators': [257]}


rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(x[0][0], x[2])
rf_random.best_params_


rfc = RandomForestClassifier(bootstrap=True,
            max_depth=20, max_features='auto',
            min_samples_leaf=41, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=257,
            random_state=0)


rfc.fit(x[0][0], x[2])
nested_score = cross_val_score(rfc, X=x[0][0], y=x[2],cv=tp,scoring='accuracy')
nested_score.mean()







list(itertools.chain.from_iterable(x2))   

x2=[x[0],x[1]]
125/3:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
125/4:
from sklearn.base import clone
LabelEncoder().clone
125/5:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
125/6:
##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
125/7:
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y, validation_data=None):
        self.model.fit(X,y)
        return None

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:,1]

    def clone(self):
        self.model = clone(self.model)
        return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
125/8:
params=[bootstrap=False, class_weight=None,
            criterion='entropy', max_depth=8, max_features=11,
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=431, n_jobs=None, oob_score=False,
            random_state=50, verbose=0, warm_start=False]
clf=randomforestClassifier(params)
125/9:
params=[bootstrap=False, class_weight=None,
            criterion='entropy', max_depth=8, max_features=11,
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=431, n_jobs=None, oob_score=False,
            random_state=50, verbose=0, warm_start=False]
#clf=randomforestClassifier(params)
125/10:
params=[bootstrap=False, 
           class_weight=None,
            criterion='entropy', 
        max_depth=8,
        max_features=11,
            max_leaf_nodes=None, 
        min_impurity_decrease=0.0,
            min_impurity_split=None,
        min_samples_leaf=1,
            min_samples_split=2, 
        min_weight_fraction_leaf=0.0,
            n_estimators=431,
        n_jobs=None, 
            random_state=50
        ]
#clf=randomforestClassifier(params)
125/11:
params=[#bootstrap=False, 
           class_weight=None,
            criterion='entropy', 
        max_depth=8,
        max_features=11,
            max_leaf_nodes=None, 
        min_impurity_decrease=0.0,
            min_impurity_split=None,
        min_samples_leaf=1,
            min_samples_split=2, 
        min_weight_fraction_leaf=0.0,
            n_estimators=431,
        n_jobs=None, 
            random_state=50
        ]
#clf=randomforestClassifier(params)
125/12:
params=[#bootstrap=False, 
          # class_weight=None,
            criterion='entropy', 
        max_depth=8,
        max_features=11,
            max_leaf_nodes=None, 
        min_impurity_decrease=0.0,
            min_impurity_split=None,
        min_samples_leaf=1,
            min_samples_split=2, 
        min_weight_fraction_leaf=0.0,
            n_estimators=431,
        n_jobs=None, 
            random_state=50
        ]
#clf=randomforestClassifier(params)
125/13:
params=[#bootstrap=False, 
          # class_weight=None,
           # criterion='entropy', 
        max_depth=8,
        max_features=11,
            max_leaf_nodes=None, 
        min_impurity_decrease=0.0,
            min_impurity_split=None,
        min_samples_leaf=1,
            min_samples_split=2, 
        min_weight_fraction_leaf=0.0,
            n_estimators=431,
        n_jobs=None, 
            random_state=50
        ]
#clf=randomforestClassifier(params)
125/14:
params= {#bootstrap=False, 
          # class_weight=None,
            criterion='entropy', 
        max_depth=8,
        max_features=11,
            max_leaf_nodes=None, 
        min_impurity_decrease=0.0,
            min_impurity_split=None,
        min_samples_leaf=1,
            min_samples_split=2, 
        min_weight_fraction_leaf=0.0,
            n_estimators=431,
        n_jobs=None, 
            random_state=50
}
#clf=randomforestClassifier(params)
125/15:
params= {#bootstrap=False, 
          # class_weight=None,
           # criterion='entropy', 
        max_depth=8,
        max_features=11,
            max_leaf_nodes=None, 
        min_impurity_decrease=0.0,
            min_impurity_split=None,
        min_samples_leaf=1,
            min_samples_split=2, 
        min_weight_fraction_leaf=0.0,
            n_estimators=431,
        n_jobs=None, 
            random_state=50
}
#clf=randomforestClassifier(params)
125/16:
params= {#bootstrap=False, 
          # class_weight=None,
           # criterion='entropy', 
       # max_depth=8,
       # max_features=11,
       #     max_leaf_nodes=None, 
        min_impurity_decrease=0.0,
            min_impurity_split=None,
        min_samples_leaf=1,
            min_samples_split=2, 
        min_weight_fraction_leaf=0.0,
            n_estimators=431,
        n_jobs=None, 
            random_state=50
}
#clf=randomforestClassifier(params)
125/17:
params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs'=None, 
            'random_state':50
       }
#clf=randomforestClassifier(params)
125/18:
params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
#clf=randomforestClassifier(params)
125/19:
params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
clf=randomforestClassifier(**params)
125/20:
params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
clf=RandomForestClassifier(**params)
125/21:

model_set = {
    "rf": {
        "clf": [RandomForestClassifierWrapper(**rf_params)],
        "run": False #run the model or fit the saved model
    }
}#end of model set
125/22:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
clf=RandomForestClassifier(**params)
125/23:

model_set = {
    "rf": {
        "clf": [RandomForestClassifierWrapper(**rf_params)],
        "run": False #run the model or fit the saved model
    }
}#end of model set
125/24:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
125/25:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
125/26:
def acc_model(classifier):
    return cross_val_score(classifier, X_train, y_train,cv=my_cv).mean()
125/27:

model_set = {
    "rf": {
        "clf": [RandomForestClassifierWrapper()],
        "params": rf_params
        "run": False #run the model or fit the saved model
    }
}#end of model set
125/28:

model_set = {
    "rf": {
        "clf": [RandomForestClassifierWrapper()],
        "params": rf_params,
        "run": False #run the model or fit the saved model
    }
}#end of model set
125/29:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.001)
input_files = df[:index]
125/30:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
125/31: from sklearn.base import clone
125/32:
import pandas as pd
import numpy as np
import yaml 
import itertools
125/33:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
125/34:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
125/35:
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
125/36:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.001)
input_files = df[:index]
125/37:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
125/38:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:,1]

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
125/39:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
125/40:

model_set = {
    "rf": {
        "clf": [RandomForestClassifierWrapper(**rf_params)],
        "run": False #run the model or fit the saved model
    }
}#end of model set
125/41:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save_models(saved_folder)

print("Done!")
125/42: tc
125/43: model_set
125/44:

model_set = {
    "rf": {
        "clf": [RandomForestClassifierWrapper(**rf_params)],
        "run": True #run the model or fit the saved model
    }
}#end of model set
125/45:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save_models(saved_folder)

print("Done!")
125/46:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
125/47:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save_models(saved_folder)

print("Done!")
125/48:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
125/49:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save_models(saved_folder)

print("Done!")
125/50:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
125/51: tc
125/52:
"""
how to load saved models
tc2 = TextClassifier(vectorizerList=[vec], classifierList=[clf])
tc2.load_models("../saved_models/cntvecnb_lr")
pred = tc2.predict(X)
print(metrics.f1_score(y, pred>0.5))
"""
125/53:
"""
how to load saved models
tc2 = TextClassifier(vectorizerList=[vec], classifierList=[clf])
tc2.load_models("../saved_models/cntvecnb_lr")
pred = tc2.predict(X)
print(metrics.f1_score(y, pred>0.5))

"""
127/1:

sig_lookback_periods = [ "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s", "12s", "18s", "21s" ]

inv_sig_lookback_periods = [
        "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
        "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
    ]

p = ["40", "30", "20"]

signals = {
        "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
        "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
        "CCI": {"lookback_periods": sig_lookback_periods},
        "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
        "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "CMO": {"lookback_periods": sig_lookback_periods},
        "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
        "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
        "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
        "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
        "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
      }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                 **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}

#logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')
127/2: signal[0]
127/3: signal[MOVING_AVG]
127/4: signal["MOVING_AVG"]
127/5: signals["MOVING_AVG"]
127/6: 'Moving_AVE'+signals["MOVING_AVG"]
127/7: signals['ti']+signals["MOVING_AVG"]
127/8: ti +signals["MOVING_AVG"]
127/9: signals[ti["MOVING_AVE"]]
127/10: signals[ti[0]]
125/54:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    pred = tc.predict(X)
    print(metrics.f1_score(y, pred>0.5))
print("Done!")
125/55:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    pred = tc.predict_proba(X)
    print(metrics.f1_score(y, pred>0.5))
print("Done!")
125/56:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    pred = tc.predict_proba(X_test)
    print(metrics.f1_score(y_test, pred>0.5))
print("Done!")
125/57: pred
125/58: pred.shape
125/59:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict_proba(X)[:,1]

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
125/60:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    pred = tc.predict(X_test)
    print(metrics.f1_score(y_test, pred>0.5))
print("Done!")
125/61:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict_proba(X)[:,1]

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
125/62:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    pred = tc.predict(X_test)
    print(metrics.f1_score(y_test, pred>0.5))
print("Done!")
125/63:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
125/64:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
125/65:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    pred = tc.predict(X_test)
    print(metrics.f1_score(y_test, pred>0.5))
print("Done!")
125/66:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    pred = tc.predict(X_test)
   # print(metrics.f1_score(y_test, pred>0.5))
print("Done!")
125/67: pred.shape
125/68:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    pred = tc.predict(X_test)
    print(metrics.accuracy_score(y_test, pred))
print("Done!")
125/69: pred
125/70:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
125/71: y_pred
129/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
129/2: from sklearn.base import clone
129/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
129/4:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
129/5:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
129/6:
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
129/7:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
129/8:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
129/9: y_test.dtypes
129/10:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict_proba(X)[:,1]

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
129/11:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
129/12:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
129/13:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
129/14:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
129/15: y_pred
129/16:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
129/17:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
129/18: y_pred
129/19:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict_proba(X)[:,1]

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
129/20:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
129/21:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
129/22: y_pred
130/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
130/2: from sklearn.base import clone
130/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
130/4:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
130/5:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
130/6:
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
130/7:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
130/8: y_test.dtypes
130/9:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)[:,1]

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
130/10:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
130/11:
def acc_model(classifier):
    return cross_val_score(classifier, X_train, y_train,cv=my_cv).mean()
130/12:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
130/13:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
130/14:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
130/15: y_test.dtypes
130/16:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
130/17: y_pred
130/18:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    #print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
130/19:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
130/20:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
130/21:
def acc_model(classifier):
    return cross_val_score(classifier, X_train, y_train,cv=my_cv).mean()
130/22:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
130/23:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
130/24:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    #print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
130/25:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
130/26: pred.shape
130/27: y_pred
130/28: df.columns
130/29: df.columns[0:10]
135/1:
list_columns=list(input_files)
list_columns
135/2:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
135/3: from sklearn.base import clone
135/4:
import pandas as pd
import numpy as np
import yaml 
import itertools
135/5:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
135/6:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
135/7:
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
135/8:
list_columns=list(input_files)
list_columns
135/9:
#we have 11 sets of features is created in features.py
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = raw_data[lf].copy()
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = raw_data[lf].copy()



    X["MOVING_AVG"]



    X["INV_STOCH"]
135/10:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
135/11:
#we have 11 sets of features is created in features.py
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = raw_data[lf].copy()
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = raw_data[lf].copy()



    X["MOVING_AVG"]



    X["INV_STOCH"]
135/12:
#we have 11 sets of features is created in features.py
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = df[lf].copy()
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = df[lf].copy()



    X["MOVING_AVG"]



    X["INV_STOCH"]
135/13:
#we have 11 sets of features is created in features.py
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = df[lf].copy()
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = df[lf].copy()



X["MOVING_AVG"]



X["INV_STOCH"]
135/14:

X["MOVING_AVG"]



X["INV_STOCH"]
135/15: X.keys
135/16: X
135/17: list(X)
135/18:
list(X)

for feature_set and X_train in features_dic.items():
    print(feature_set)
135/19:
list(X)

for feature_set, X_train in features_dic.items():
    print(feature_set)
135/20:
list(X)
feature_dic=X
for feature_set, X_train in features_dic.items():
    print(feature_set)
135/21:
list(X)
features_dic=X
for feature_set, X_train in features_dic.items():
    print(feature_set)
135/22: lf
135/23:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf
135/24: lf
135/25: X
135/26:

X["MOVING_AVG"]



X["INV_STOCH"]
135/27:
for feature, feature_set in features_dic.items():
    #We need to run all the models in this for loop for all set of features. for the time being 
    #we do not use it. but later we put every thing here.
    print(feature_set)
135/28:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic=X #dictionary of feature sets to be used for each model
135/29:
for feature, feature_set in features_dic.items():
    #We need to run all the models in this for loop for all set of features. for the time being 
    #we do not use it. but later we put every thing here.
    print(feature_set)
135/30:
for feature, feature_set in features_dic.items():
    #We need to run all the models in this for loop for all set of features. for the time being 
    #we do not use it. but later we put every thing here.
    print(feature,'=',feature_set)
135/31:
#for feature, feature_set in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.

    #X_train = input_files_train_contracts[feature_set]
    #X_test=input_files_test_contracts[feature_set]
    ##the rest of the code below will be here!
135/32:
for feature, feature_set in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)
    #X_train = input_files_train_contracts[feature_set]
    #X_test = input_files_test_contracts[feature_set]
    ##the rest of the code below will be here!
135/33:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic['features_dic']
135/34:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
135/35:
for feature, feature_set in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)
    #X_train = input_files_train_contracts[feature_set]
    #X_test = input_files_test_contracts[feature_set]
    ##the rest of the code below will be here!
135/36:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic['features_dic']
135/37:

sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
135/38:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic[feature]
135/39:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic[feature]
feature_set
135/40:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic[feature]
X_train = input_files_train_contracts[feature_set]
X_test = input_files_test_contracts[feature_set]
136/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
136/2: from sklearn.base import clone
136/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
136/4:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
136/5:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
136/6:
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
136/7:
list_columns=list(input_files)
list_columns
136/8:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
136/9:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
136/10:
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
136/11:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
136/12:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
136/13:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic[feature]
X_train = input_files_train_contracts[feature_set]
X_test = input_files_test_contracts[feature_set]
136/14:
list_columns=list(input_files)
#list_columns
136/15:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic[feature]
X_train = input_files_train_contracts[feature_set]
X_test = input_files_test_contracts[feature_set]
136/16:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
136/17:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
136/18:
model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
136/19:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name 
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'feature'+".model")

print("Done!")
136/20:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name 
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+feature+".model")

print("Done!")
136/21:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name 
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")

print("Done!")
136/22:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")

print("Done!")
136/23:
#creating directories we need, if they do not exsit already
import os.path
if not os.path.exists('/tmp/newdir'):
    os.makedirs('/tmp/newdir')
136/24:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True)
136/25:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True)
136/26:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")

print("Done!")
136/27:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
136/28:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")

print("Done!")
136/29:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+'_'+feature+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
136/30:
#We do not need this cell when everything goes to the for loop above
#feature = 'MOVING_AVG'
feature = 'CCI'

feature_set = features_dic[feature]
X_train = input_files_train_contracts[feature_set]
X_test = input_files_test_contracts[feature_set]
136/31:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
136/32:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+'_'+feature+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
136/33:
#We do not need this cell when everything goes to the for loop above
#feature = 'MOVING_AVG'
feature = 'CCI'

feature_cols = features_dic[feature]
X_train = input_files_train_contracts[feature_cols]
X_test = input_files_test_contracts[feature_cols]
136/34:
for feature, feature_cols in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)

    ##the rest of the codes below will be here!
136/35:

print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_df = input_files_train_contracts.copy()
136/36:
y_train=sig_df['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
136/37:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols)
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
136/38:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
136/39:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feat_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
136/40:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
136/41:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feat_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
136/42:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__(self)
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
136/43:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feat_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
136/44:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
136/45:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def savve(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
136/46:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.predict(sig_df, feat_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.savve(saved_folder+'_'+feature+".model")
     
    
print("Done!")
136/47:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.predict(sig_df, feature_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.savve(saved_folder+'_'+feature+".model")
     
    
print("Done!")
136/48:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    #tc.predict(sig_df, feature_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.savve(saved_folder+'_'+feature+".model")
     
    
print("Done!")
136/49:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def savve(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         super().__init__()
    self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
136/50:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def savve(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         super().__init__()
         self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
136/51:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def savve(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )
            super().__init__()


#---------------------------------------------------------------------------------
136/52:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def savve(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )
        super().__init__()


#---------------------------------------------------------------------------------
136/54:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def savve(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )
         super().__init__()


#---------------------------------------------------------------------------------
136/55:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feature_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
136/56:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )
        super().__init__(self)


#---------------------------------------------------------------------------------
136/58:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight)
         super().__init__(self)


#---------------------------------------------------------------------------------
136/59:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feature_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
137/1:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight)
         super().__init__(self)


#---------------------------------------------------------------------------------
137/2:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
137/3: from sklearn.base import clone
137/4:
import pandas as pd
import numpy as np
import yaml 
import itertools
137/5:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
137/6:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True)
137/7:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight)
         super().__init__(self)


#---------------------------------------------------------------------------------
137/8:
#we need to creat a dictionary
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
137/9:
model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
137/10:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None

    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
         super().__init__(self)
         self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight)
         


#---------------------------------------------------------------------------------
137/11:
#we need to creat a dictionary
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
137/12:
model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
137/13:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
137/14:
#we need to creat a dictionary
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
137/15:
model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
137/16:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def savve(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
137/17:
#we need to creat a dictionary
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
137/18:
model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
137/19:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
137/20:
#handling the data: we need to check if the data is for predicting or for modeling. ...
#--------------------------under construction!!!
modeling = True #if we want to use the data for modeling and saving models, else 

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
137/21:
list_columns=list(input_files)
#list_columns
137/22:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
137/23:

print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_df = input_files_train_contracts.copy() #sig_df is the whole processed data frame with 
                                            #all features(excluding the test data)
137/24:
y_train=sig_df['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
137/25:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
137/26:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
137/27:
for feature, feature_cols in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)

    ##the rest of the codes below will be here!
137/28:
#We do not need this cell when everything goes to the for loop above
#feature = 'MOVING_AVG'
feature = 'CCI'

feature_cols = features_dic[feature]
X_train = sig_df[feature_cols]
X_test = input_files_test_contracts[feature_cols]
137/29:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feature_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
137/30:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.fit(sig_df, feature_cols)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
137/31:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    #tc.train(sig_df, feature_cols)
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
137/32:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    #tc.train(sig_df, feature_cols)
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.savve(saved_folder+'_'+feature+".model")
     
    
print("Done!")
137/33:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
137/34:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
137/35:
model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
137/36:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
137/37:
#rf_MOVING_AVG is for the parameters of RandomForestClassifier obtained by tuning of feature columns MOVING_AVG
model_set = {
    "rf_MOVING_AVG": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
137/38:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
137/39:
#We do not need this cell when everything goes to the for loop above
#feature = 'MOVING_AVG'
feature = 'MOVING_AVG'

feature_cols = features_dic[feature]
X_train = sig_df[feature_cols]
X_test = input_files_test_contracts[feature_cols]
137/40:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
139/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
139/2: from sklearn.base import clone
139/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
139/4:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
139/5:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True)
139/6:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
139/7:
#we need to creat a dictionary
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
139/8:
#rf_MOVING_AVG is for the parameters of RandomForestClassifier obtained by tuning of feature columns MOVING_AVG
model_set = {
    "rf_MOVING_AVG": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/9:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
139/10:
#handling the data: we need to check if the data is for predicting or for modeling. ...
#--------------------------under construction!!!
modeling = True #if we want to use the data for modeling and saving models, else 

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
139/11:
list_columns=list(input_files)
#list_columns
139/12:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
139/13:

print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_df = input_files_train_contracts.copy() #sig_df is the whole processed data frame with 
                                            #all features(excluding the test data)
139/14:
y_train=sig_df['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
139/15:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
139/16:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
139/17:
for feature, feature_cols in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)

    ##the rest of the codes below will be here!
139/18:
#We do not need this cell when everything goes to the for loop above
#feature = 'MOVING_AVG'
feature = 'MOVING_AVG'

feature_cols = features_dic[feature]
X_train = sig_df[feature_cols]
X_test = input_files_test_contracts[feature_cols]
139/19:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
139/20:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+'_'+feature+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
139/21: tc.model
139/22:
#we need to creat a dictionary
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }

rf_params2={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':430,
        'n_jobs':None, 
            'random_state':50
       }
139/23:
#rf_MOVING_AVG is for the parameters of RandomForestClassifier obtained by tuning of feature columns MOVING_AVG
model_set = {
    "rf_MOVING_AVG": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
    "rf_MOVING_AVG2": {
        "clf": RandomForestClassifierWrapper(**rf_params2),
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/24:
#rf_MOVING_AVG is for the parameters of RandomForestClassifier obtained by tuning of feature columns MOVING_AVG
model_set = {
    "rf_MOVING_AVG": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    },
    "rf_MOVING_AVG2": {
        "clf": RandomForestClassifierWrapper(**rf_params2),
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/25:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+'_'+feature+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
139/26:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" 
    tc = v["clf"]
    tc.load(saved_folder+'_'+feature+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
139/27:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    tc = v["clf"]
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
139/28: tc.model
139/29:
#rf_MOVING_AVG is for the parameters of RandomForestClassifier obtained by tuning of feature columns MOVING_AVG
model_set = {
    "rf_MOVING_AVG": {
        "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
        ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/30:
#rf_MOVING_AVG is for the parameters of RandomForestClassifier obtained by tuning of feature columns MOVING_AVG
model_set = {
    "rf_MOVING_AVG": {
        "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/31:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    clf = v["clf"]
    tc = clf(**v[params])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
139/32:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    clf = v["clf"]
    tc = clf(**v['params'])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
139/33: tc.model
139/34: tc.model(X_test,y_test, rf_params)
139/35: tc.model.fit(X_test,y_test, rf_params)
139/36: tc.model.fit(X_test,y_test, **rf_params)
139/37: clf=RandomForestClassifier()
139/38:
clf=RandomForestClassifier()
clf.fit(X_test,y_test,**rf_params)
139/39:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" + model_name 
    clf = v["clf"]
    clf.params = v['params']
    clf.model.set_params(**v['params'])
    #tc = clf(**v['params'])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
139/40:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/"  
    clf = v["clf"]
    clf.params = v['params']
    clf.model.set_params(**v['params'])
    #tc = clf(**v['params'])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+feature+".model")
     
    
print("Done!")
139/41:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/"  
    clf = v["clf"]
    clf.params = v['params']
    clf.model.set_params(**v['params'])
    #tc = clf(**v['params'])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+model_name+'_'+feature+".model")
     
    
print("Done!")
139/42:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/" 
    tc = v["clf"]
    tc.load(saved_folder+'_'+feature+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
139/43: feature
139/44:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/"  
    clf = v["clf"]
    clf.params = v['params']
    clf.model.set_params(**v['params'])
    #tc = clf(**v['params'])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+'_'+model_name+".model")
     
    
print("Done!")
139/45:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/"  
    clf = v["clf"]
    clf.params = v['params']
    clf.model.set_params(**v['params'])
    #tc = clf(**v['params'])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+model_name+".model")
     
    
print("Done!")
139/46:
for k, v in model_set.items(): #k=model name, v[clf]=classifier name
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"saved_models/"  
    clf = v["clf"]
    clf.params = v['params']
    clf.model.set_params(**v['params'])
    #tc = clf(**v['params'])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+model_name+".model")
     
    
print("Done!")
139/47: features_dic
139/48: features_dic["MOVING_AVG"]
139/49: features_dic["MOVING_AVG"].append(1)
139/50: features_dic["MOVING_AVG"]
139/51: features_dic["MOVING_AVG"]["gg"]='ff'
139/52: features_dic.update(params)
139/53: features_dic.update(rf_params)
139/54: features_dic
139/55: features_dic["MOVING_AVG"].append(rf_params)
139/56: features_dic
139/57: features_dic["MOVING_AVG"]
139/58: features_dic["MOVING_AVG"].update(rf_params)
139/59:
g={}
for 
for feature, feature_cols in features_dic.items():
    g[feature]={feature_cols, "rf": rf_params}
139/60:
g={}
for 
for feature, feature_cols in features_dic.items():
    print('d')
139/61:
g={}

for feature, feature_cols in features_dic.items():
    g[feature] = {feature_cols, "rf": rf_params}
139/62:
numbers = dict(x=5, y=0)
print('numbers = ',numbers)
print(type(numbers))
139/63:
g={}

for feature, feature_cols in features_dic.items():
    g[feature] = {feature_cols, dict(rf_params = rf_params)}
139/64:
g={}

for feature, feature_cols in features_dic.items():
    g[feature] = {feature_cols, dict(rf_param = rf_params)}
139/65:
g={}

for feature, feature_cols in features_dic.items():
    g[feature] = {feature_cols, dict(rf_p = rf_params)}
139/66:
g={}

for feature, feature_cols in features_dic.items():
    g[feature] = dict(fcol = feature_cols, rf_p = rf_params)}
139/67:
g={}

for feature, feature_cols in features_dic.items():
    g[feature] = dict(fcol = feature_cols, rf_p = rf_params)
139/68: g
139/69:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
139/70: features_dic
139/71:
g={}

for feature, feature_cols in features_dic.items():
    g[feature] = dict(fcol = feature_cols, rf_p = rf_params)
139/72: g
139/73:
#we need to creat a dictionary
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }

model_set = {
    "RF":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/74:
#we need to creat a dictionary
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }

model_set = {
    "RF":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CNN":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/75:
g={}

for feature, feature_cols in features_dic.items():
    g[feature] = dict(fcol = feature_cols, clf_param = rf_params)
139/76: g
139/77:
g={}

for feature, feature_cols in features_dic.items():
    g[feature] = dict(fcol = feature_cols, clf_param = model_set)
139/78: g
139/79:
#we need to creat a dictionary
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }

model_set = {
    "RF":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CNN": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/80:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set)
139/81:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols[feature], clf_param = model_set)
139/82:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set[feature])
139/83:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

model_set = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CNN": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/84:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set[feature])
139/85:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

model_set = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/86:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set[feature])
139/87:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

model_set = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 1,
   'CMO': 2,
   'STOCH':3,
   'BBI_2':4,
   'BBI_3':4 ,
   'INV_MOVING_AVG':4,
   'INV_CCI':4,
   'INV_RSI':4,
   'INV_CMO':4,
   'INV_STOCH':4
}#end of model set
139/88:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

model_set = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 1,
   'CMO': 2,
   'STOCH':3,
   'BBI_2':4,
   'BBI_3':4 ,
   'INV_MOVING_AVG':4,
   'INV_CCI':4,
   'INV_RSI':4,
   'INV_CMO':4,
   'INV_STOCH':4
}#end of model set
139/89:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set[feature])
139/90: g
139/91: g
139/92:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set[feature])
139/93: g
139/94: dic
139/95:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

model_set = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 1,#needs to be changed
   'CMO': 2,
   'STOCH':3,
   'BBI_2':4,
   'BBI_3':4 ,
   'INV_MOVING_AVG':4,
   'INV_CCI':4,
   'INV_RSI':4,
   'INV_CMO':4,
   'INV_STOCH':4
}#end of model set
139/96:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set[feature])
139/97: dic
139/98:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

model_set = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO': 2,
   'STOCH':3,
   'BBI_2':4,
   'BBI_3':4 ,
   'INV_MOVING_AVG':4,
   'INV_CCI':4,
   'INV_RSI':4,
   'INV_CMO':4,
   'INV_STOCH':4
}#end of model set
139/99:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set[feature])
139/100: dic
139/101:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

model_set = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/102:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set[feature])
139/103: dic
139/104:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(fcol = feature_cols, clf_param = model_set[feature])
    dic[feature][fcol]
139/105:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_cols in features_dic.items():
    dic[feature] = dict(feature_cols = feature_cols, clf_param = model_set[feature])
    #dic[feature]['fcol']
139/106: dic
139/107:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = model_set[feature])
    feature_cols = dic[feature]['feature_cols']
139/108: feature_cols
139/109: dic
139/110:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = model_set[feature])
    feature_cols = dic[feature]['feature_cols']
    clf = dic(clf_param)['clf']
139/111:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = model_set[feature])
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['clf_params']
139/112:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = model_set[feature])
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['clf_param']
139/113: clf
139/114:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = model_set[feature])
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['clf_param'][clf]
139/115:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = model_set[feature])
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['clf_param']['clf']
139/116: clf
139/117: dic
139/118:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = model_set[feature])
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['clf_param']['clf']
    params = dic[feature]['clf_param']['params']
139/119: params
139/120:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

model_set = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/121:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = model_set[feature])
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['clf_param']['clf'] #classifier
    params = dic[feature]['clf_param']['params'] #parameter of the classfifier
139/122: params
139/123: dic
139/124:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

RF_set = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/125:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = RF_set[feature
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['clf_param']['clf'] #classifier
    params = dic[feature]['clf_param']['params'] #parameter of the classfifier
139/126:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}

for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, clf_param = RF_set[feature])
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['clf_param']['clf'] #classifier
    params = dic[feature]['clf_param']['params'] #parameter of the classfifier
139/127: dic
139/128: dic
139/129:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, RF_param = RF_set[feature])
    #--------------------------------------------------------------------------
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['RF_param']['clf'] #classifier
    params = dic[feature]['RF_param']['params'] #parameter of the classfifier
139/130:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, RF = RF_set[feature])
    #--------------------------------------------------------------------------
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['RF']['clf'] #classifier
    params = dic[feature]['RF']['params'] #parameter of the classfifier
139/131:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

RF = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of model set
139/132:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, RF = RF[feature])
    #--------------------------------------------------------------------------
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['RF']['clf'] #classifier
    params = dic[feature]['RF']['params'] #parameter of the classfifier
139/133:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

RF = {
    "MOVING_AVG":
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of RF
#----------------------------------------------------------
139/134:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, RF = RF[feature])
    #--------------------------------------------------------------------------
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['RF']['clf'] #classifier
    params = dic[feature]['RF']['params'] #parameter of the classfifier
139/135: dic
139/136: dic['RF']['run']
139/137: dic
139/138: dic['RF']
139/139: dic
139/140:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, RF = RF[feature])
    #--------------------------------------------------------------------------
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['RF']['clf'] #classifier
    params = dic[feature]['RF']['params'] #parameter of the classfifier
    run = dic[feature]['RF']['run']
139/141: run
139/142:
if run is False:
    continue
model_name = k
    saved_folder = path+"saved_models/"  
    clf = v["clf"]
    clf.params = v['params']
    clf.model.set_params(**params)
    #tc = clf(**v['params'])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+model_name+".model")
     
    
print("Done!")
139/143:
for key and v in dic[feature].items():
 print(key)
139/144:
for key, v in dic[feature].items():
 print(key)
139/145:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

RF = {
    "MOVING_AVG":{'model':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'RF':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of RF
#----------------------------------------------------------
139/146: dic
139/147: RF['MOVING_AVG']['model'][0]
139/148: RF['MOVING_AVG']['model']
139/149: list(RF['MOVING_AVG']['model'])
139/150: list(RF['MOVING_AVG']['model'])[2]
139/151: list(RF['MOVING_AVG']['model'])[3]
139/152: list(RF['MOVING_AVG']['model'])[1]
139/153: RF['MOVING_AVG']
139/154: list(RF['MOVING_AVG']['model'])
139/155:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

RF = {
    "MOVING_AVG":{'model':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'RF':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
                  
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'blahblah':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
    },
   #----------------------------------------------------------------------------------
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of RF
#----------------------------------------------------------
139/156:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

RF = {
    "MOVING_AVG":
    {'model':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'RF':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50},
       # ************
        "run": True #run the model or fit the saved model
     },         
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'blahblah':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    },
   #----------------------------------------------------------------------------------
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of RF
#----------------------------------------------------------
139/157:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

RF = {
    "MOVING_AVG":
    {'model': 'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    },
   #----------------------------------------------------------------------------------
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of RF
#----------------------------------------------------------
139/158:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods

RF = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of RF
#----------------------------------------------------------
139/159: list(RF['MOVING_AVG']['model'])
139/160: RF['MOVING_AVG']
139/161:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI": #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of RF
#----------------------------------------------------------
139/162: list(model_params['MOVING_AVG']['model'])
139/163: model_params['MOVING_AVG']
139/164:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, model_params = model_params[feature])
    #--------------------------------------------------------------------------
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['model']['clf'] #classifier
    model_name=list(RF['MOVING_AVG']['model'])[1]
    params = dic[feature]['model'][model_name] #parameter of the classfifier
    run = dic[feature]['model']['run']
139/165:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, model_params = model_params[feature])
139/166: dic
139/167:
    for for feature, _ in dic.items():
    model_params['MOVING_AVG']['model']
    #--------------------------------------------------------------------------
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['model']['clf'] #classifier
    model_name=list(RF['MOVING_AVG']['model'])[1]
    params = dic[feature]['model'][model_name] #parameter of the classfifier
    run = dic[feature]['model']['run']
139/168:
for for feature, _ in dic.items():
    model_params['MOVING_AVG']['model']
    #--------------------------------------------------------------------------
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['model']['clf'] #classifier
    model_name=list(RF['MOVING_AVG']['model'])[1]
    params = dic[feature]['model'][model_name] #parameter of the classfifier
    run = dic[feature]['model']['run']
139/169:
for for feature, k in dic.items():
    model_params['MOVING_AVG']['model']
    #--------------------------------------------------------------------------
    feature_cols = dic[feature]['feature_cols']
    clf = dic[feature]['model']['clf'] #classifier
    model_name=list(RF['MOVING_AVG']['model'])[1]
    params = dic[feature]['model'][model_name] #parameter of the classfifier
    run = dic[feature]['model']['run']
139/170: list(model_params['MOVING_AVG']['model'].get_keys)
139/171: list(model_params['MOVING_AVG']['model'])
139/172: len(list(model_params['MOVING_AVG']['model']))
139/173:
for for feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
139/174:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
139/175:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{'model': #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of RF
#----------------------------------------------------------
139/176: list(model_params['MOVING_AVG']['model'])
139/177: model_params['MOVING_AVG']
139/178:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, model_params = model_params[feature])
139/179:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, model_params = model_params[feature])
139/180: dic
139/181:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model']['clf'] #classifier
        params = dic[feature]['model'][model_name] #parameter of the classfifier
        run = dic[feature]['model']['run']
139/182:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{'model': #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{ 
       {'model':
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of RF
#----------------------------------------------------------
139/183:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{'model': #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{ 
       
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }
}#end of RF
#----------------------------------------------------------
139/184:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{'model': #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{ 
       {'model':
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of RF
#----------------------------------------------------------
139/185:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{'model': #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_STOCH':{ 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of RF
#----------------------------------------------------------
139/186:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{'model': #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
    "RSI": 
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'CMO':  {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'STOCH': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_2': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'BBI_3': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CCI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_RSI': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    },
   'INV_CMO': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
   'INV_STOCH':{ 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of RF
#----------------------------------------------------------
139/187:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{'model': #needs to be chanfed this is just example
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
    "RSI": 
     { 
         'model':{
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
   'CMO':  {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
   'STOCH': { 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
   'BBI_2': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
   'BBI_3': { 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
   'INV_CCI': { 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
   'INV_RSI': { 
       'model':{
     "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
   'INV_CMO': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
   'INV_STOCH':{ 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of RF
#----------------------------------------------------------
139/188:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model']['clf'] #classifier
        params = dic[feature]['model'][model_name] #parameter of the classfifier
        run = dic[feature]['model']['run']
139/189:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
    "RSI": 
     { 
         'model':{
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': { 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': { 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': { 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': { 
       'model':{
     "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{ 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of RF
#----------------------------------------------------------
139/190: list(model_params['MOVING_AVG']['model'])
139/191: model_params
139/192:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': { 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': { 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    } ,
   'INV_MOVING_AVG': {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': { 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': { 
       'model':{
     "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{ 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of RF
#----------------------------------------------------------
139/193:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': { 
       'model':{
     "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{ 
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of RF
#----------------------------------------------------------
139/194:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model']['clf'] #classifier
        params = dic[feature]['model'][model_name] #parameter of the classfifier
        run = dic[feature]['model']['run']
139/195:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': { 
       'model':{
     "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
}#end of RF
#----------------------------------------------------------
139/196:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': { 
       'model':{
     "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {  
       'model':{
       "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of RF
#----------------------------------------------------------
139/197: list(model_params['MOVING_AVG']['model'])
139/198:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': { 
       'model':{
     "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of 
#----------------------------------------------------------
139/199:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of 
#----------------------------------------------------------
139/200: list(model_params['MOVING_AVG']['model'])
139/201:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
     {'model': {'RF':
        "CCI":{
            'model': #needs to be chanfed this is just example
            {'RF':
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of 
#----------------------------------------------------------
139/202:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
            {'RF':
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of 
#----------------------------------------------------------
139/203:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
            {'RF1':
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of 
#----------------------------------------------------------
139/204:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model']['clf'] #classifier
        params = dic[feature]['model'][model_name] #parameter of the classfifier
        run = dic[feature]['model']['run']
139/205:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }},  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
            {'RF1':
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of 
#----------------------------------------------------------
139/207:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
            {'RF1':
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of 
#----------------------------------------------------------
139/208:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
            {'RF1':
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}
}#end of 
#----------------------------------------------------------
139/209:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
            {'RF1':
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}
}#end of 
#----------------------------------------------------------
139/210:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
            {'RF1':
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}
}#end of 
#----------------------------------------------------------
139/211:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #----------------------------------------------------
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":{
            'model': #needs to be chanfed this is just example
            {'RF1':
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
139/212:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
139/213:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
{
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'CMO':  {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'STOCH': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_2': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'BBI_3': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CCI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_RSI': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_CMO': {
            'model': #needs to be chanfed this is just example
     {  
         "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
    }},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
139/214:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
139/215:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model']['clf'] #classifier
        params = dic[feature]['model'][model_name] #parameter of the classfifier
        run = dic[feature]['model']['run']
139/216:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
      #  feature_cols = dic[feature]['feature_cols']
       # clf = dic[feature]['model']['clf'] #classifier
       # params = dic[feature]['model'][model_name] #parameter of the classfifier
       # run = dic[feature]['model']['run']
139/217:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
       feature_cols = dic[feature]['feature_cols']
       # clf = dic[feature]['model']['clf'] #classifier
       # params = dic[feature]['model'][model_name] #parameter of the classfifier
       # run = dic[feature]['model']['run']
139/219:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
       # clf = dic[feature]['model']['clf'] #classifier
       # params = dic[feature]['model'][model_name] #parameter of the classfifier
       # run = dic[feature]['model']['run']
139/220: feature_cols
139/221: dic
139/222:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model']['clf'] #classifier
        params = dic[feature]['model'][params] #parameter of the classfifier
       # run = dic[feature]['model']['run']
139/223:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model']['clf'] #classifier
        params = dic[feature]['model']['params'] #parameter of the classfifier
       # run = dic[feature]['model']['run']
139/224: dic[feature]['model']
139/225: feature
139/226:
feature
dic
139/227:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model_']['clf'] #classifier
        #params = dic[feature]['model_params']['params'] #parameter of the classfifier
       # run = dic[feature]['model']['run']
139/228:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model']['clf'] #classifier
        #params = dic[feature]['model_params']['params'] #parameter of the classfifier
       # run = dic[feature]['model']['run']
139/229:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model_params']['clf'] #classifier
        #params = dic[feature]['model_params']['params'] #parameter of the classfifier
       # run = dic[feature]['model']['run']
139/230:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model_params']['model'][model_name]#classifier
        #params = dic[feature]['model_params']['params'] #parameter of the classfifier
        #run = dic[feature]['model']['run']
139/231:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model_params']['model'][model_name][clf]#classifier
        #params = dic[feature]['model_params']['params'] #parameter of the classfifier
        #run = dic[feature]['model']['run']
139/232:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model_params']['model'][model_name]['clf']#classifier
        #params = dic[feature]['model_params']['params'] #parameter of the classfifier
        #run = dic[feature]['model']['run']
139/233: dic[feature]['model_params']['model'][model_name]
139/234: dic[feature]['model_params']['model']
139/235: dic[feature]['model_params']['model']['clf']
139/236: dic[feature]['model_params']['model']
139/237: dic
139/238: dic[feature]['model_params']['model']['clf']
139/239: dic[feature]['model_params']['model']['RF']
139/240: model_params
139/241: model_params['model']
139/242: model_params
139/243: model_params['MOVING_AVG']['model']
139/244: model_params['MOVING_AVG']['model']['RF']
139/245:
#Creating a dictionary  of features, models and parameters obtained from parameter tuning
dic={}
##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
for feature, feature_col in features_dic.items():
    dic[feature] = dict(feature_cols = feature_col, model_params = model_params[feature])
139/246: dic
139/247: len(list(model_params['MOVING_AVG']['model']))
139/248:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model_params']['model'][model_name]['clf']#classifier
        #params = dic[feature]['model_params']['params'] #parameter of the classfifier
        #run = dic[feature]['model']['run']
139/249:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model_params']['model'][model_name]['clf']#classifier
        params = dic[feature]['model_params']['params'] #parameter of the classfifier
        #run = dic[feature]['model']['run']
139/250:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model_params']['model'][model_name]['clf']#classifier
        params = dic[feature]['model_params']['model'][model_name]["params"] #parameter of the classfifier
        #run = dic[feature]['model']['run']
139/251: params
139/252:
for  feature, k in dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
    #--------------------------------------------------------------------------
        feature_cols = dic[feature]['feature_cols']
        clf = dic[feature]['model_params']['model'][model_name]['clf']#classifier
        params = dic[feature]['model_params']['model'][model_name]["params"] #parameter of the classfifier
        run = dic[feature]['model_params']['model'][model_name]["run"]
139/253: run
139/254: dic
139/255:
for feature, feature_col in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
139/256: model_params['model'][model_name]['clf']
139/257: model_params[feature]['model'][model_name]['clf']
139/258:
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
        clf=model_params[feature]['model'][model_name]['clf']
139/259:
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
        clf=model_params[feature]['model'][model_name]['clf']
        params=model_params[feature]['model'][model_name]['params']
139/260: params
139/261:
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        print(model_name)
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
139/262: run
139/263:
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
140/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
140/2: from sklearn.base import clone
140/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
140/4:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
140/5:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True)
140/6:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self, base_model):
        self.base_model = base_model
    def fit(self, X, y=None, **fit_params):

        if not isinstance(X, pd.DataFrame):
            raise ValueError('X is not a pandas.DataFrame')

        self.models_ = {}

        columns = self._get_fit_columns(X)

        for key in X[self.by].unique():

            # Copy the model
            model = clone(self.base_model)

            # Select the rows that will be fitted
            mask = (X[self.by] == key).tolist()
            rows = X.index[mask]

            # Fit the model
            model.fit(X.loc[rows, columns], y[mask], **fit_params)

            # Save the model
            self.models_[key] = model

        return self
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------


#---------------------------------------------------------------------------------
140/7:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
140/8:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
140/9:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
140/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
140/11:
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
140/12:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
140/13:
#handling the data: we need to check if the data is for predicting or for modeling. ...
#--------------------------under construction!!!
modeling = True #if we want to use the data for modeling and saving models, else 

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
140/14:
list_columns=list(input_files)
#list_columns
140/15:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
140/16:

print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_df = input_files_train_contracts.copy() #sig_df is the whole processed data frame with 
                                            #all features(excluding the test data)
140/17:
y_train=sig_df['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
140/18:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
140/19:
if run is False:
    continue
    clf.model.set_params(**params)
    #tc = clf(**v['params'])
    tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+model_name+".model")
     
    
print("Done!")
140/20:
if run is False:
    continue
clf.model.set_params(**params)
tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
tc.save(saved_folder+model_name+".model")
     
    
print("Done!")
140/21:
if run is False:
    continue
clf.model.set_params(**params)
tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
tc.save(saved_folder+model_name+".model")
     
    
print("Done!")
140/22:
##I will add the following when I have the for loop
#if run is False:
#    continue
clf.model.set_params(**params)
tc.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
tc.save(saved_folder+model_name+".model")
     
    
print("Done!")
140/23:
##I will add the following when I have the for loop
#if run is False:
#    continue
clf.model.set_params(**params)
clf.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
clf.save(saved_folder+model_name+".model")
     
    
print("Done!")
140/24:
##I will add the following when I have the for loop
#if run is False:
#    continue
clf.model.set_params(**params)
clf.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
clf.save(saved_folder+feature+model_name+".model")
     
    
print("Done!")
140/25:
##I will add the following when I have the for loop
#if run is False:
#    continue
clf.model.set_params(**params)
clf.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
clf.save(saved_folder+feature+"_"+model_name+".model")
     
    
print("Done!")
140/26:
##I will add the following when I have the for loop
#if run is False:
#    continue
clf.model.set_params(**params)
clf.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
clf.save(saved_folder+feature+"*"+model_name+".model")
     
    
print("Done!")
140/27:
##I will add the following when I have the for loop
#if run is False:
#    continue
clf.model.set_params(**params)
clf.train(sig_df, feature_cols)
    #tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
clf.save(saved_folder+feature+"_"+model_name+".model")
     
    
print("Done!")
140/28:
from sklearn import metrics
#how to load saved models


 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+'_'+feature+".model")
y_pred = tc.predict(X_test)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
140/29:
from sklearn import metrics
#how to load saved models


 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = tc.predict(X_test)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
140/30:
from sklearn import metrics
#how to load saved models


 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X_test)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
140/31:
from sklearn import metrics
#how to load saved models to test 
X_test = input_files_test_contracts[feature_cols]

 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X_test)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
140/32: X
142/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/model-evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
142/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/model-evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
142/3: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/model-evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
142/4: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/model-evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
142/5: yaml_file
142/6: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/model-evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
140/33:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
140/34:
from sklearn import metrics
#how to load saved models to test 
X = input_files_test_contracts

 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X, feature_cols)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
140/35:
from sklearn import metrics
#how to load saved models to test 
X = input_files_test_contracts

 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X, feature_cols)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
140/36: clf
140/37:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[eat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
140/38:
from sklearn import metrics
#how to load saved models to test 
X = input_files_test_contracts

 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X, feature_cols)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
140/39:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
140/40:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
140/41:
##I will add the following when I have the for loop
#if run is False:
#    continue
clf.model.set_params(**params)
clf.train(sig_df, feature_cols)
clf.save(saved_folder+feature+"_"+model_name+".model")
     
    
print("Done!")
140/42:
from sklearn import metrics
#how to load saved models to test 
X = input_files_test_contracts

 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X, feature_cols)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
140/43:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
140/44:
from sklearn import metrics
#how to load saved models to test 
X = input_files_test_contracts

 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X, feature_cols)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
140/45:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
140/46:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
140/47:
##I will add the following when I have the for loop
#if run is False:
#    continue
clf.model.set_params(**params)
clf.train(sig_df, feature_cols)
clf.save(saved_folder+feature+"_"+model_name+".model")
     
    
print("Done!")
140/48:
from sklearn import metrics
#how to load saved models to test 
X = input_files_test_contracts

 #   if v["run"] is False:
 #       continue
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X, feature_cols)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
144/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
144/2: from sklearn.base import clone
144/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
144/4:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
144/5:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
144/6:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
144/7:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
144/8:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
144/9:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
144/10:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
144/11:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
144/12:
#handling the data: we need to check if the data is for predicting or for modeling. ...
#--------------------------under construction!!!
modeling = True #if we want to use the data for modeling and saving models, else 

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
144/13:
list_columns=list(input_files)
#list_columns
144/14:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
144/15:

print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_df = input_files_train_contracts.copy() #sig_df is the whole processed data frame with 
                                            #all features(excluding the test data)
144/16:
y_train=sig_df['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
144/17:
from  testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
144/18:
from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
144/19:
from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
144/20:
from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
144/21:
from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
144/22:
from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
144/23:
from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
144/24: from sklearn.base import clone
144/25:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/26:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/27:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/28:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
144/29:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project/testv.yaml')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/30:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.baseloader(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project/testv.yaml')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/31:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/32:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/33:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project/testv.yaml')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/34:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.baseloader(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project/testv.yaml')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/35:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
    
    
r=Data_Model_Manager('/Users/shahla/Dropbox/MLCourse/ML1030/Project/testv.yaml')   
x=r.filter_data_x()
y=r.filter_data_y()
xy=r.filter_data_xy()
y=r.load_data()
144/36:
class Data_Model_Manager:
    def __init__(self,PATH):
        self.lb=LabelEncoder()
        self.path=PATH
        self.raw_data=None        
        self.yaml_file=None
        self.train=None
        self.test=None                
        self.X_train = []
        self.X_train_inv=[]
        self.X_test = []
        self.X_test_inv=[]    
        self.y_train = None
        self.y_test = None
        self.data=[]
      #load data                   
    def load_data(self):
        self.yaml_file = yaml.load(open(self.path,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
144/37: load_data()
144/38:
class Data_Model_Manager:
    def __init__(self):
        self.raw_data=None        
        self.yaml_file=None
      #load data                   
    def load_data(self, PATH):
        self.yaml_file = yaml.load(open(PATH,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
144/39:
class Data_Manager:
    def __init__(self):
        self.raw_data=None        
        self.yaml_file=None
      #load data                   
    def load_data(self, PATH):
        self.yaml_file = yaml.load(open(PATH,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            self.raw_data=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
144/40:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
144/41: df=Data_Manager().load_data(path)
144/42: df=Data_Manager().load_data(path_'1.h5')
144/43: df=Data_Manager().load_data(path+'1.h5')
144/44: df=Data_Manager().load_data(path+'testv.yaml')
144/45:
df=Data_Manager().load_data(path+'testv.yaml')
df
144/46: features_dic
144/47:
import nltk
import sklearn

print('The nltk version is {}.'.format(nltk.__version__))
print('The scikit-learn version is {}.'.format(sklearn.__version__))
145/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
145/2:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
#path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
#df = pd.read_hdf(path_to_data+'1.h5')
#removing zeros data is imbalanced
##for google cloud
df = pd.read_hdf('1.h5')
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
145/3:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.001)
input_files = df[:index]
145/4:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
my_cv = TimeSeriesSplit(n_splits=10)
145/5:
clf=RandomForestClassifier()
clf.get_params()
145/6:
# File to save first results
import csv
##for kaggle and my local computer
#out_file = path_to_data+'RF_trials.csv'
#for google cloud
out_file ='RF_trials.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write the headers to the file
writer.writerow(['loss', 'params', 'iteration'])
of_connection.close()
145/7:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()
param_space = {'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"]),
    'bootstrap': hp.choice('bootstrap', [True, False])
              }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=200, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
145/8:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
145/9:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()
param_space = {'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"]),
    'bootstrap': hp.choice('bootstrap', [True, False])
              }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=200, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
145/10:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()
param_space = {'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"]),
    'bootstrap': hp.choice('bootstrap', [True, False])
              }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
145/11:

#beccause the following error, I read the results from the csv file
#https://github.com/hyperopt/hyperopt/issues/431
results = pd.read_csv('RF_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = True, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
145/12: best
145/13:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50)
best_bayes_model.fit(X_train, y_train)
145/14:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
145/15:
from sklearn.metrics import confusion_metrics
print("Confusion",confusion_metrics(y_test, y_pred))
145/16:
from sklearn import metrics
print("Confusion",metrics.confusion_metrics(y_test, y_pred))
145/17:
from sklearn import metrics
print("Confusion",confusion_matrix(y_test, y_pred))
145/18:
from sklearn import metrics
confusion_matrix(y_test, y_pred)
147/1:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
147/2: csv_filepath = 'path_to_data' + '*.h5'
147/3:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
147/4:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
147/5: lookbacks['100ms']
147/6: lookbacks['300ms']
147/7:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
147/8:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
147/9: csv_filepath = 'path_to_data' + '*.h5'
147/10:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
147/11: input_files.info()
147/12: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
147/13:
list_columns=list(input_files)
list_columns[10]
147/14:
list_columns=list(input_files)
list_columns[20]
147/15:
list_columns=list(input_files)
list_columns[0:4]
147/16: input_files.dtypes
147/17:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
147/18:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
147/19:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig[0:3]
147/20: input_files[sig] = input_files[sig].apply(lambda x: x.replace(([-1],[0])))
147/21:
#Cast a pandas object to a specified dtype 
categories=[-1, 0, 1]
input_files[sig] = input_files[sig].astype('category', categories = categories)
147/22:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2

input_files[sig]=input_files[sig].apply(lambda x: x.cat.rename_categories([0,2])) # there is no zero in the input_files
input_files[sig]=input_files[sig].apply(lambda x: x.cat.add_categories([0]))
149/1:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
149/2:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
149/3:
class Data_Manager:
    def __init__(self):
        self.raw_data=None        
        self.yaml_file=None
      #load data                   
    def load_data(self, PATH):
        self.yaml_file = yaml.load(open(PATH,"r")) 
        if self.yaml_file['PATH'][0].endswith('csv')==True:
            df=pd.read_csv(self.yaml_file['PATH'][0])
            
        elif  self.yaml_file['PATH'][0].endswith('h5')==True:            
            self.raw_data=pd.read_hdf(self.yaml_file['PATH'][0])
            
        else: 
            print('Error: Use CSV or H5 files')
        #removing 0 labels in the target
        df = df.drop(df[df.sig_label_sec_1 ==0].index)
        #---------------------------------------------------------------------------------
        input_files=df.copy()
        #---------------------------------------------------------------------------------
        list_columns=list(input_files)
        #---------------------------------------------------------------------------------
        #extracting features that needs to be categorical
        sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
        input_files[sig] = input_files[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        input_files[sig] = input_files[sig].astype('category', categories = [0, 1, 2])
#---------------------------------------------------------------------------------
        return self.raw_data
    
    def filter_data_x(self):
        data=self.load_data()

        for n in self.yaml_file['ti']:
            #get Xs 
            self.X_train.append((data.filter(like=('sig_'+n+'_')))+1)
            #turn X into categorical
            for i in range(0,len(self.X_train)-1):
                for col in  self.X_train[i].columns.values.tolist():
                    self.X_train[i][col]=self.lb.fit_transform(self.X_train[i][col].astype('category',copy=False))
                    
        for n in self.yaml_file['inv_ti']:
            #get X inverse
            self.X_train_inv.append((data.filter(like=n))+1)
            #turn into categorical
            for i in range(0,len(self.X_train_inv)-1):
                for col in  self.X_train_inv[i].columns.values.tolist():
                    self.X_train_inv[i][col]=self.lb.fit_transform(self.X_train_inv[i][col].astype('category',copy=False)) 

        self.data.extend([self.X_train,self.X_train_inv])
        self.data=list(itertools.chain.from_iterable(self.data))   

        return self.data 
    
    def filter_data_y(self):
        self.y_train=self.load_data()['sig_label_sec_1']+1 
        self.y_train=self.lb.fit_transform(self.y_train.astype('category',copy=False))
        return self.y_train
        
    def filter_data_xy(self):
        self.data=self.filter_data_x()
        self.y_trainy=self.filter_data_y()
        return self.data,self.y_train
    
    def build_model(self):
        model=[]
        model.append(LogisticRegression(solver='lbfgs', multi_class='multinomial'))
        return model
149/4:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
149/5:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
149/6: from sklearn.base import clone
149/7:
import pandas as pd
import numpy as np
import yaml 
import itertools
149/8:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
149/9:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
149/10:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
149/11:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
149/12: features_dic
149/13:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
149/14:
#handling the data: we need to check if the data is for predicting or for modeling. ...
#--------------------------under construction!!!
modeling = True #if we want to use the data for modeling and saving models, else 

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
149/15:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
149/16: features_dic
149/17:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
149/18:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
149/19:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
149/20:
list_columns=list(input_files)
#list_columns
149/21:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
149/22:

print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_df = input_files_train_contracts.copy() #sig_df is the whole processed data frame with 
                                            #all features(excluding the test data)
149/23:
y_train=sig_df['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
149/24:
##I will add the following when I have the for loop
#if run is False:
#    continue
clf.model.set_params(**params)
clf.train(sig_df, feature_cols)
clf.save(saved_folder+feature+"_"+model_name+".model")
     
    
print("Done!")
149/25:

#how to load saved models to test 
X = input_files_test_contracts
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X, feature_cols)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
149/26:

#how to load saved models to test 
X = input_files_test_contracts
clf.load(saved_folder+feature+"_"+model_name+".model")
y_pred = clf.predict(X, feature_cols)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
147/23:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
147/24: lookbacks['100ms']
147/25: lookbacks['300ms']
147/26:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
147/27:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
147/28: csv_filepath = 'path_to_data' + '*.h5'
147/29:
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
147/30: input_files.info()
147/31: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
147/32:
list_columns=list(input_files)
list_columns[0:4]
147/33: input_files.dtypes
147/34:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
147/35:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig[0:3]
147/36:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
147/37: input_files[sig] = input_files[sig].apply(lambda x: x.replace(([-1],[0])))
147/38:
#Cast a pandas object to a specified dtype 
categories=[-1, 0, 1]
input_files[sig] = input_files[sig].astype('category', categories = categories)
147/39:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2

input_files[sig]=input_files[sig].apply(lambda x: x.cat.rename_categories([0,2])) # there is no zero in the input_files
input_files[sig]=input_files[sig].apply(lambda x: x.cat.add_categories([0]))
147/40:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
147/41: input_files_train_contracts.tail(10)
147/42: input_files_test_contracts.head(10)
147/43:
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
147/44:
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
147/45: y_train=input_files_train_contracts['sig_label_sec_1']
147/46: y_test=input_files_test_contracts['sig_label_sec_1']
147/47: y_test=input_files_test_contracts['sig_label_sec_1']
147/48:
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
147/49:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig[0:3]
147/50: sig = list(input_files.select_dtypes(include=['int8']).columns)
147/51:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
147/52: sig = list(input_files.select_dtypes(include=['int8']).columns)
147/53:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
147/54:
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
147/55:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
147/56: lookbacks['100ms']
147/57: lookbacks['300ms']
147/58:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
147/59:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
147/60: csv_filepath = 'path_to_data' + '*.h5'
147/61:
#handling the data: we need to check if the data is for predicting or for modeling. ...

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
147/62:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
147/63:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
147/64:
#handling the data: we need to check if the data is for predicting or for modeling. ...

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
147/65: input_files.info()
147/66: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
147/67:
list_columns=list(input_files)
list_columns[0:4]
147/68: input_files.dtypes
147/69:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
147/70: sig = list(input_files.select_dtypes(include=['int8']).columns)
147/71:
#train = pd.Series(list('abbaa'))
#test = pd.Series(list('abcd'))
#categories = np.union1d(train, test)
#train = train.astype('category', categories=categories)
#test = test.astype('category', categories=categories)
#pd.get_dummies(train)
#pd.get_dummies(test)
147/72:
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
147/73: sig = list(input_files.select_dtypes(include=['int8']).columns)
147/74:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
147/75:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
151/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
151/2: lookbacks['100ms']
151/3: lookbacks['300ms']
151/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
151/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
151/6:
#handling the data: we need to check if the data is for predicting or for modeling. ...

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
151/7: input_files.info()
151/8: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
151/9:
list_columns=list(input_files)
list_columns[0:4]
151/10: input_files.dtypes
151/11:
list_columns=list(input_files)
list_columns[0:4]
151/12:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
151/13: input_files['sig_label_sec_1'].dtypes
151/14: input_files.dtypes
151/15: min(input_files.index)
151/16: max(input_files.index)
151/17:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['price'].plot(grid=True)

# Show the plot
plt.show()
151/18:
# Import matplotlib
import matplotlib.pyplot as plt

# Plot the distribution of `
input_files[['price']].hist(bins=50)

# Show the plot
plt.show()

# Pull up summary statistics
print(input_files[['price']].describe())
151/19:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['forward_price_sec_1'].plot(grid=True)

# Show the plot
plt.show()
151/20:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['forward_price_sec_3'].plot(grid=True)

# Show the plot
plt.show()
151/21:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['ret_sec_1'].plot(grid=True)

# Show the plot
plt.show()
151/22:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
151/23:

# Pull up summary statistics
print(input_files[['sig_label_sec_1']].describe())
151/24: print("Creating train/test split of data...")
151/25:
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
151/26: input_files_train_contracts.tail(10)
151/27: input_files_test_contracts.head(10)
151/28:
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
151/29:
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
151/30: y_train=input_files_train_contracts['sig_label_sec_1']
151/31: y_test=input_files_test_contracts['sig_label_sec_1']
150/1: features_dic
150/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
150/3:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
150/4: from sklearn.base import clone
150/5:
import pandas as pd
import numpy as np
import yaml 
import itertools
150/6:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
150/7:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
150/8:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
150/9:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
150/10: features_dic
150/11:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
150/12:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
150/13:
#handling the data: we need to check if the data is for predicting or for modeling. ...

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
150/14:
list_columns=list(input_files)
#list_columns
150/15:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
150/16:

print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_df = input_files_train_contracts.copy() #sig_df is the whole processed data frame with 
                                            #all features(excluding the test data)
150/17:
y_train=sig_df['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
150/18:
target_col = 'sig_label_sec_1'
y= df['sig_label_sec_1']
150/19:
target_col = 'sig_label_sec_1'
y= df['sig_label_sec']
150/20:
target_col = 'sig_label_sec_1'
run_testing = True #if the data is for testing then run_tesiting=True, else it is for training
if run_testing == False:
    y= df['sig_label_sec']
    X=df.drop('sig_label_sec_1', axis=1)
150/21:
target_col = 'sig_label_sec_1'
run_testing = True #if the data is for testing then run_tesiting=True, else it is for training
150/22:



if run_testing == False: #data is for training
    y= df[target_col]
    X=df.drop('sig_label_sec_1', axis=1)
    else: 
        X = df
150/23:



if run_testing == False: #data is for training
    y= df[target_col]
    X=df.drop('sig_label_sec_1', axis=1)
    else X = df
150/24:



if run_testing == False: #data is for training
    y= df[target_col]
    X=df.drop('sig_label_sec_1', axis=1)
    else:
    X = df
150/25:



if run_testing == False: #data is for training
    y= df[target_col]
    X=df.drop('sig_label_sec_1', axis=1)
    else:
    X = df.copy()
150/26:



if run_testing == False: #data is for training
    y= df[target_col]
    X=df.drop('sig_label_sec_1', axis=1)
else:
    X = df.copy()
150/27:


feature_cols[0:3]
150/28:
prep = Data_prep(df, target_col)
X = prep.process(feature_cols)
if run_testing == False: #data is for training
    y= prep.process(target_col)
150/29:


##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
152/1:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
152/2:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
152/3: from sklearn.base import clone
152/4:
import pandas as pd
import numpy as np
import yaml 
import itertools
152/5:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
152/6:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col):
            self.df = df 
            self.target_col = target_col

            
            
    def process(self, list_columns):
        df=df[list_columns]
        
        #removing zeros data is imbalanced
        target_col = self.target_col
        input_files = df.drop(df[df.target_col ==0].index)
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
       #extracting features that needs to be categorical
        sig = list(input_files.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        input_files[sig] = input_files[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        input_files[sig] = input_files[sig].astype('category', categories = categories)
        return input_files
#---------------------------------------------------------------------------------
152/7:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
152/8:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
152/9:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
152/10: features_dic
152/11:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
152/12:
##For my local computer
df = pd.read_hdf(path+'1.h5')
152/13:
target_col = 'sig_label_sec_1'
run_testing = True #if the data is for testing then run_tesiting=True, else it is for training
152/14:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
152/15:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col):
            self.df = df 
            self.target_col = target_col

            
            
    def process(self, list_columns):
        df=df[list_columns]
        
        #removing zeros data is imbalanced
        target_col = self.target_col
        input_files = df.drop(df[df.target_col ==0].index)
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
       #extracting features that needs to be categorical
        sig = list(input_files.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        input_files[sig] = input_files[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        input_files[sig] = input_files[sig].astype('category', categories = categories)
        return input_files
#---------------------------------------------------------------------------------
152/16:
prep = Data_prep(df, target_col)
X = prep.process(feature_cols)
if run_testing == False: #data is for training
    y= prep.process(target_col)
152/17:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col):
            self.df = df 
            self.target_col = target_col

            
            
    def process(self, list_columns):
        input_files =self.df[list_columns]
        
        #removing zeros data is imbalanced
        target_col = self.target_col
        input_files = df.drop(df[df.target_col ==0].index)
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
       #extracting features that needs to be categorical
        sig = list(input_files.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        input_files[sig] = input_files[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        input_files[sig] = input_files[sig].astype('category', categories = categories)
        return input_files
#---------------------------------------------------------------------------------
152/18:
prep = Data_prep(df, target_col)
X = prep.process(feature_cols)
if run_testing == False: #data is for training
    y= prep.process(target_col)
152/19:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col):
            self.df = df 
            self.target_col = target_col

            
            
    def process(self, list_columns):
        input_files =self.df[list_columns]
        
        #removing zeros data is imbalanced
        target_col = self.target_col
        input_files = df.drop(df[df[target_col] ==0].index)
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
       #extracting features that needs to be categorical
        sig = list(input_files.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        input_files[sig] = input_files[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        input_files[sig] = input_files[sig].astype('category', categories = categories)
        return input_files
#---------------------------------------------------------------------------------
152/20:
prep = Data_prep(df, target_col)
X = prep.process(feature_cols)
if run_testing == False: #data is for training
    y= prep.process(target_col)
152/21:
target_col = 'sig_label_sec_1'
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
152/22:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
152/23:
prep = Data_prep(df, target_col)
X = prep.process(feature_cols)
153/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json

## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
153/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
153/3:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'trump',  
        'result_type': 'popular',#mixed recent popular
        'count': 100,
        'lang': 'en',
        }
153/4:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
153/5:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
153/6:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'trump',  
        'result_type': 'popular',#mixed recent popular
        'count': 40,
        'lang': 'en',
        }
153/7:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
153/8:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
153/9:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'trump',  
        'result_type': 'popular',#mixed recent popular
        'count': 10,
        'lang': 'en',
        }
153/10:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
153/11:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
153/12:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'trump',  
        'result_type': 'popular',#mixed recent popular
        'count': 20,
        'lang': 'en',
        }
153/13:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
153/14:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
153/15:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'trump',  
        'result_type': 'mixed',#mixed recent popular
        'count': 20,
        'lang': 'en',
        }
153/16:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
153/17:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
153/18:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'trump',  
        'result_type': 'mixed',#mixed recent popular
        'count': 100,
        'lang': 'en',
        }
153/19:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
153/20:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
154/1:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
154/2:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
154/3: from sklearn.base import clone
154/4:
import pandas as pd
import numpy as np
import yaml 
import itertools
154/5:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
154/6:
#handling the data: we need to check if the data is for predicting or for modeling. ...

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
155/1:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
155/2:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
155/3: from sklearn.base import clone
155/4:
import pandas as pd
import numpy as np
import yaml 
import itertools
155/5:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
155/6:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
155/7:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
155/8:
#handling the data: we need to check if the data is for predicting or for modeling. ...

##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
155/9:
#handling the data: we need to check if the data is for predicting or for modeling. ...

##For my local computer
df = pd.read_hdf(path+'1.h5')

##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
155/10: input_files.to_hdf('reduced_1.h5', key='df', mode='w')
155/11: input_files.to_hdf(path+'reduced_1.h5', key='df', mode='w')
157/1:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
157/2:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
155/12:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
#---------------------------------------------------------------
from classifierwrapper import * #we created it
155/13:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
155/14:
#creating directories we need, if they do not exsit already
import os
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
155/15:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
155/16:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
155/17: features_dic
155/18:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
155/19:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
155/20:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col):
            self.df = df 
            self.target_col = target_col

            
            
    def process(self, list_columns):
        data =self.df[list_columns]
        
        #removing zeros data is imbalanced
        target_col = self.target_col
        input_files = input_files.drop(data[data[target_col] ==0].index)
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        return data
#---------------------------------------------------------------------------------
155/21:
#handling the data: we need to check if the data is for predicting or for modeling. ...

##For my local computer. I am using only the first one percent of data
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')

#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
155/22: d=input_files[list_columns,target_col]
155/23:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
155/24: d=input_files[list_columns,target_col]
155/25: d=input_files[list_columns]
155/26:
list_columns.append(target_col)
d=input_files[list_columns]
155/27: list_columns
155/28:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col, run_testing, X = None, y = None):
            self.df = df 
            self.target_col = target_col
            self.run_testing = run_testing
            
            
    def process(self, list_columns):
        data =self.df
        if self.run_testing == False:
          #removing zeros data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
        columns = list_columns
        columns.append(target_col)
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
155/29:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols)
if run_testing== False:
    y = prep.y
X=prep.X
155/30: X.columns
155/31: y.columns
155/32: y
155/33:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col, run_testing, X = None, y = None):
            self.df = df 
            self.target_col = target_col
            self.run_testing = run_testing
            
            
    def process(self, list_columns):
        data =self.df
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
155/34: len(X)
155/35:
training_index = ceil(len(X)*0.7)
X = X[:training_index]
155/36:
training_index = ceil(len(X)*0.7)
X = X[:training_index]
len(X)
161/1:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
161/2:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
161/3:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
#---------------------------------------------------------------
from classifierwrapper import * #we created it
161/4:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col, run_testing, X = None, y = None):
            self.df = df 
            self.target_col = target_col
            self.run_testing = run_testing
            
            
    def process(self, list_columns):
        data =self.df
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
161/5:
##For my local computer
df = pd.read_hdf(path+'1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
161/6:
#creating directories we need, if they do not exsit already
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
161/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
161/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
161/9: features_dic
161/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
161/11:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
          = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
161/13:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
161/14:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col.model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col.model")
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/15:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col.model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col.'model")
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/16:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col."model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col.'model")
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/17:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/18:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
from classifierwrapper import * #we created it
161/19:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/20: df.columns
161/21:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col, run_testing, X = None, y = None):
            self.df = df 
            self.target_col = target_col
            self.run_testing = run_testing
            
            
    def process(self, list_columns):
        data =self.df
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        dataa = data[columns]
       #extracting features that needs to be categorical
        sig = list(dataa.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        dataa[sig] = dataa[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        dataa[sig] = dataa[sig].astype('category', categories = categories)
        self.X = dataa[list_columns]
        if self.run_testing == False:
            self.y = dataa[target_col]
        return self
#---------------------------------------------------------------------------------
161/22:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/23:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col, run_testing, X = None, y = None):
            self.df = df 
            self.target_col = target_col
            self.run_testing = run_testing
            
            
    def process(self, list_columns):
        data =self.df
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
161/24:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col, run_testing, X = None, y = None):
            self.df = df 
            self.target_col = target_col
            self.run_testing = run_testing
            
            
    def process(self, list_columns):
        data =self.df.copy
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
161/25:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/26:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col, run_testing, X = None, y = None):
            self.df = df 
            self.target_col = target_col
            self.run_testing = run_testing
            
            
    def process(self, list_columns):
        data =(self.df).copy
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
161/27:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/28:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col, run_testing, X = None, y = None):
            self.df = df 
            self.target_col = target_col
            self.run_testing = run_testing
            
            
    def process(self, list_columns, df):
        data = df
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
161/29: df.copy
161/30:
prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/31:
#Data_prep.py file
class Data_prep():
    def __init__(self,  X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
161/32:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
161/33:
#Data_prep.py file
#class Data_prep():
#    def __init__(self,  X = None, y = None):
#            self. X = None
#            self. y = None
            
            
def process(list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return X, y
#---------------------------------------------------------------------------------
161/34: X, y=process(feature_cols, df, target_col, run_testing)
161/35:
#Data_prep.py file
#class Data_prep():
#    def __init__(self,  X = None, y = None):
#            self. X = None
#            self. y = None
            
            
def process(list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        #data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return X, y
#---------------------------------------------------------------------------------
161/36: X, y=process(feature_cols, df, target_col, run_testing)
161/37:
#Data_prep.py file
#class Data_prep():
#    def __init__(self,  X = None, y = None):
#            self. X = None
#            self. y = None
            
            
def process(list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        #data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return X, y
#---------------------------------------------------------------------------------
161/38: X, y=process(feature_cols, df, target_col, run_testing)
161/39:
#Data_prep.py file
#class Data_prep():
#    def __init__(self,  X = None, y = None):
#            self. X = None
#            self. y = None
            
            
def process(list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        #data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        X = data[list_columns]
        if run_testing == False:
            y = data[target_col]
        return X, y
#---------------------------------------------------------------------------------
161/40: X, y=process(feature_cols, df, target_col, run_testing)
161/41:
#Data_prep.py file
#class Data_prep():
#    def __init__(self,  X = None, y = None):
#            self. X = None
#            self. y = None
            
            
def process(list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns].copy()
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        X = data[list_columns]
        if run_testing == False:
            y = data[target_col]
        return X, y
#---------------------------------------------------------------------------------
161/42: X, y=process(feature_cols, df, target_col, run_testing)
161/43: data = df.loc[:, columns]
161/44: data = df.loc[:, feature_cols]
161/45:
#Data_prep.py file
#class Data_prep():
#    def __init__(self,  X = None, y = None):
#            self. X = None
#            self. y = None
            
            
def process(list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        df.loc[:, 'C':'E']
        data = df.loc[:, columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        X = data[list_columns]
        if run_testing == False:
            y = data[target_col]
        return X, y
#---------------------------------------------------------------------------------
161/46: X, y=process(feature_cols, df, target_col, run_testing)
161/47:
#Data_prep.py file
#class Data_prep():
#    def __init__(self,  X = None, y = None):
#            self. X = None
#            self. y = None
            
            
def process(list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        #df.loc[:, 'C':'E']
        data = df.loc[:, columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        X = data[list_columns]
        if run_testing == False:
            y = data[target_col]
        return X, y
#---------------------------------------------------------------------------------
161/48: X, y=process(feature_cols, df, target_col, run_testing)
161/49: data = df.loc[:, feature_cols]
161/50:
data = df.loc[:, feature_cols]
data.shape
161/51: len(feature_cols)
161/52:
#Data_prep.py file
#class Data_prep():
#    def __init__(self,  X = None, y = None):
#            self. X = None
#            self. y = None
            
            
def process(list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        #df.loc[:, 'C':'E']
        data = df.loc[:, columns]
        print(data.shape)
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        X = data[list_columns]
        if run_testing == False:
            y = data[target_col]
        return X, y
#---------------------------------------------------------------------------------
161/53:
data = df.loc[:, feature_cols]
data.shape
161/54: len(feature_cols)
161/55: X, y=process(feature_cols, df, target_col, run_testing)
162/1:
data = df.loc[:, feature_cols]
sig = list(data.select_dtypes(include=['int8']).columns)
data[sig] = data[sig].apply(lambda x: x+1)
162/2:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
162/3:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
162/4:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
from classifierwrapper import * #we created it
162/5:
#Data_prep.py file
class Data_prep():
    def __init__(self, df, target_col, run_testing, X = None, y = None):
            self.df = df 
            self.target_col = target_col
            self.run_testing = run_testing
            
            
    def process(self, list_columns):
        data =self.df
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
162/6:
##For my local computer
df = pd.read_hdf(path+'1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
162/7:
#creating directories we need, if they do not exsit already
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
162/8:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
162/9:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
162/10: features_dic
162/11:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
162/12:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
162/13:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
162/14:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = Noneting
            
            
    def process(self, list_columns):
        data =self.df
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
162/15:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
162/16:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
162/17:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns):
        data =self.df
        columns = list_columns
        if self.run_testing == False:
          #removing zeros, data is imbalanced
            target_col = self.target_col
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if self.run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
162/18:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
162/19:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
162/20:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, feature_cols, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
162/21:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
162/22:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
162/23:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
162/24:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
162/25:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
163/1:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
163/2:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
163/3:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
from classifierwrapper import * #we created it
163/4:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
163/5:
##For my local computer
df = pd.read_hdf(path+'1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
163/6:
#creating directories we need, if they do not exsit already
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
163/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
163/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
163/9: features_dic
163/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
163/11:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
163/12:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
163/13:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

#how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
163/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
#     clf.train(X_train, feature_cols)
#     clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
#     print("Done training!", model_name)

# #how to load saved models to test 
# clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
# y_pred = clf.predict(X)
# print("Accuracy", metrics.accuracy_score(y, y_pred))
# print("Done!")
163/15:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
     clf.train(X_train, feature_cols)
#     clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
#     print("Done training!", model_name)

# #how to load saved models to test 
# clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
# y_pred = clf.predict(X)
# print("Accuracy", metrics.accuracy_score(y, y_pred))
# print("Done!")
163/16:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, feature_cols)
#     clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
#     print("Done training!", model_name)

# #how to load saved models to test 
# clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
# y_pred = clf.predict(X)
# print("Accuracy", metrics.accuracy_score(y, y_pred))
# print("Done!")
163/17: clf
163/18:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, X, y):
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
163/19:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
163/20:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
163/21:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
165/1:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
165/2:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
165/3:
##For my local computer
df = pd.read_hdf(path+'1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
165/4:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
165/5:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
166/1:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
166/2:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
166/3:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
166/4:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
166/5:
##For my local computer
df = pd.read_hdf(path+'1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
166/6:
#creating directories we need, if they do not exsit already
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
166/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
166/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
166/9: features_dic
166/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
166/11:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
166/12:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, X, y):
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
166/13:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
166/14:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
166/15:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
166/16:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
#     clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
#     print("Done training!", model_name)

# #how to load saved models to test 
# clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
# y_pred = clf.predict(X)
# print("Accuracy", metrics.accuracy_score(y, y_pred))
# print("Done!")
167/1:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
167/2:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
167/3:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
167/4:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
        # -1 is replaced with 0
        # 0 is  replaced with 1
        # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        #Cast a pandas object to a specified dtype 
        categories=[0, 1, 2]
        data[sig] = data[sig].astype('category', categories = categories)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
167/5:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None
    def train(self, X, y):
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
167/6:
##For my local computer
df = pd.read_hdf(path+'1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
167/7:
#creating directories we need, if they do not exsit already
os.makedirs(path+"/saved_models", exist_ok=True) 

saved_folder = path+"saved_models/"
#---------------------------------------------------
167/8:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
167/9:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
167/10: features_dic
167/11:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
167/12:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
167/13:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
167/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
#     clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
#     print("Done training!", model_name)

# #how to load saved models to test 
# clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
# y_pred = clf.predict(X)
# print("Accuracy", metrics.accuracy_score(y, y_pred))
# print("Done!")
167/15: X.shape
167/16: y.shape
167/17: df[target.col]
167/18: df[target_col]
167/19: df[target_col].shape
167/20: cl=clf.model.set_params(**params)
167/21: cl
167/22: cl.fit(X_train,y_train)
167/23:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
167/24:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
167/25:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
167/26:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
167/27:
##For my local computer
df = pd.read_hdf(path+'1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
167/28:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
167/29: df.shape
167/30:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
167/31:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
167/32: df.shape
167/33:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
167/34:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
167/35:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
167/36: features_dic
167/37:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
167/38:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
167/39:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
167/40:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
167/41:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
167/42:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
167/43:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
168/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
168/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
168/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
168/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols):
        X = sig_df[feat_cols]
        y = sig_df['sig_label_sec_1']
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
168/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
168/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
168/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
168/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
168/9: features_dic
168/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
168/11:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
168/12:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
168/13:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
#     clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
#     print("Done training!", model_name)

# #how to load saved models to test 
# clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
# y_pred = clf.predict(X)
# print("Accuracy", metrics.accuracy_score(y, y_pred))
# print("Done!")
168/14:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
168/15:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
168/16:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
168/17:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
168/18:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
168/19: features_dic
168/20:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
168/21:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
168/22:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
168/23:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
#     clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
#     print("Done training!", model_name)

# #how to load saved models to test 
# clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
# y_pred = clf.predict(X)
# print("Accuracy", metrics.accuracy_score(y, y_pred))
# print("Done!")
168/24:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

# #how to load saved models to test 
# clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
# y_pred = clf.predict(X)
# print("Accuracy", metrics.accuracy_score(y, y_pred))
# print("Done!")
168/25:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
168/26:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
# clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
# y_pred = clf.predict(X)
# print("Accuracy", metrics.accuracy_score(y, y_pred))
# print("Done!")
168/27: X.shape
168/28: y.shape
168/29: X.shape
168/30:  y = y[training_index:]
168/31:
y = y[training_index:]
y.shape
169/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
169/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
169/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
169/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
169/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
169/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
169/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
169/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
169/9: features_dic
169/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
169/11:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
169/12:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
169/13:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
169/14: y
170/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
170/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
170/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
170/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
170/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
170/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
170/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
170/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
170/9: features_dic
170/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}}
}#end of 
#----------------------------------------------------------
170/11:
#------------------------the rest of the codes will be in the loop-----------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
for feature, feature_cols in features_dic.items():
    for model_name in list(model_params[feature]['model']):
        clf = model_params[feature]['model'][model_name]['clf']
        params = model_params[feature]['model'][model_name]['params']
        run = model_params[feature]['model'][model_name]['run']
170/12:
prep = Data_prep()
#prep = Data_prep(df, target_col, run_testing)
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
170/13:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
170/14: features_dic.keys
170/15: features_dic.get_keys
170/16: list(features_dic)
170/17: feature_cols = features_dic[feature]
170/18:
feature_cols = features_dic[feature]
feature_cols
170/19:
feature_cols = features_dic[feature]
feature
170/20:
feature_cols = features_dic[feature]
feature_cols
170/21: feature_cols = features_dic[feature]
170/22:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
        "run": True #run the model or fit the saved model
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
        "run": True #run the model or fit the saved model
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
170/23:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
#['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
170/24:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
170/25:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
170/26:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
170/27:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
170/28:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
170/29:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
170/30:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
170/31: list(features_dic)
170/32:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
170/33:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
170/34:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
170/35:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] #sig_df is the  processed data frame with featue_cols 
    #(excluding the test data  and the target column)
    X = X[training_index:]
    y_train = y[:training_index]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
171/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
171/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
171/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
171/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
171/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
#input_files=df.copy()
#---------------------------------------------------------------------------------
171/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
171/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
171/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
171/9: list(features_dic)
171/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
171/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
171/12:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
171/13:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X = X[training_index:]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
171/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.6)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X = X[training_index:]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
171/15:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
171/16:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.6)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X = X[training_index:]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
172/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
172/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
172/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
172/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
172/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
#input_files=df.copy()
#---------------------------------------------------------------------------------
172/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
172/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
172/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
172/9: list(features_dic)
172/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
172/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
172/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
172/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
172/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.6)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X = X[training_index:]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
172/15: df
172/16: df[target_col].info()
172/17: df[target_col].describe()
172/18: y.describe()
172/19:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
172/20: sig = list(df.select_dtypes(include=['int8']).columns)
172/21:
sig = list(df.select_dtypes(include=['int8']).columns)
sig
172/22: df[target_col].describe()
172/23: df[target_col].value_counts()
172/24:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.001)
df = df[:index]
172/25:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.001)
df = df[:index]
172/26:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
172/27:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
172/28:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
172/29: list(features_dic)
172/30:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
172/31:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
172/32:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
172/33:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
172/34:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.6)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X = X[training_index:]
    y = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X)
print("Accuracy", metrics.accuracy_score(y, y_pred))
print("Done!")
172/35:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
174/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
174/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
174/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
174/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
174/5:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.001)
df = df[:index]
174/6:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
175/1:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
175/2:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
176/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
176/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
176/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
176/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
176/5:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.001)
df = df[:index]
176/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
176/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
176/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
176/9: list(features_dic)
176/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
176/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
176/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
176/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
176/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
177/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
177/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
177/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
177/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
177/5:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.003)
df = df[:index]
177/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
177/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
177/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
177/9: list(features_dic)
177/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
177/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
177/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
177/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
177/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
177/15: y.value_counts()
177/16: y
177/17: y[0:30]
177/18: training_index
177/19: y[:4536]
177/20:
y[:4536]
y[4536:]
177/21:
y[:4536]
sum(y[4536:]==y_train)
177/22: len(y[:4536])
177/23: len(y_train)
177/24: sum(y[4536:]==y_train)
177/25: yy=(y[:4536])
177/26: sum(yy==y_train)
177/27: sum(y_pred==y_test)
177/28: len(y_pred)
173/1:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'2.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.001)
df = df[:index]
173/2:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
173/3: from sklearn.base import clone
173/4:
import pandas as pd
import numpy as np
import yaml 
import itertools
173/5:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
173/6:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
173/7:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
173/8:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'2.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.001)
df = df[:index]
173/9:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
173/10:
list_columns=list(input_files)
#list_columns
173/11:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
173/12:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
173/13:
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
173/14:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
173/15:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
173/16:
for feature, feature_set in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)

    ##the rest of the codes below will be here!
173/17:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic[feature]
X_train = input_files_train_contracts[feature_set]
X_test = input_files_test_contracts[feature_set]
173/18:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
173/19:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
173/20:
def acc_model(classifier):
    return cross_val_score(classifier, X_train, y_train,cv=my_cv).mean()
173/21:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
173/22:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
178/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
178/2: from sklearn.base import clone
178/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
178/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
178/5:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
178/6:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
178/7:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'2.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.001)
df = df[:index]
178/8:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
#input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
178/9:
list_columns=list(input_files)
#list_columns
178/10:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
178/11:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
178/12:
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
178/13:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
178/14:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
178/15:
for feature, feature_set in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)

    ##the rest of the codes below will be here!
178/16:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic[feature]
X_train = input_files_train_contracts[feature_set]
X_test = input_files_test_contracts[feature_set]
178/17:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
178/18:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
178/19:
def acc_model(classifier):
    return cross_val_score(classifier, X_train, y_train,cv=my_cv).mean()
178/20:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
178/21:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
177/29:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
177/30:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
177/31:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
177/32:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(file_path, 'wb') as handle:
            pickle.dump(saved_folder+model_name+"_"+feature+'_'+target_col+".model",clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
177/33:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(saved_folder+model_name+"_"+feature+'_'+target_col+".model",clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
177/34:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
177/35:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
179/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
179/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
179/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
179/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
179/5:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.003)
df = df[:index]
179/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
179/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
179/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
179/9: list(features_dic)
179/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
179/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
179/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
179/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
179/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
179/15: clf
179/16: len(X)
179/17: len(X_train)
181/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
181/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
181/3:
#Data_prep.py file
class Data_prep():
    def __init__(self, X = None, y = None):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
181/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
181/5:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.003)
input_files = df[:index]
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
181/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
181/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
181/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
181/9: list(features_dic)
181/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
181/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
181/12:
X=input_files[feature_cols]
y=input_files[target_col]
181/13:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
181/14:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return self
#---------------------------------------------------------------------------------
181/15:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
181/16:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
181/17:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
181/18:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
181/19:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
181/20: list(features_dic)
181/21:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
181/22:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
181/23:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
181/24:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
181/25:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
181/26:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.003)
input_files = df[:index]
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
181/27:
X=input_files[feature_cols]
y=input_files[target_col]
181/28:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
181/29:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
181/30: clone(clf)
181/31: clone(clf)
181/32:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
181/33:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
182/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
182/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
182/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
182/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X_,y_):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
182/5:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
182/6:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
182/7:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
182/8:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
182/9:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
182/10:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
182/11: list(features_dic)
182/12:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
182/13:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
182/14:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
182/15:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
182/16:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
182/17:
X=input_files[feature_cols]
y=input_files[target_col]
182/18:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
183/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
183/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
183/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
183/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
183/5:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.003)
input_files = df[:index]
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
183/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
183/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
183/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
183/9: list(features_dic)
183/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
183/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
183/12:
X=input_files[feature_cols]
y=input_files[target_col]
183/13:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
            pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
183/14:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    #with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
    #        pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
183/15:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
183/16:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    #clf=RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    #with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
    #        pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
183/17:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    #clf=RandomForestClassifier(**params)

    clf.train(X_train, y_train)
    #with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
    #        pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
183/18:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
183/19:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X=prep.X
183/20:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
183/21:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
183/22:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
183/23:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
#X_1=prep.X
184/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
184/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
184/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
184/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
184/5:
df = pd.read_hdf('../2.h5')
index = ceil(len(df)*0.003)
input_files = df[:index]
184/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
184/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
184/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
184/9: list(features_dic)
184/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
184/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
184/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
df=input_files.copy()
184/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
184/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
185/1:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    #clf=RandomForestClassifier(**params)

    clf.train(X_train, y_train)
    #with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
    #        pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
186/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
186/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
186/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
186/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
186/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
186/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
186/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
186/9: list(features_dic)
186/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
186/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
186/12:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
186/13:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/15:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/16:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/17:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
186/18:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
186/19:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
186/20:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
186/21:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
186/22:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
186/23:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
186/24:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
186/25: list(features_dic)
186/26:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
186/27:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
186/28:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
186/29:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/30:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    #clf=RandomForestClassifier(**params)

    clf.train(X_train, y_train)
    #with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
    #        pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/31:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf=RandomForestClassifier(**params)

    clf.train(X_train, y_train)
    #with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
    #        pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/32:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X_1)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf=RandomForestClassifier(**params)

    clf.train(X_train, y_train)
    #with open(saved_folder+model_name+"_"+feature+'_'+target_col+".model", 'wb') as handle:
    #        pickle.dump(clf, handle)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/33:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
186/34:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
186/35:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
186/36:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
186/37:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
186/38:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
186/39:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
186/40:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
186/41: list(features_dic)
186/42:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
186/43:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
186/44:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
186/45:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
186/46:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
186/47:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
188/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
188/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
188/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
188/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
188/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
188/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
188/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
188/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
188/9: list(features_dic)
188/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
188/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
188/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
188/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
188/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
188/15:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
189/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
189/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
189/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
189/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
189/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
189/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
189/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
189/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
189/9: list(features_dic)
189/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
189/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
189/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
189/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
189/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
189/15:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
189/16:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
189/17:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
190/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
190/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
190/3:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
190/4:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
190/5:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
190/6:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
190/7:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
190/8:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
190/9:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
190/10:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
190/11: list(features_dic)
190/12:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
190/13:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
190/14:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
190/15:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
190/16:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
190/17:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
190/18:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
190/19:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
190/20:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
191/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
191/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
191/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
191/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
191/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
191/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
191/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
191/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
191/9: list(features_dic)
191/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
191/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
191/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
191/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
191/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
191/15:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
191/16:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
192/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
192/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
192/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
192/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
192/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
192/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
192/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
192/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
192/9: list(features_dic)
192/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
192/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
192/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
192/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
192/14:
if run_testing== False:
    y = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X_1[:training_index] 
    y_train = y[:training_index]
    X_test = X_1[training_index:]
    y_test = y[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
193/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
193/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
193/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
193/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
193/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
193/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
193/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
193/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
193/9: list(features_dic)
193/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
193/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
193/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
193/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
193/14:
if run_testing== False:
    y_1 = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X_1)*0.7)
    X_train = X_1[:training_index] 
    y_train = y_1[:training_index]
    X_test = X_1[training_index:]
    y_test = y_1[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
194/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
194/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
194/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
194/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
194/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
194/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
194/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
194/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
194/9: list(features_dic)
194/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
194/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
194/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
194/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
194/14:
if run_testing== False:
    y_1 = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X_1)*0.7)
    X_train = X_1[:training_index] 
    y_train = y_1[:training_index]
    X_test = X_1[training_index:]
    y_test = y_1[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
194/15: len(X_1)
194/16: len(X)
194/17: X
194/18:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
195/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
195/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
195/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
195/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
195/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
195/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
195/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
195/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
195/9: list(features_dic)
195/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
195/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
195/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
195/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
195/14:
if run_testing== False:
    y_1 = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X_1)*0.7)
    X_train = X_1[:training_index] 
    y_train = y_1[:training_index]
    X_test = X_1[training_index:]
    y_test = y_1[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
195/15: len(X_1)
195/16: len(X)
195/17: y_1
195/18: len(y_1)
195/19: len(X_train)
195/20: len(X_test)
196/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
196/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
196/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
196/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
196/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
196/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
196/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
196/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
196/9: list(features_dic)
196/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
196/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
196/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
196/13:
prep = Data_prep()
prep.process(feature_cols, df, target_col, run_testing)
X_1=prep.X
196/14:
if run_testing== False:
    y_1 = prep.y
    print("Creating train/test split of data...")
    training_index = ceil(len(X_1)*0.7)
    X_train = X_1[:training_index] 
    y_train = y_1[:training_index]
    X_test = X_1[training_index:]
    y_test = y_1[training_index:]
    #clf.model.set_params(**params)
    clf.train(X_train, y_train)
    #clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
196/15:
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
197/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
197/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
197/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
197/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, X,y):
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
197/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
197/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
197/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
197/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
197/9: list(features_dic)
197/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
197/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
197/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
197/13:
# prep = Data_prep()
# prep.process(feature_cols, df, target_col, run_testing)
# X_1=prep.X
# y_1 = prep.y
197/14:
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
X_1=input_files[feature_cols]
y_1=input_files[target_col]
197/15:
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
X=input_files[feature_cols]
y=input_files[target_col]
197/16:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(X_train, y_train)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
197/17:
y_pred = clf.predict(X_test)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
198/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
198/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
198/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
198/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols, target_col):
        X = sig_df[feat_cols]
        y = sig_df[target_col]
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])


   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
198/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
198/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
198/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
198/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
198/9: list(features_dic)
198/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
198/11:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
198/12:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
198/13:
# prep = Data_prep()
# prep.process(feature_cols, df, target_col, run_testing)
# X_1=prep.X
# y_1 = prep.y
198/14:
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
sig_df=input_files
198/15:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(X)*0.7)
    X_train = X[:training_index] 
    y_train = y[:training_index]
    X_test = X[training_index:]
    y_test = y[training_index:]
    clf.model.set_params(**params)
    clf.train(sig_df, feature_cols, target_col)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)

 #how to load saved models to test 
#clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(sig_df, feature_cols)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
198/16:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(input_files)*0.7)
    sig_df = input_files[:training_index] 
    clf.model.set_params(**params)
    clf.train(sig_df, feature_cols, target_col)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)
    sig_df = input_files[training_index:]

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(sig_df, feature_cols)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
198/17:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(input_files)*0.7)
    sig_df = input_files[:training_index] 
    clf.model.set_params(**params)
    clf.train(sig_df, feature_cols, target_col)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)
    sig_df = input_files[training_index:]

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(sig_df, feature_cols)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_test, sig_df[targe_col]))
print("Done!")
198/18:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(input_files)*0.7)
    sig_df = input_files[:training_index] 
    clf.model.set_params(**params)
    clf.train(sig_df, feature_cols, target_col)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)
    sig_df = input_files[training_index:]

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(sig_df, feature_cols)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_pred, sig_df[targe_col]))
print("Done!")
198/19:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(input_files)*0.7)
    sig_df = input_files[:training_index] 
    clf.model.set_params(**params)
    clf.train(sig_df, feature_cols, target_col)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)
    sig_df = input_files[training_index:]

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(sig_df, feature_cols)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_pred, sig_df[target_col]))
print("Done!")
198/20: features_dic
198/21: list(features_dic)
198/22:
with open('../features_dic.txt', 'wb') as handle:
    pickle.dump(features_dic, handle)
198/23:
with open('../features_dic.txt', 'wb') as handle:
    pickle.dump(features_dic, handle)
    
with open('../features_dic.txt', 'rb') as handle:
    b = pickle.loads(handle.read())
b==features_dic
198/24:
with open('../features_dic.txt', 'wb') as handle:
    pickle.dump(features_dic, handle)
    
with open('../features_dic.txt', 'rb') as handle:
    b = pickle.loads(handle.read())
b==features_dic
b
198/25:
with open('../features_dic.txt', 'wb') as handle:
    pickle.dump(features_dic, handle)
    
with open('../features_dic.txt', 'rb') as handle:
    b = pickle.loads(handle.read())
b==features_dic
200/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'Ada' #the model used for training to testing
200/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
200/3:
#Data_prep.py file
class Data_prep():
    def __init__(self):
            self. X = None
            self. y = None
            
            
    def process(self, list_columns, data, target_col, run_testing):
        columns = list_columns
        if run_testing == False:
          #removing zeros, data is imbalanced
            data = data.drop(data[data[target_col] ==0].index)
            columns.append(target_col)
            #---------------------------------------------------------------------------------
        data = data[columns]
       #extracting features that needs to be categorical
        sig = list(data.select_dtypes(include=['int8']).columns)
#         # -1 is replaced with 0
#         # 0 is  replaced with 1
#         # 1 is replaced with 2
        data[sig] = data[sig].apply(lambda x: x+1)
        self.X = data[list_columns]
        if run_testing == False:
            self.y = data[target_col]
        return None
#---------------------------------------------------------------------------------
200/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols, target_col):
        X = sig_df[feat_cols]
        y = sig_df[target_col]
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])


   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
200/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
200/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
200/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
200/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
200/9: list(features_dic)
200/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'Ada':
     {  "clf": AdaBoostClassifierWrapper(),
        #************
         'params':{'base_estimator':None,
                'n_estimators':50,
                'learning_rate':1.0,
                'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
200/11:
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
import pickle
from sklearn.base import clone
#-------------------------------------------------------------------------
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols, target_col):
        X = sig_df[feat_cols]
        y = sig_df[target_col]
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])


   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )
#----------------------------------------------------------------------------
class AdaBoostClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 base_estimator=None,
                n_estimators=50,
                learning_rate=1.0,
                random_state=None
                ):
        super().__init__()
        self.model = AdaBoostClassifier(
                base_estimator=base_estimator,
                n_estimators=n_estimators,
                learning_rate=learning_rate,
                random_state=random_state)
#---------------------------------------------------------------------------------
200/12:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'Ada':
     {  "clf": AdaBoostClassifierWrapper(),
        #************
         'params':{'base_estimator':None,
                'n_estimators':50,
                'learning_rate':1.0,
                'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
200/13:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
200/14:
# #------------------------the rest of the codes will be in the loop-----------------------
# #-----------------------------------------------------------------------------------
# #-----------------------------------------------------------------------------------
# for feature, feature_cols in features_dic.items():
#     for model_name in list(model_params[feature]['model']):
#         clf = model_params[feature]['model'][model_name]['clf']
#         params = model_params[feature]['model'][model_name]['params']
200/15:
# prep = Data_prep()
# prep.process(feature_cols, df, target_col, run_testing)
# X_1=prep.X
# y_1 = prep.y
200/16:
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
sig_df=input_files
200/17:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(input_files)*0.7)
    sig_df = input_files[:training_index] 
    clf.model.set_params(**params)
    clf.train(sig_df, feature_cols, target_col)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)
    sig_df = input_files[training_index:]

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(sig_df, feature_cols)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_pred, sig_df[target_col]))
print("Done!")
200/18:
with open('../model_params.txt', 'wb') as handle:
    pickle.dump(model_params, handle)
    
with open('../model_params.txt', 'rb') as handle:
    b = pickle.loads(handle.read())
b==model_params
200/19:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'Ada':
     {  "clf": AdaBoostClassifierWrapper(),
        #************
         'params':{'base_estimator':None,
                'n_estimators':50,
                'learning_rate':1.0,
                'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
200/20:
with open('../model_params.txt', 'wb') as handle:
    pickle.dump(model_params, handle)
    
with open('../model_params.txt', 'rb') as handle:
    b = pickle.loads(handle.read())
b==model_params
200/21:
with open('../model_params.txt', 'wb') as handle:
    pickle.dump(model_params, handle)
    
with open('../model_params.txt', 'rb') as handle:
    a = pickle.loads(handle.read())
a==model_params
200/22:
with open('../model_params.txt', 'wb') as handle:
    pickle.dump(model_params, handle)
    
with open('../model_params.txt', 'rb') as handle:
    a = pickle.loads(handle.read())
a==model_params
200/23: b
200/24: model_params
200/25: b
201/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'Ada' #the model used for training to testing
201/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
201/3:
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
import pickle
from sklearn.base import clone
#-------------------------------------------------------------------------
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols, target_col):
        X = sig_df[feat_cols]
        y = sig_df[target_col]
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])


   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )
#----------------------------------------------------------------------------
class AdaBoostClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 base_estimator=None,
                n_estimators=50,
                learning_rate=1.0,
                random_state=None
                ):
        super().__init__()
        self.model = AdaBoostClassifier(
                base_estimator=base_estimator,
                n_estimators=n_estimators,
                learning_rate=learning_rate,
                random_state=random_state)
#---------------------------------------------------------------------------------
201/4:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
201/5:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
201/6:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
201/7:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
201/8: list(features_dic)
201/9:
with open('../features_dic.txt', 'wb') as handle:
    pickle.dump(features_dic, handle)
    
with open('../features_dic.txt', 'rb') as handle:
    b = pickle.loads(handle.read())
b==features_dic
201/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'blablah':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
201/11:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'Ada':
     {  "clf": AdaBoostClassifierWrapper(),
        #************
         'params':{'base_estimator':None,
                'n_estimators':50,
                'learning_rate':1.0,
                'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
201/12:
with open('../model_params.txt', 'wb') as handle:
    pickle.dump(model_params, handle)
    
with open('../model_params.txt', 'rb') as handle:
    a = pickle.loads(handle.read())
a==model_params
201/13:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'Ada':
     {  "clf": AdaBoostClassifierWrapper(),
        #************
         'params':{'base_estimator':None,
                'n_estimators':50,
                'learning_rate':1.0,
                'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
201/14:
with open('../model_params.txt', 'wb') as handle:
    pickle.dump(model_params, handle)
    
with open('../model_params.txt', 'rb') as handle:
    a = pickle.loads(handle.read())
a==model_params
201/15: b
201/16: a['MOVING_AVG']==model_params['MOVING_AVG']
201/17: a['MOVING_AVG']
201/18: model_params['MOVING_AVG']
199/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/main.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
199/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/main.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
199/3: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/main.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
199/4: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/main.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
199/5: df[target_col]
199/6: df[target_col].dtypes
199/7: df[sig].dtypes
199/8: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/main.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
199/9: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/main.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
199/10: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline/main.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipeline')
199/11: runfile('/Users/shahla/Desktop/untitled5.py', wdir='/Users/shahla/Desktop')
199/12: runfile('/Users/shahla/Desktop/untitled5.py', wdir='/Users/shahla/Desktop')
199/13: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/14: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/15: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/16: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/17: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/18: j
199/19: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/20: k
199/21: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/22: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/23: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/24: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
199/25: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe/untitled5.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/Pipe')
204/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
204/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
204/3:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols, target_col):
        X = sig_df[feat_cols]
        y = sig_df[target_col]
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])


   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
204/4:
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
import pickle
from sklearn.base import clone
#-------------------------------------------------------------------------
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols, target_col):
        X = sig_df[feat_cols]
        y = sig_df[target_col]
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])


   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )
#----------------------------------------------------------------------------
class AdaBoostClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 base_estimator=None,
                n_estimators=50,
                learning_rate=1.0,
                random_state=None
                ):
        super().__init__()
        self.model = AdaBoostClassifier(
                base_estimator=base_estimator,
                n_estimators=n_estimators,
                learning_rate=learning_rate,
                random_state=random_state)
#---------------------------------------------------------------------------------
204/5:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
204/6:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
204/7:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
204/8:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
204/9: list(features_dic)
204/10:
with open('../features_dic.txt', 'wb') as handle:
    pickle.dump(features_dic, handle)
    
with open('../features_dic.txt', 'rb') as handle:
    b = pickle.loads(handle.read())
b==features_dic
204/11:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'Ada':
     {  "clf": AdaBoostClassifierWrapper(),
        #************
         'params':{'base_estimator':None,
                'n_estimators':50,
                'learning_rate':1.0,
                'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
204/12:
with open('../model_params.txt', 'wb') as handle:
    pickle.dump(model_params, handle)
    
with open('../model_params.txt', 'rb') as handle:
    a = pickle.loads(handle.read())
204/13:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
204/14:
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
sig_df=input_files
204/15:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(input_files)*0.7)
    sig_df = input_files[:training_index] 
    clf.model.set_params(**params)
    clf.train(sig_df, feature_cols, target_col)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)
    sig_df = input_files[training_index:]

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(sig_df, feature_cols)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_pred, sig_df[target_col]))
print("Done!")
205/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
205/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
import pickle
from sklearn.base import clone
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
205/3:

#-------------------------------------------------------------------------
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None

    def train(self, sig_df, feat_cols, target_col):
        X = sig_df[feat_cols]
        y = sig_df[target_col]
        clone(self.model)
        self.model.fit(X,y)
        return None
    def predict(self, X, feat_cols):
        return self.model.predict(X[feat_cols])


   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )
#----------------------------------------------------------------------------
class AdaBoostClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 base_estimator=None,
                n_estimators=50,
                learning_rate=1.0,
                random_state=None
                ):
        super().__init__()
        self.model = AdaBoostClassifier(
                base_estimator=base_estimator,
                n_estimators=n_estimators,
                learning_rate=learning_rate,
                random_state=random_state)
#---------------------------------------------------------------------------------
205/4:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
205/5:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
205/6:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
205/7:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
205/8: list(features_dic)
205/9:
with open('../features_dic.txt', 'wb') as handle:
    pickle.dump(features_dic, handle)
    
with open('../features_dic.txt', 'rb') as handle:
    b = pickle.loads(handle.read())
b==features_dic
205/10:
#we need to creat a dictionary
#to be updated once we have all the parameter tunning for all the methods
model_params = {
    "MOVING_AVG":
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     },  
     #---
     'Ada':
     {  "clf": AdaBoostClassifierWrapper(),
        #************
         'params':{'base_estimator':None,
                'n_estimators':50,
                'learning_rate':1.0,
                'random_state':50
       },
       # ************
     }
    }},
   #----------------------------------------------------------------------------------
        "CCI":    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
    "RSI": 
  {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'CMO':    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'STOCH':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_2':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'BBI_3':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#-----------------------------------------------------------------------------------------
   'INV_MOVING_AVG':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CCI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_RSI':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_CMO':   {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}},
#---------------------------------------------------------------------------------------
   'INV_STOCH':  
    {'model': {'RF':
     {  "clf": RandomForestClassifierWrapper(),
        #************
         'params':{'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':-1, 
            'random_state':50
              },#end of 'params'
       # ************
     }}}
}#end of 
#----------------------------------------------------------
205/11:
with open('../model_params.txt', 'wb') as handle:
    pickle.dump(model_params, handle)
    
with open('../model_params.txt', 'rb') as handle:
    a = pickle.loads(handle.read())
205/12:
feature_cols = features_dic[feature]
clf = model_params[feature]['model'][model_name]['clf']
params = model_params[feature]['model'][model_name]['params']
205/13:
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
sig_df=input_files
205/14:
if run_testing== False:
    print("Creating train/test split of data...")
    training_index = ceil(len(input_files)*0.7)
    sig_df = input_files[:training_index] 
    clf.model.set_params(**params)
    clf.train(sig_df, feature_cols, target_col)
    clf.save(saved_folder+model_name+"_"+feature+'_'+target_col+".model")
    print("Done training!", model_name)
    sig_df = input_files[training_index:]

 #how to load saved models to test 
clf.load(saved_folder+model_name+"_"+feature+'_'+target_col+'.model')
y_pred = clf.predict(sig_df, feature_cols)
if run_testing== False:
    print("Accuracy", metrics.accuracy_score(y_pred, sig_df[target_col]))
print("Done!")
207/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json

## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
207/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
207/3:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'trump',  
        'result_type': 'mixed',#mixed recent popular
        'count': 100,
        'lang': 'en',
        }
207/4:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'politics',  
        'result_type': 'mixed',#mixed recent popular
        'count': 100,
        'lang': 'en',
        }
207/5:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
207/6:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
207/7: data=df.copy()
207/8:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
207/9: data.append(df)
207/10: df
207/11:
if df.user='JohnBrennan':
    print(df.text)
207/12: df[0]
207/13: df.user
207/14: df.user[0]
207/15: data.user[0]
207/16: df.user.str.match('JamilSmith')
207/17: sum(df.user.str.match('JamilSmith'))
207/18: sum(data.user.str.match('JamilSmith'))
207/19:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
207/20:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
207/21:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
207/22: data.append(df)
207/23:
data.append(df)
data.shape()
207/24: data.shape
207/25: data=data.append(df)
207/26: data.shape
207/27: sum(data.user.str.match('JamilSmith'))
207/28:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
207/29:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
207/30: data=data.append(df)
207/31: data.shape
207/32: sum(data.user.str.match('JamilSmith'))
207/33: screen_name = "NASA"
207/34:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
207/35:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
207/36: creds
207/37:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
208/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
208/2:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
208/3:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
209/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
209/2:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
209/3:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
209/4: creds
209/5:
#https://github.com/eleanorstrib/twitter_timeline_analysis_1/blob/master/twitter_timeline_to_csv.ipynb
screen_name = "NASA"
209/6:
t = twitter.Api(
    consumer_key = creds['TWITTER_CONS_KEY'],
    consumer_secret = creds['TWITTER_CONS_SEC'],
    access_token_key = creds['TWITTER_ACCESS_TOKEN'], 
    access_token_secret = creds['TWITTER_ACCESS_SEC'],
    tweet_mode=creds['extended'] # this ensures that we get the full text of the users' original tweets
)
209/7:
twitter = Twitter(
      auth=OAuth(token, token_key, con_secret, con_secret_key))

  # Get the public timeline
  twitter.statuses.public_timeline()
209/8:
twitter = Twitter(
auth=OAuth(token, token_key, con_secret, con_secret_key))

  # Get the public timeline
twitter.statuses.public_timeline()
209/9:
twitter = Twitter(
auth=OAuth(token, token_key, con_secret, con_secret_key))

  # Get the public timeline
twitter.statuses.public_timeline()
209/10:
import python-twitter
twitter = Twitter(
auth=OAuth(token, token_key, con_secret, con_secret_key))

  # Get the public timeline
twitter.statuses.public_timeline()
209/11:
import python-twitter
twitter = Twitter(auth=OAuth(token, token_key, con_secret, con_secret_key))

  # Get the public timeline
#twitter.statuses.public_timeline()
209/12:
import python-twitter
#twitter = Twitter(auth=OAuth(token, token_key, con_secret, con_secret_key))

  # Get the public timeline
#twitter.statuses.public_timeline()
209/13:
import Twitter
twitter = Twitter(auth=OAuth(token, token_key, con_secret, con_secret_key))

  # Get the public timeline
#twitter.statuses.public_timeline()
209/14:
t =Twitter.Api(
    consumer_key = creds['TWITTER_CONS_KEY'],
    consumer_secret = creds['TWITTER_CONS_SEC'],
    access_token_key = creds['TWITTER_ACCESS_TOKEN'], 
    access_token_secret = creds['TWITTER_ACCESS_SEC'],
    tweet_mode=creds['extended'] # this ensures that we get the full text of the users' original tweets
)
210/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
#credentials = {}  
#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

## Save the credentials object to file
#with open("twitter_credentials.json", "w") as file:  
#    json.dump(credentials, file)
210/2:
t =twitter.Api(
    consumer_key = creds['TWITTER_CONS_KEY'],
    consumer_secret = creds['TWITTER_CONS_SEC'],
    access_token_key = creds['TWITTER_ACCESS_TOKEN'], 
    access_token_secret = creds['TWITTER_ACCESS_SEC'],
    tweet_mode=creds['extended'] # this ensures that we get the full text of the users' original tweets
)
210/3:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
210/4:
t =twitter.Api(
    consumer_key = creds['TWITTER_CONS_KEY'],
    consumer_secret = creds['TWITTER_CONS_SEC'],
    access_token_key = creds['TWITTER_ACCESS_TOKEN'], 
    access_token_secret = creds['TWITTER_ACCESS_SEC'],
    tweet_mode=creds['extended'] # this ensures that we get the full text of the users' original tweets
)
210/5: creds
210/6:
t =twitter.Api(
    consumer_key = creds['CONSUMER_KEY'],
    consumer_secret = creds['CONSUMER_SECRET'],
    access_token_key = creds['ACCESS_TOKEN'], 
    access_token_secret = creds['ACCESS_SECRET'],
    tweet_mode=creds['extended'] # this ensures that we get the full text of the users' original tweets
)
210/7:
t =twitter.Api(
    consumer_key = creds['CONSUMER_KEY'],
    consumer_secret = creds['CONSUMER_SECRET'],
    access_token_key = creds['ACCESS_TOKEN'], 
    access_token_secret = creds['ACCESS_SECRET'],
    tweet_mode='extended' # this ensures that we get the full text of the users' original tweets
)
210/8:

screen_name = "NASA"
210/9:
# Call the Twitter API using the python-twitter library
first_200 = t.GetUserTimeline(screen_name=screen_name, count=200)
210/10:
#https://github.com/eleanorstrib/twitter_timeline_analysis_1/blob/master/twitter_timeline_to_csv.ipynb
#account that I want to analyze https://twitter.com/politics
screen_name = "politics"
210/11:
t =twitter.Api(
    consumer_key = creds['CONSUMER_KEY'],
    consumer_secret = creds['CONSUMER_SECRET'],
    access_token_key = creds['ACCESS_TOKEN'], 
    access_token_secret = creds['ACCESS_SECRET'],
    tweet_mode='extended' # this ensures that we get the full text of the users' original tweets
)
210/12:
# Call the Twitter API using the python-twitter library
first_200 = t.GetUserTimeline(screen_name=screen_name, count=200)
210/13:
# Let's explore the data!
print("There are %d tweets in our dataset, which has the type %s." % (len(first_200), type(first_200)))
print()
print("Here's a sample of what the data from one tweet looks like:")
print(first_200[0])
print()
print("Each tweet is stored in a %s data structure." % type(first_200[0]))
210/14:
# Let's explore the data!
print("There are %d tweets in our dataset, which has the type %s." % (len(first_200), type(first_200)))
print('--------------------------------------')
print("Here's a sample of what the data from one tweet looks like:")
print(first_200[0])
print()
print("Each tweet is stored in a %s data structure." % type(first_200[0]))
210/15:
# Let's explore the data!
print("There are %d tweets in our dataset, which has the type %s." % (len(first_200), type(first_200)))
print('--------------------------------------')
print("Here's a sample of what the data from one tweet looks like:")
print(first_200[0])
print('--------------------------------------')
print("Each tweet is stored in a %s data structure." % type(first_200[0]))
210/16:
def get_tweets(first_200, screen_name, last_id):
    all_tweets = []
    all_tweets.extend(first_200)
    for i in range(900):
        new = t.GetUserTimeline(screen_name=screen_name, max_id=last_id-1)
        if len(new) > 0:
            all_tweets.extend(new)
            last_id = new[-1].id
        else:
            break
    
    return all_tweets
210/17:
# let's run our function!
all_tweets = get_tweets(first_200, screen_name, first_200[-1].id)
210/18:
# now to check the data - we should have more than 200 tweets.
print("There are %d tweets stored in a list as the all_tweets variable." % len(all_tweets))
print("The most recent tweet in our collection was sent %s and the oldest tweet was sent %s." % (
                                                                            all_tweets[0].created_at, 
                                                                            all_tweets[-1].created_at)
     )
210/19: all_tweetss = get_tweets(all_tweets, screen_name, all_tweets[-1].id)
210/20:
# now to check the data - we should have more than 200 tweets.
print("There are %d tweets stored in a list as the all_tweets variable." % len(all_tweetss))
print("The most recent tweet in our collection was sent %s and the oldest tweet was sent %s." % (
                                                                            all_tweetss[0].created_at, 
                                                                            all_tweetss[-1].created_at)
     )
210/21: all_tweetss = get_tweets(all_tweets, screen_name, all_tweets[-1].id)
210/22:
# now to check the data - we should have more than 200 tweets.
print("There are %d tweets stored in a list as the all_tweets variable." % len(all_tweetss))
print("The most recent tweet in our collection was sent %s and the oldest tweet was sent %s." % (
                                                                            all_tweetss[0].created_at, 
                                                                            all_tweetss[-1].created_at)
     )
210/23: all_tweets[-1].id
210/24: all_tweetss[-1].id
210/25: first_200[-1].id
210/26: new = t.GetUserTimeline(screen_name=screen_name, max_id=all_tweets[-1].id-1)
210/27: new = t.GetUserTimeline(screen_name=screen_name, max_id=all_tweets[-1].id-1)
210/28: len(new)
210/29: print(all_tweets[0])
210/30:
print("Data in the created_at attribute looks like this:", all_tweets[1].created_at)
print("Data in the hashtags attribute looks like this:", all_tweets[1].hashtags)
print("Data in the urls attribute looks like this:", all_tweets[1].urls)
print("Data in the source attribute looks like this:", all_tweets[1].source)
210/31:
def clean_hashtags(hashtags):
    """
    Turns data with any number of hashtags like this - [Hashtag(Text='STEMonStation')] - to a list like this -
    ['STEMonStation']
    """
    cleaned = []
    if len(hashtags) >= 1:
        for i in range(len(hashtags)):
            cleaned.append(hashtags[i].text)        
    return cleaned

def clean_urls(urls):
    """
    Turns data with any number of expanded urls like this - 
    [URL(URL=https://t.co/sYCFHKxzBf, ExpandedURL=https://youtu.be/34bFgA3H3hQ)]- to a list like this - 
    ["https://youtu.be/34bFgA3H3hQ"]
    """
    cleaned = []
    if len(urls) >= 1:
        for i in range(len(urls)):
            cleaned.append(urls[i].expanded_url)
    return(cleaned)
        

def clean_source(source):
    """
    Turns data including the source and some html like this - 
    <a href="https://www.sprinklr.com" rel="nofollow">Sprinklr</a> - to a list like this -
    ['Sprinklr']
    """
    raw = lxml.html.document_fromstring(source)
    return raw.cssselect('body')[0].text_content()


def string_to_datetime(date_str):
    """
    Turns a string including date and time like this - Sun Jul 01 21:06:07 +0000 2018 - to a Python datetime object
    like this - datetime.datetime(2018, 7, 1, 21, 6, 7, tzinfo=datetime.timezone.utc)
    """
    return datetime.strptime(date_str, '%a %b %d %H:%M:%S %z %Y')
210/32:
def write_to_csv(tweets, filename):
    # the headers are the fields that we identified in step 4
    headers = ['id', 'full_text', 'hashtags', 'urls', 'created_at', 'favorite_count', 'retweet_count', 'source']
    
    # here we create the file and write the header row with the headers list
    # note that the 'filename' argument will be the name of the csv file
    with open(filename + '.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile, delimiter=',')
        writer.writerow(headers)
        
        # in this loop, we write a new row for each tweet object, with the data taken from the tweet object in 
        # the order we listed the headers
        # note where we call the helper functions from step 4 on hashtags, urls, and source
        for item in tweets:
            writer.writerow([item.id, 
                             item.full_text, 
                             clean_hashtags(item.hashtags), 
                             clean_urls(item.urls), 
                             item.created_at, 
                             item.favorite_count, 
                             item.retweet_count, 
                             clean_source(item.source)])
    csvfile.close()
210/33:
# now we call the function, passing in the all_tweets list
# here the filename will be the screen_name variable defined in step 2 with "_tweets" after it (e.g. NASA_tweets.csv),
# but you can change it to whatever you want
write_to_csv(all_tweets, screen_name + '_tweets')
210/34:
def create_dict(tweets):
    dict = {}
    for item in tweets:
        clean_source(item.source)
        dict[item.id_str] = {
            'full_text': item.full_text,
            'hashtags': clean_hashtags(item.hashtags),
            'urls': clean_urls(item.urls),
            'created_at': string_to_datetime(item.created_at),
            'favorite_count': item.favorite_count,
            'retweet_count' : item.retweet_count,
            'source': clean_source(item.source)
        }
    return dict
210/35: tweet_dict = create_dict(all_tweets)
210/36: all_tweetss = get_tweets(all_tweets, screen_name, all_tweets[-1].id)
210/37: all_tweetss = get_tweets(all_tweets, screen_name, all_tweets[-1].id)
210/38:
# now to check the data - we should have more than 200 tweets.
print("There are %d tweets stored in a list as the all_tweets variable." % len(all_tweetss))
print("The most recent tweet in our collection was sent %s and the oldest tweet was sent %s." % (
                                                                            all_tweetss[0].created_at, 
                                                                            all_tweetss[-1].created_at)
     )
210/39: all_tweetss = get_tweets(all_tweets, screen_name, all_tweets[-1].id)
210/40: all_tweetss = get_tweets(all_tweets, screen_name, all_tweets[-1].id)
210/41:
# now to check the data - we should have more than 200 tweets.
print("There are %d tweets stored in a list as the all_tweets variable." % len(all_tweetss))
print("The most recent tweet in our collection was sent %s and the oldest tweet was sent %s." % (
                                                                            all_tweetss[0].created_at, 
                                                                            all_tweetss[-1].created_at)
     )
211/1: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
211/2: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
210/42:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])

with open("stream_politics.json", "r") as file:  
    pol = json.load(file)
210/43:
with open("stream_politics.json", "r") as file:  
    pol = json.load(file)
210/44:
with open("stream_politics.json", "r") as file:  
    pol = json.load(file)
F = open("stream_politics.json",r)
210/45:
#with open("stream_politics.json", "r") as file:  
#    pol = json.load(file)
F = open("stream_politics.json",'r')
210/46: F
210/47: F[0]
210/48: type(F)
210/49:
#with open("stream_politics.json", "r") as file:  
#    pol = json.load(file)
F = open("stream_politics.txt",'r')
210/50:
#with open("stream_politics.json", "r") as file:  
#    pol = json.load(file)
F = open("stream_politics.txt",'r')
210/51: type(F)
211/3: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
210/52:
import pandas as pd
df = pd.read_json("stream_politics.json")
210/53:
import pandas as pd
df = pd.read_json("stream_politics.json",lines=True)
210/54: df
210/55: df.text[0]
210/56: df.text[1]
210/57: df.text[2]
210/58: df.text[4]
210/59: df.text[10]
210/60: df.text[30]
210/61: df.text[90]
210/62: list(df.columns)
210/63: df.withheld_in_countries
210/64: df.source
210/65: df.lang
210/66:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
210/67: df
211/4: ?Stream
210/68: df.shape
210/69: df.text
210/70: df[df['text'].str.contains("politics")]
210/71: df[df['text'].str.contains("politics")][0]
210/72: df[df['text'].str.contains("politics")]
210/73: df[df['text'].str.contains("politics")][4]
210/74: x=df[df['text'].str.contains("politics")]
210/75: type(x)
210/76: x.shape
210/77: x.text
210/78: x.text[0]
210/79: x.text
210/80: x.columns
210/81: df[0]
210/82: df.text[0]
210/83: x.text[0]
210/84: x.text
210/85: df.text[4]
210/86: df.place
210/87: df.in_reply_to_status_id_str
210/88: df.retweeted
210/89: df.text[1]
210/90: df.text[20]
210/91: tweet_dict
210/92: tweet_dict[full_text]
210/93: tweet_dict['full_text']
210/94: all_tweets.full_text
210/95: all_tweets
210/96:
for item in all_tweets
print(item.full_text)
210/97:
for item in all_tweets:
    print(item.full_text)
210/98: x[1][3]
210/99: x[1]
210/100: x.text[1]
210/101: x.text[1][11:13]
210/102:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'
ACCESS_SECRET
# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
210/103:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
210/104:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
210/105:
import pandas as pd
df = pd.read_json("stream_politics.json",lines=True)
210/106: df.shape
210/107: df.text[20]
212/1:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
212/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    creds = json.load(file)

# Instantiate an object
python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])
212/3:
import pandas as pd
df = pd.read_json("stream_politics.json",lines=True)
212/4: df.shape
212/5: df.text[20]
212/6: list_columns=list(df.columns)
212/7:
list_columns=list(df.columns)
list
212/8:
list_columns=list(df.columns)
list_columns
212/9: en_lang=df.lang==en
212/10: en_lang=df.lang=='en'
212/11: data=df[en_lang]
212/12: data.shape
212/13: data=df[df.lang=='en']
212/14: data.shape
212/15: data.head()
212/16: data.created_at
212/17: max(data.created_at)
212/18: print('time range tweet written', min(data.created_at), max(data.created_at))
212/19: print('time range tweet written is from', min(data.created_at),'to', max(data.created_at))
212/20: print('The time range tweet written is from', min(data.created_at),'to', max(data.created_at))
212/21:
# Create our query
#We'll use only four arguments in the query: 
#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.
#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html
query = {'q': 'politics',  
        'result_type': 'mixed',#mixed recent popular
        'count': 100,
        'lang': 'en',
        }
212/22:
import pandas as pd

# Search tweets
dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  
for status in python_tweets.search(**query)['statuses']:  
    dict_['user'].append(status['user']['screen_name'])
    dict_['date'].append(status['created_at'])
    dict_['text'].append(status['text'])
    dict_['favorite_count'].append(status['favorite_count'])
212/23:
# Structure data in a pandas DataFrame for easier manipulation
df = pd.DataFrame(dict_)  
df.sort_values(by='favorite_count', inplace=True, ascending=False)  
df.shape
212/24: sum(data.user.str.match('JamilSmith'))
212/25: data
212/26: data.text
212/27:
# # Structure data in a pandas DataFrame for easier manipulation
# df = pd.DataFrame(dict_)  
# df.sort_values(by='favorite_count', inplace=True, ascending=False)  
# df.shape
212/28:
import pandas as pd
df = pd.read_json("stream_politics.json",lines=True)
212/29: df.shape
212/30:
import pandas as pd
df = pd.read_json("stream_politics.json",lines=True)
212/31: df.shape
212/32:
list_columns=list(df.columns)
list_columns
212/33:
#keep English languages
data=df[df.lang=='en']
212/34: data.shape
212/35: print('The time range tweet written is from', min(data.created_at),'to', max(data.created_at))
212/36: data.text
212/37: data.text[0]
212/38: data.text.head()
212/39: data.text[1]
212/40: data.text[2]
212/41: data.text[3]
212/42: data.text[20]
212/43: data.text[20][0:100]
212/44: data.text[20][0:200]
212/45: data.text[20][0:300]
212/46: data.text[22]
212/47: data.text[27]
213/1: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
213/2: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
213/3: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
214/1: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
214/2: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
212/48:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
212/49: df.shape
212/50:
list_columns=list(df.columns)
list_columns
212/51:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
212/52: df.shape
212/53:
list_columns=list(df.columns)
list_columns
212/54: data.text[27]
212/55:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
212/56: df.shape
212/57:
list_columns=list(df.columns)
list_columns
212/58:
#keep English languages
data=df[df.lang=='en']
212/59: data.shape
212/60: print('The time range tweet written is from', min(data.created_at),'to', max(data.created_at))
212/61: data.text[27]
212/62: data.retweeted_status
212/63:
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
             try:
                 tweet = status.retweeted_status.extended_tweet["full_text"]
             except:
                 tweet = status.retweeted_status.text
        else:
             try:
                tweet = status.extended_tweet["full_text"]
             except AttributeError:
                tweet = status.text
212/64:
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
             try:
                    tweet = status.retweeted_status.extended_tweet["full_text"]
                except:
                    tweet = status.retweeted_status.text
        else:
             try:
                tweet = status.extended_tweet["full_text"]
                except AttributeError:
                tweet = status.text
212/66:
 def on_status(self, status):
    if hasattr(status, 'retweeted_status'):
        try:
            tweet = status.retweeted_status.extended_tweet["full_text"]
            except:
                tweet = status.retweeted_status.text
                else:
                    try:
                        tweet = status.extended_tweet["full_text"]
                        except AttributeError:
                            tweet = status.text
212/67:
import tweepy
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import config
import json
212/68:
import tweepy
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                print(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
            except:
                tweet = status.retweeted_status.text
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/69:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
212/70:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
212/71:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                print(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
            except:
                tweet = status.retweeted_status.text
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/72:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
212/73:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                print(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                 print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/74:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                print(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/75:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/76:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/77:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
212/78: df.shape
212/79:
#keep English languages
data=df[df.lang=='en']
212/80: data.retweeted_status
212/81: data.text[27]
212/82:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                print(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/83:
df = pd.DataFrame(columns=['tweet']
df
212/84:
df = pd.DataFrame(columns=['tweet']
df
212/85:
df = pd.DataFrame(columns=['tweet'])
df
212/86:
df = pd.DataFrame(columns=['tweet'])
df
df.append('dddd')
212/87:
df = pd.DataFrame(columns=['tweet'])
df
df.append(['dddd'])
212/88:
df = pd.DataFrame(columns=['tweet'])
df
df.append(['dddd'],axis=0)
212/89:
df = pd.DataFrame(columns=['tweet'])
df
df.append(['dddd'],ignore_index=True)
212/90:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df_2.tweet='bbbb'
df.append(df_2,ignore_index=True)
212/91:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df2.tweet='bbbb'
df.append(df_2,ignore_index=True)
212/92:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df2.tweet='bbbb'
df.append(df2,ignore_index=True)
212/93:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df2.tweet='bbbb'
df.append(df2,ignore_index=True)
df
212/94:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df2.tweet='bbbb'
df.append(df2,ignore_index=True)
df2
212/95:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=['ddddkjkjfkj'] 
df.append(df2,ignore_index=True)
df
212/96:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=['ddddkjkjfkj'] 
df.append(df2,ignore_index=True)
df2
212/97:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=['ddddkjkjfkj'] 
df.append(df2)
df2
212/98:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=['ddddkjkjfkj'] 
df.append(df2)
df
212/99:
df = pd.DataFrame(columns=['tweet'])
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=['ddddkjkjfkj'] 
df.append(df2,ignore_index=True)
df
212/100:
df = pd.DataFrame(columns=['tweet'])
df.loc[len(df2)]=['xxx jnjcn kjfkj'] 
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=['ddddkjkjfkj'] 
df.append(df2,ignore_index=True)
df
212/101:
df = pd.DataFrame(columns=['tweet'])
df.loc[len(df2)]=['xxx jnjcn kjfkj'] 
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=['ddddkjkjfkj'] 
df.append(df2,ignore_index=True)
df2
212/102:
df = pd.DataFrame(columns=['tweet'])
df.loc[len(df2)]=['xxx jnjcn kjfkj'] 
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=['ddddkjkjfkj'] 
df= df.append(df2,ignore_index=True)
df
212/103:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, status):
        try:
            with open(self.outfile, 'a') as f:
                f.write(status.retweeted_status.extended_tweet["full_text"])
                print(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/104:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
212/105:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/106:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                print(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                with open('fetched_tweets.txt','a') as tf:
                    #tf.write(status.full_text.encode('utf-8') + '\n')
                    tf.write(status.retweeted_status.extended_tweet["full_text"]+'\n')
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/107:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
212/108:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/109:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/110: f= open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/fetched_tweets.txt',"w+")
212/111:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        #print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)
            open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/fetched_tweets.txt','a+') as tf:
                    tf.write(tweet+'\n')
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/112:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        #print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)
            with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/fetched_tweets.txt','a+') as tf:
                    tf.write(tweet+'\n')
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/113:
tweet='ffff'
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/fetched_tweets.txt','a+') as tf:
                    tf.write(tweet+'\n')
212/114:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        #print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)
            with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/fetched_tweets.txt','a+') as tf:
                    tf.write(tweet+'\n')
        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text
212/115:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
212/116:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/117:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        #print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)

        else:
            try:
                tweet = status.extended_tweet["full_text"]
            except AttributeError:
                tweet = status.text  
        with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/fetched_tweets.txt','a+') as tf:
                tf.write(tweet+'\n')
212/118:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
212/119:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/120:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        #print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)

        else:
            try:
                tweet = status.extended_tweet["full_text"]
                print(tweet)
            except AttributeError:
                tweet = status.text  
        with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/fetched_tweets.txt','a+') as tf:
                tf.write(tweet+'\n')
212/121:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
212/122:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/123:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        #print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)

        else:
            try:
                tweet = status.extended_tweet["full_text"]
                print(tweet)
            except AttributeError:
                tweet = status.text  
                print(tweet)
        with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/fetched_tweets.txt','a+') as tf:
                tf.write(tweet+'\n')
212/124:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
212/125:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/126:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

#     def on_data(self, data):
#         try:
#             with open(self.outfile, 'a') as f:
#                 f.write(data)
#                 #print(data)
#                 return True
#         except BaseException as e:
#             #print("Error on_data: %s" % str(e))
#             time.sleep(5)
#         return True

    def on_error(self, status):
        print(status)
        return True
    def on_status(self, status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet = status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
            except:
                tweet = status.retweeted_status.text
                print(tweet)

        else:
            try:
                tweet = status.extended_tweet["full_text"]
                print(tweet)
            except AttributeError:
                tweet = status.text  
                print(tweet)
        try:
            with open(self.outfile, 'a') as f:
                f.write(tweet)
                print(tweet)
                return True
        except BaseException as e:
            print("Error on_status: %s" % str(e))
            time.sleep(5)
        return True
212/127:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
212/128:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/1:
import gzip
import time
import os
from tweepy import StreamListener, Stream, OAuthHandler
from virtualornithology.analysis.importer import Importer


class MyStreamListener(tweepy.StreamListener):

    def on_status(self, status):
        if from_creator(status):
            try:
                # Prints out the tweet
                print(status.text)
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.text)
                return True
            except BaseException as e:
                print("Error on_data %s" % str(e))
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/2:
import gzip
import time
import os
from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(tweepy.StreamListener):

    def on_status(self, status):
        if from_creator(status):
            try:
                # Prints out the tweet
                print(status.text)
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.text)
                return True
            except BaseException as e:
                print("Error on_data %s" % str(e))
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/3:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(tweepy.StreamListener):

    def on_status(self, status):
        if from_creator(status):
            try:
                # Prints out the tweet
                print(status.text)
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.text)
                return True
            except BaseException as e:
                print("Error on_data %s" % str(e))
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/4:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):

    def on_status(self, status):
        if from_creator(status):
            try:
                # Prints out the tweet
                print(status.text)
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.text)
                return True
            except BaseException as e:
                print("Error on_data %s" % str(e))
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/5:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/6:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/7:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/8:
def from_creator(status):
    if hasattr(status, 'retweeted_status'):
        return False
    elif status.in_reply_to_status_id != None:
        return False
    elif status.in_reply_to_screen_name != None:
        return False
    elif status.in_reply_to_user_id != None:
        return False
    else:
        return True
216/9:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/10:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):

    def on_status(self, status):
        if from_creator(status):
            try:
                # Prints out the tweet
                print(status.extended_tweet["full_text"])
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.extended_tweet["full_text"])
                return True
            except BaseException as e:
                print("Error on_data %s" % str(e))
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/11:
def from_creator(status):
    if hasattr(status, 'retweeted_status'):
        return False
    elif status.in_reply_to_status_id != None:
        return False
    elif status.in_reply_to_screen_name != None:
        return False
    elif status.in_reply_to_user_id != None:
        return False
    else:
        return True
216/12:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/13:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/14:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/15:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):

    def on_status(self, status):
        if from_creator(status):
            try:
                # Prints out the tweet
                print(status.full_text)
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.full_text)
                return True
            except BaseException as e:
                print("Error on_data %s" % str(e))
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/16:
def from_creator(status):
    if hasattr(status, 'retweeted_status'):
        return False
    elif status.in_reply_to_status_id != None:
        return False
    elif status.in_reply_to_screen_name != None:
        return False
    elif status.in_reply_to_user_id != None:
        return False
    else:
        return True
216/17:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/18:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/19:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/20:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):

    def on_status(self, status):
        if from_creator(status):
            try:
                # Prints out the tweet
                print(status.extended_tweet["full_text"])
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.extended_tweet["full_text"])
                return True
            except BaseException as e:
                print("Error on_data %s" % str(e))
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/21:
def from_creator(status):
    if hasattr(status, 'retweeted_status'):
        return False
    elif status.in_reply_to_status_id != None:
        return False
    elif status.in_reply_to_screen_name != None:
        return False
    elif status.in_reply_to_user_id != None:
        return False
    else:
        return True
216/22:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/23:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/24:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/25:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):

    def on_status(self, status):
        if from_creator(status):
            try:
                # Prints out the tweet
                print(status.extended_tweet["full_text"])
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.extended_tweet["full_text"])
                return True
            except BaseException as e:
                print("Error on_data %s" % str(e))
                with open("Test.txt", 'a') as file:
                    file.write(status.text)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/26:
def from_creator(status):
    if hasattr(status, 'retweeted_status'):
        return False
    elif status.in_reply_to_status_id != None:
        return False
    elif status.in_reply_to_screen_name != None:
        return False
    elif status.in_reply_to_user_id != None:
        return False
    else:
        return True
216/27:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/28:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/29:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/30:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):

    def on_status(self, status):
        if from_creator(status):
            try:
                # Prints out the tweet
                print(status.extended_tweet["full_text"])
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.extended_tweet["full_text"])
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                print(status.text)
                with open("Test.txt", 'a') as file:
                    file.write(status.text)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/31:
def from_creator(status):
    if hasattr(status, 'retweeted_status'):
        return False
    elif status.in_reply_to_status_id != None:
        return False
    elif status.in_reply_to_screen_name != None:
        return False
    elif status.in_reply_to_user_id != None:
        return False
    else:
        return True
216/32:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/33:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/34:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/35:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):

    def on_status(self, status):
       # if from_creator(status):
        if hasattr(status, 'retweeted_status'):
            try:
                print(status.retweeted_status.extended_tweet["full_text"])
                with open("Test.txt", 'a') as file:
                    file.write(status.retweeted_status.extended_tweet["full_text"])
            except:
                print(status.retweeted_status.text)
                with open("Test.txt", 'a') as file:
                    file.write(status.retweeted_status.text)
        else:
        #-------------------
            try:
                # Prints out the tweet
                print(status.extended_tweet["full_text"])
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(status.extended_tweet["full_text"])
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                print(status.text)
                with open("Test.txt", 'a') as file:
                    file.write(status.text)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/36:
def from_creator(status):
    if hasattr(status, 'retweeted_status'):
        return False
    elif status.in_reply_to_status_id != None:
        return False
    elif status.in_reply_to_screen_name != None:
        return False
    elif status.in_reply_to_user_id != None:
        return False
    else:
        return True
216/37:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/38:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/39:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
215/1: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
215/2: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
215/3: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
217/1: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
216/40:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/129:
x={}
x["text"]="dddd"
212/130:
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/output.json') as f:
    # this would place the entire output on one line
    # use json.dump(lista_items, f, indent=4) to "pretty-print" with four spaces per indent
    json.dump(x, f)
212/131:
x={}
x["text"]="dddd"
x
212/132:
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/output.json', 'w+'') as f:
    # this would place the entire output on one line
    # use json.dump(lista_items, f, indent=4) to "pretty-print" with four spaces per indent
    json.dump(x, f)
212/133:
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/output.json', 'w+'') as f:
    # this would place the entire output on one line
    # use json.dump(lista_items, f, indent=4) to "pretty-print" with four spaces per indent
       json.dump(x, f)
212/134:
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/output.json', 'w+') as f:
    # this would place the entire output on one line
    # use json.dump(lista_items, f, indent=4) to "pretty-print" with four spaces per indent
       json.dump(x, f)
212/135:
x["text"]="dddjdhkjhk"
x
212/136:
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/output.json', 'w+') as f:
    # this would place the entire output on one line
    # use json.dump(lista_items, f, indent=4) to "pretty-print" with four spaces per indent
       json.dump(x, f)
212/137:
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/output.json', 'a') as f:
    # this would place the entire output on one line
    # use json.dump(lista_items, f, indent=4) to "pretty-print" with four spaces per indent
       json.dump(x, f)
212/138:
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/output.json', "r") as file:  
    xx = json.load(file)
212/139:
x={}
x["text"]="dddd"

x
212/140:
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/output.json', 'a') as f:
    # this would place the entire output on one line
    # use json.dump(lista_items, f, indent=4) to "pretty-print" with four spaces per indent
       json.dump(x, f)
212/141:
with open('/Users/shahla/Dropbox/MLCourse/SharpestMinds/output.json', "r") as file:  
    xx = json.load(file)
216/41: df = pd.DataFrame(columns=['tweet'])
216/42:
import pandas as pd
df = pd.DataFrame(columns=['tweet'])
216/43:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):

    def on_status(self, status):
       # if from_creator(status):
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                print(tweet)
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/44:
# def from_creator(status):
#     if hasattr(status, 'retweeted_status'):
#         return False
#     elif status.in_reply_to_status_id != None:
#         return False
#     elif status.in_reply_to_screen_name != None:
#         return False
#     elif status.in_reply_to_user_id != None:
#         return False
#     else:
#         return True
216/45:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/46:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/47:
import pandas as pd
df = pd.DataFrame(columns=['tweet'])
216/48:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
212/142:
tweet=dddkdk
df = pd.DataFrame(columns=['tweet'])
df.loc[len(df2)]=['xxx jnjcn kjfkj'] 
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=[tweet] 
df= df.append(df2,ignore_index=True)
df
212/143:
tweet='dddkdk'
df = pd.DataFrame(columns=['tweet'])
df.loc[len(df2)]=['xxx jnjcn kjfkj'] 
df2=pd.DataFrame(columns=['tweet'])
df2.loc[len(df2)]=[tweet] 
df= df.append(df2,ignore_index=True)
df
216/49:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):

    def on_status(self, status):
       # if from_creator(status):
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/50:
# def from_creator(status):
#     if hasattr(status, 'retweeted_status'):
#         return False
#     elif status.in_reply_to_status_id != None:
#         return False
#     elif status.in_reply_to_screen_name != None:
#         return False
#     elif status.in_reply_to_user_id != None:
#         return False
#     else:
#         return True
216/51:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/52:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/53:
import pandas as pd
df = pd.DataFrame(columns=['tweet'])
216/54:

df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
216/55:
import pandas as pd
df = pd.DataFrame(columns=['tweet'])
df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
216/56:

df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
216/57:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/58:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    global df

    def on_status(self, status):
       # if from_creator(status):
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/59:
# def from_creator(status):
#     if hasattr(status, 'retweeted_status'):
#         return False
#     elif status.in_reply_to_status_id != None:
#         return False
#     elif status.in_reply_to_screen_name != None:
#         return False
#     elif status.in_reply_to_user_id != None:
#         return False
#     else:
#         return True
216/60:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/61:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
216/62:
import pandas as pd
df = pd.DataFrame(columns=['tweet'])
df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
216/63:

df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
216/64:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/65:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    df = pd.DataFrame(columns=['tweet'])
    df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')

    def on_status(self, status):
       # if from_creator(status):
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/66:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/67:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
       # if from_creator(status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
216/68:
# def from_creator(status):
#     if hasattr(status, 'retweeted_status'):
#         return False
#     elif status.in_reply_to_status_id != None:
#         return False
#     elif status.in_reply_to_screen_name != None:
#         return False
#     elif status.in_reply_to_user_id != None:
#         return False
#     else:
#         return True
216/69:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
216/70:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/71:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df
216/72:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet[3]
216/73:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet[20]
216/74:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet[27]
216/75:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
216/76:
import pandas as pd
df = pd.DataFrame(columns=['tweet'])
df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
216/77:

df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
216/78:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],,languages='en')
216/79:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],languages='en')
216/80:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],languages='en')
219/1:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
       # if from_creator(status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
219/2:
# def from_creator(status):
#     if hasattr(status, 'retweeted_status'):
#         return False
#     elif status.in_reply_to_status_id != None:
#         return False
#     elif status.in_reply_to_screen_name != None:
#         return False
#     elif status.in_reply_to_user_id != None:
#         return False
#     else:
#         return True
219/3:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
219/4:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
219/5:
import pandas as pd
df = pd.DataFrame(columns=['tweet'])
df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
219/6:

df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
219/7:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],languages='en')
219/8:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
       # if from_creator(status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
219/9:
# def from_creator(status):
#     if hasattr(status, 'retweeted_status'):
#         return False
#     elif status.in_reply_to_status_id != None:
#         return False
#     elif status.in_reply_to_screen_name != None:
#         return False
#     elif status.in_reply_to_user_id != None:
#         return False
#     else:
#         return True
219/10:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
219/11:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
219/12:
import pandas as pd
df = pd.DataFrame(columns=['tweet'])
df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
219/13:

df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
219/14:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],languages='en')
219/15:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],languages='en')
220/1:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
       # if from_creator(status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                print(tweet)
                df2.loc[len(df2)]=[tweet] 
                df= df.append(df2,ignore_index=True)
                df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
220/2:
# def from_creator(status):
#     if hasattr(status, 'retweeted_status'):
#         return False
#     elif status.in_reply_to_status_id != None:
#         return False
#     elif status.in_reply_to_screen_name != None:
#         return False
#     elif status.in_reply_to_user_id != None:
#         return False
#     else:
#         return True
220/3:
#https://stackabuse.com/accessing-the-twitter-api-with-python/
# to save my twitter_credentials.json
import json
import csv
from datetime import datetime
import os
import re
import requests
import twitter
import lxml.html
## Enter your keys/secrets as strings in the following fields
credentials = {}  
credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'
credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' 
credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'
credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'

# Save the credentials object to file
with open("twitter_credentials.json", "w") as file:  
   json.dump(credentials, file)
220/4:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
220/5:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],languages='en')
220/6:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
       # if from_creator(status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                #df2.loc[len(df2)]=[tweet] 
               # df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                #tweet=status.text
                print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
220/7:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],languages='en')
221/1: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download2.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
221/2: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download2.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
221/3: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/twitter_stream_download2.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream')
220/8:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
220/9: import pandas as pd
220/10:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
220/11:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
       # if from_creator(status):
        #df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        #df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                print(tweet)
                
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                print(tweet)
                #df2.loc[len(df2)]=[tweet] 
               # df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
            return True
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
220/12:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
220/13:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                #print(tweet)
                
                #with open("Test.txt", 'a') as file:
                 #   file.write(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                #print(tweet)
                #df2.loc[len(df2)]=[tweet] 
               # df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                #print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
#                 with open("Test.txt", 'a') as file:
#                     file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                #print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
#               with open("Test.txt", 'a') as file:
#                     file.write(tweet)
            return True
        print(tweet)
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
220/14:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
220/15:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                #print(tweet)
                
                #with open("Test.txt", 'a') as file:
                 #   file.write(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                #print(tweet)
                #df2.loc[len(df2)]=[tweet] 
               # df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                #print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
#                 with open("Test.txt", 'a') as file:
#                     file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                #print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
#               with open("Test.txt", 'a') as file:
#                     file.write(tweet)
            return True
        print(tweet)
        df2.loc[len(df2)]=[tweet] 
        df= df.append(df2,ignore_index=True)
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
220/16:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
220/17:

df = pd.DataFrame(columns=['tweet'])
df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
220/18:

df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
220/19:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
220/20:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet[27]
220/21:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet[0]
220/22:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet
220/23:

df = pd.DataFrame(columns=['tweet'])
df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
220/24:

df = pd.DataFrame(columns=['tweet'])
df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
220/25:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
                #print(tweet)
                
                #with open("Test.txt", 'a') as file:
                 #   file.write(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                #json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)
            except:
                tweet=status.retweeted_status.text
                #print(tweet)
                #df2.loc[len(df2)]=[tweet] 
               # df= df.append(df2,ignore_index=True)
               # df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                with open("Test.txt", 'a') as file:
                    file.write(tweet)
        else:
        #-------------------
            try:
                # Prints out the tweet
                tweet=status.extended_tweet["full_text"]
                #print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
                # Saves tweet into a file
#                 with open("Test.txt", 'a') as file:
#                     file.write(tweet)
                return True
            except BaseException as e:
                #print("Error on_data %s" % str(e))
                tweet=status.text
                #print(tweet)
                #df2.loc[len(df2)]=[tweet] 
                #df= df.append(df2,ignore_index=True)
                #df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
#               with open("Test.txt", 'a') as file:
#                     file.write(tweet)
            return True
        print(tweet)
        df2.loc[len(df2)]=[tweet] 
        df= df.append(df2,ignore_index=True)
        df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
220/26:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
220/27:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet
220/28:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet[0]
220/29:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet[2]
220/30:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.tweet[7]
220/31:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.shape()
220/32:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.shape
220/33: df.tweet[0]
220/34:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
220/35:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.shape
220/36: df.tweet[0]
220/37:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
222/1:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("../stream_politics.json", "r") as file:  
    data = json.load(file)
222/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json", "r") as file:  
    data = json.load(file)
222/3:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
222/4: df.shape
220/38:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.shape
220/39: df.tweet[0]
220/40:
import gzip
import time
import os
from tweepy.streaming import StreamListener
#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
            except:
                tweet=status.retweeted_status.text
        else:
        #-------------------
            try:
                tweet=status.extended_tweet["full_text"]
                return True
            except BaseException as e:
                tweet=status.text
            return True
        print(tweet)
        df2.loc[len(df2)]=[tweet] 
        df= df.append(df2,ignore_index=True)
        df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
220/41:
# def from_creator(status):
#     if hasattr(status, 'retweeted_status'):
#         return False
#     elif status.in_reply_to_status_id != None:
#         return False
#     elif status.in_reply_to_screen_name != None:
#         return False
#     elif status.in_reply_to_user_id != None:
#         return False
#     else:
#         return True
220/42:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
220/43:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.shape
222/5: data.retweeted_status[0:10]
222/6:
#keep English languages
data=df[df.lang=='en']
222/7: data.retweeted_status[0:10]
222/8: data.retweeted_status.extended_tweet["full_text"]
222/9:
if hasattr(data, 'retweeted_status'):
            try:
                tweet= data.retweeted_status.extended_tweet["full_text"]
            except:
                tweet=data.retweeted_status.text
        else:
        #-------------------
            try:
                tweet=data.extended_tweet["full_text"]
                return True
            except BaseException as e:
                tweet=data.text
            return True
222/11:
if hasattr(data, 'retweeted_status'):
            try:
                tweet= data.retweeted_status.extended_tweet["full_text"]
            except:
                tweet=data.retweeted_status.text
else:
        #-------------------
            try:
                tweet=data.extended_tweet["full_text"]
                return True
            except BaseException as e:
                tweet=data.text
            return True
222/12:
if hasattr(data, 'retweeted_status'):
            try:
                tweet= data.retweeted_status.extended_tweet["full_text"]
            except:
                tweet=data.retweeted_status.text
else:
        #-------------------
            try:
                tweet=data.extended_tweet["full_text"]
                return True
            except BaseException as e:
                tweet=data.text
222/13:
if hasattr(data, 'retweeted_status'):
            try:
                tweet= data.retweeted_status.extended_tweet["full_text"]
            except:
                tweet=data.retweeted_status.text
else:
        #-------------------
            try:
                tweet=data.extended_tweet["full_text"]
               
            except BaseException as e:
                tweet=data.text
222/14:
#keep English languages
data=df[df.lang=='en']
data
222/15:
#keep English languages
data=df[df.lang=='en']
data.text[0:10]
222/16:
#keep English languages
data=df[df.lang=='en']
data.text[0]
222/17:
#keep English languages
data=df[df.lang=='en']
data.text[2]
222/18:
#keep English languages
data=df[df.lang=='en']
data.text[3]
222/19:
#keep English languages
data=df[df.lang=='en']
data.text[9]
222/20:
#keep English languages
data=df[df.lang=='en']
data.text[1]
222/21:
#keep English languages
data=df[df.lang=='en']
data.text[20]
222/22: data.retweeted_status[0]
222/23: data.retweeted_status[0].text
222/24: data.retweeted_status[0][text]
222/25: data.retweeted_status[0]['text']
222/26: data.text[0]
222/27: data.retweeted_status[20]['text']
222/28:
#keep English languages
data=df[df.lang=='en']
data.text[21]
222/29: data.retweeted_status[21]['text']
222/30: data.retweeted_status[21]
222/31: data.retweeted_status[21]['extended_tweet']
222/32: data.retweeted_status[21]['extended_tweet']['full_text']
222/33: data.retweeted_status[20]['extended_tweet']['full_text']
222/34: data.retweeted_status[30]['extended_tweet']['full_text']
222/35: data.retweeted_status[20]['extended_tweet']['full_text']
222/36: data.retweeted_status[21]['extended_tweet']['full_text']
222/37: data.shape
222/38: data.retweeted_status[743]
222/39: data.retweeted_status[21]
222/40: data.retweeted_status[20]['extended_tweet']['full_text']
222/41: data.retweeted_status[30]['extended_tweet']['full_text']
222/42: data.retweeted_status[30]
222/43: data.retweeted_status[21]
222/44: data.retweeted_status[31]
222/45: data.retweeted_status[21]['extended_tweet']['full_text']
222/46:
if hasattr(data, 'retweeted_status'):
            try:
                tweet= data.retweeted_status['extended_tweet']["full_text"]
            except:
                tweet=data['retweeted_status']['text']
else:
        #-------------------
            try:
                tweet=data['extended_tweet']["full_text"]
               
            except BaseException as e:
                tweet=data.text
222/47: data.retweeted_status[0]
222/48: data.retweeted_status[31]
222/49: data.retweeted_status[21]
222/50: data.extended_tweet[31]
222/51:
if hasattr(data, 'retweeted_status'):
            try:
                tweet= data.retweeted_status['extended_tweet']["full_text"]
            except:
                tweet=data.retweeted_status['text']
else:
        #-------------------
            try:
                tweet=data.extended_tweet["full_text"]
               
            except BaseException as e:
                tweet=data.text
222/52: data.retweeted_status[21]['text']
222/53: data.retweeted_status['text']
222/54: data.retweeted_status[31]['text']
222/55: data.retweeted_status[21]['text']
222/56: data.extended_tweet[31]['full_text']
222/57: data.text[31]
222/58: data.shape[0]
222/59: data.shape
220/44:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
222/60: hasattr(data, 'retweeted_status'):
222/61: hasattr(data, 'retweeted_status')
222/62: hasattr(data.retweeted_status,'extended_tweet')
222/63: data.retweeted_status['extended_tweet'].isnull()
222/64:
if 'extended_tweet' in data.retweeted_status.keys():
 print('ff')
222/65:
if 'extended_tweet' in data.retweeted_status.keys():
     print('ff')
222/66:
if 'extended_tweet' in data.retweeted_status.keys():
     print('ff')
222/67:
if 'extended_tweet' in data.retweeted_status.keys():
     print('ff')
222/68: data.extended_tweet[31]['full_text']
222/69:
if 'extended_tweet' in data.retweeted_status.keys():
     print('ff')
222/70:
if 'extended_tweet' in data.retweeted_status.keys():
     print(data.retweeted_status.keys())
222/71:
if 'extended_tweet' in data.retweeted_status.keys():
     print('ff')
else:
      print(data.retweeted_status.keys())
222/72:
for i in 1:data.shape[0]:
        if 'extended_tweet' in data.retweeted_status[].keys():
            print('ff')
        else:
            print(data.retweeted_status.keys())
222/73:
for i in range(data.shape[0]):
        if 'extended_tweet' in data.retweeted_status[].keys():
            print('ff')
        else:
            print(data.retweeted_status.keys())
222/74:
for i in range(data.shape[0]):
        if 'extended_tweet' in data.retweeted_status[i].keys():
            print('ff')
        else:
            print(data.retweeted_status.keys())
222/75:
for i in range(data.shape[0]):
        if 'extended_tweet' in data.retweeted_status[i].keys():
            print('ff')
        else:
            print(data.retweeted_status[i].keys())
222/76:
for i in range(data.shape[0]):   
    if hasattr(data, 'retweeted_status'):
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
#                tweet=data.retweeted_status['text']
            tweet=data.extended_tweet[i]["full_text"]
    print(tweet)
222/77:
for i in range(data.shape[0]):   
    if hasattr(data, 'retweeted_status'):
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
#               tweet=data.retweeted_status['text']
                tweet=data.extended_tweet[i]["full_text"]
    print(tweet)
222/78:
i=0   
if hasattr(data, 'retweeted_status'):
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
                tweet=data.extended_tweet[i]["full_text"]
print(tweet)
222/79: tweet= data.retweeted_status
222/80: data.retweeted_status
222/81: data.retweeted_status[0]
222/82: tweet= data.retweeted_status[i]['extended_tweet']
222/83: data.retweeted_status[i]
220/45:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],lang='en')
220/46:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],lan='en')
220/47:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query],languages='en')
220/48:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.shape
220/49:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
222/84: data.retweeted_status[i].keys()
222/85: 'extended_tweet' in data.retweeted_status[i].keys()
222/86: 'extended_tweet' in data.extended_tweet[i].keys()
222/87: data.extended_tweet[i]==NA
222/88: data.extended_tweet[i]
222/89: data.extended_tweet[i]==nan
222/90: data.extended_tweet[i]
222/91: data.dropnan()
222/92: data.dropna()
222/93: data.shape
222/94: data.extended_tweet[i]
222/95: data.extended_tweet
222/96: data.dropna(axis=1, how='any') #remve all column where any value is 'NaN' exists
222/97: data.shape
222/98: data.extended_tweet
222/99: data.extended_tweet
222/100: df[~df.isin(['NaN']).any(axis=1)]
222/101:
data.replace(["NaN"], np.nan, inplace = True)
data.dropna()
222/102:
import numpy as np
data.replace(["NaN"], np.nan, inplace = True)
data.dropna()
222/103: data.extended_tweet
222/104:
import numpy as np
data.replace(["NaN"], np.nan, inplace = True)
data.dropna()
222/105: data.extended_tweet
222/106:
pd.isna(data.extended_tweet)
#data.extended_tweet
222/107:
pd.isna(data.extended_tweet)
data.extended_tweet.dropna()
222/108:
pd.isna(data.extended_tweet)
data.extended_tweet.dropna()
data.extended_tweet
222/109:
pd.isna(data.extended_tweet)
data.extended_tweet.dropna()
data.dropna()
222/110:
pd.isna(data.extended_tweet)
data.extended_tweet.dropna()
data=data.dropna()
222/111: data.extended_tweet
222/112: data.extended_tweet[0]
222/113: data.columns
220/50:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.shape
222/114: data.extended_tweet
222/115: data.extended_tweet[0]
222/116: data.text[0]
222/117: data.text[1]
222/118: data.text
222/119: data
222/120:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
222/121: df.shape
222/122:
list_columns=list(df.columns)
list_columns
222/123:
#keep English languages
data=df[df.lang=='en']
222/124: data.shape
222/125:

data.text[21]
222/126: sum(data.text==nan)
222/127: sum(data.text=='nan')
222/128: data.text.isna.sum()
222/129: data.text.isna().sum
222/130: data.isna().sum
222/131: x=data.isna().sum
222/132:
x=data.isna().sum
x
222/133: data
222/134: data.text[0]
222/135: data.text[1]
222/136: data.text[2]
222/137: data.truncated[2]
222/138: data.truncated[31]
222/139:
data.truncated[31]
data.extended_tweet[31]
222/140: data.text[31]
222/141: data.truncated[21]
222/142: data.retweeted_status.isna().sum
222/143: data.retweeted_status.dropna()
222/144: data.retweeted_status
222/145:
data.retweeted_status.dropna()
data.retweeted_status[31]
222/146:
data.retweeted_status.dropna()
data.retweeted_status[1:31]
222/147: data.retweeted_status[31]==nan
222/148: data.retweeted_status[31]=='nan'
222/149: pd.isnull(data.retweeted_status[31])
222/150:
for i in range(data.shape[0]):   
     if ~pd.isnull(data.retweeted_status[i])
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
                tweet=data.retweeted_status['text']
    else:
            try:
                tweet=data.extended_tweet[i]["full_text"]
            except:
                 tweet=data.text  
    print(tweet)
222/152:
for i in range(data.shape[0]):   
     if ~pd.isnull(data.retweeted_status[i]):
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
                tweet=data.retweeted_status['text']
        else:
            try:
                tweet=data.extended_tweet[i]["full_text"]
            except:
                 tweet=data.text  
    print(tweet)
222/154:
for i in range(data.shape[0]):   
       if ~pd.isnull(data.retweeted_status[i]):
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
                tweet=data.retweeted_status['text']
        else:
            try:
                tweet=data.extended_tweet[i]["full_text"]
            except:
                 tweet=data.text  
        print(tweet)
222/156:
for i in range(data.shape[0]):   
       if ~pd.isnull(data.retweeted_status[i]):
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
                tweet=data.retweeted_status['text']
222/157:
for i in range(data.shape[0]):   
       if ~pd.isnull(data.retweeted_status[i]):
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
                tweet=data.retweeted_status['text']
       else:
            try:
                tweet=data.extended_tweet[i]["full_text"]
            except:
                 tweet=data.text  
       print(tweet)
222/158:
i=0   

if ~pd.isnull(data.retweeted_status[i]):
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
                tweet=data.retweeted_status['text']
else:
            try:
                tweet=data.extended_tweet[i]["full_text"]
            except:
                 tweet=data.text  
print(tweet)
222/159:
i=0   

if ~pd.isnull(data.retweeted_status[i]):
            try:
                tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            except:
                tweet=data.retweeted_status['text']
else:
            try:
                tweet=data.extended_tweet[i]["full_text"]
            except:
                 tweet=data.text[i]
print(tweet)
222/160: tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
222/161: tweet= data.retweeted_status[i]
222/162: data.extended_tweet[i].keys()
222/163: data.extended_tweet[i]
222/164: ~pd.isnull(data.retweeted_status[i])
222/165: pd.isnull(data.retweeted_status[i])
222/166: data.retweeted_status[i]
222/167: tweet= data.retweeted_status[i]
222/168: data.retweeted_status[i]['extended_tweet']
222/169: data.retweeted_status[i].keys()
222/170: data.retweeted_status[1].keys()
222/171: data.retweeted_status[i]['text']
222/172: data.retweeted_status[31]['text']
222/173: data.retweeted_status[21]['text']
222/174: data.retweeted_status[21].keys()
222/175: data.retweeted_status[0].keys()
222/176: data.text[0]
222/177: data.retweeted_status[1].keys()
222/178: data.retweeted_status[2].keys()
222/179: data.retweeted_status[2]
222/180: data.retweeted_status[3]
222/181: data.retweeted_status[3].keys()
222/182: data.retweeted_status[3]['extended_tweet']
222/183: data.retweeted_status[0][text]
222/184: data.retweeted_status[0]['text']
222/185:
for i in range(data.shape[0]):   
       if pd.isnull(data.retweeted_status[i]):
            if "full_text" in data.extended_tweet[i].keys():
                tweet=data.extended_tweet[i]["full_text"]
            else:
                 tweet=data.text  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
222/186:
for i in range(data.shape[0]):   
       if pd.isnull(data.retweeted_status[i]):
            if "full_text" in data.extended_tweet[i].keys():
                tweet=data.extended_tweet[i]["full_text"]
            else:
                 tweet=data.text  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/187: data.retweeted_status[5]
222/188: data.retweeted_status[5].keys()
222/189: data.retweeted_status[5]['text']
222/190: data.retweeted_status[4]['text']
222/191: data.retweeted_status[4].keys()
222/192: data.retweeted_status[4]
222/193: data.retweeted_status
222/194:
#keep English languages
data=df[df.lang=='en']
222/195: data.retweeted_status
222/196: data.index
222/197:
for i in data.index:   
       if pd.isnull(data.retweeted_status[i]):
            if "full_text" in data.extended_tweet[i].keys():
                tweet=data.extended_tweet[i]["full_text"]
            else:
                 tweet=data.text  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/198:
for i in data.index:  
    print(data.text)
222/199: data.retweeted_status[9]
222/200: data.extended_tweet[9]
222/201:
for i in data.index:   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/202:
for i in data.index & range(1:20):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/203:
for i in data.index && range(1:20):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/204:
for i in data.index && range(20):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/205:
for i in data.index and range(20):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/206:
for i in (data.index and range(20)):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/207:
for i in (range(20)):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/208:
for i in (data.index and range(20)):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/209:
for i in (data.index & range(20)):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/210:
for i in (data.index & range(10)):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/211: data.extended_tweet[9]
222/212: data.text[9]
222/213:
for i in (data.index & range(10)):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data[i].text 
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
222/214: data.text[9]
222/215:
for i in (data.index & range(10)):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweet=data.text[i]
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                    tweet=data.extended_tweet[i]["full_text"]
                else:
                    tweet=data.text 
                  
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweet= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                tweet=data.retweeted_status[i]['text']
       print(tweet)
       print('----------------\n')
223/1:
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    credentials = json.load(file)
223/2:
import pandas as pd
import json
from tweepy.streaming import StreamListener
223/3:


# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    credentials = json.load(file)
223/4:

#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
            except:
                tweet=status.retweeted_status.text
        else:
        #-------------------
            try:
                tweet=status.extended_tweet["full_text"]
                return True
            except BaseException as e:
                tweet=status.text
            return True
        print(tweet)
        df2.loc[len(df2)]=[tweet] 
        df= df.append(df2,ignore_index=True)
        df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
223/5:
import pandas as pd
import json
from tweepy.streaming import StreamListener
import twitter
223/6:
import pandas as pd
import json
from tweepy.streaming import StreamListener
import twitter
import tweepy
223/7:

auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
api = tweepy.API(auth)
223/8:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
223/9:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.shape
223/10: df.tweet[0]
224/1:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
224/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json", "r") as file:  
    data = json.load(file)
224/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
224/4:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
224/5: from tweepy import Stream
224/6:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
224/7:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
224/8:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
224/9:
# query = 'politics'
# data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
# twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
# twitter_stream.filter(track=[query])
224/10:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
224/11: df.shape
224/12:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en #install it in the terminal

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
224/13:
wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I)
    doc = doc.lower()
    doc = doc.strip()
    # tokenize document
    tokens = wpt.tokenize(doc)
    # filter stopwords out of document
    filtered_tokens = [token for token in tokens if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(normalize_document)
224/14:
corpus = ['The sky is blue and beautiful.',
          'Love this blue and beautiful sky!',
          'The quick brown fox jumps over the lazy dog.',
          'The brown fox is quick and the blue dog is lazy!',
          'The sky is very blue and the sky is very beautiful today',
          'The dog is lazy but the brown fox is quick!'    
]
norm_corpus = normalize_corpus(corpus)
norm_corpus
224/15: df.text[2]
224/16: normalize_corpus(df.text)
224/17:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
224/18:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:100]
224/19: x[9]
224/20: df.text[2]
224/21: df.text[9]
224/22:
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)

filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
    dictionary = dict(zip(post_titles, filtered_word_list))
    vectorized_titles = pd.DataFrame(columns=["Titles", "Vectors"])
    for title in post_titles: 
        word_vecs = [model[word] for word in dictionary[title]]
        if len(word_vecs) == 0:
            title_vec = [np.zeros(300)]
        else: 
            title_vec = normalize(sum(word_vecs).reshape(1, -1))
        vectorized_titles = vectorized_titles.append({'Titles': title, 'Vectors': title_vec}, ignore_index=True)
    vectorized_titles.to_pickle("/Users/angelateng/Dropbox/AMBER/SharpestMinds/vectorized_titles.pkl")
224/23:
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)

filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
dictionary = dict(zip(post_titles, filtered_word_list))
vectorized_titles = pd.DataFrame(columns=["Titles", "Vectors"])
for title in post_titles: 
    word_vecs = [model[word] for word in dictionary[title]]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = normalize(sum(word_vecs).reshape(1, -1))
    vectorized_titles = vectorized_titles.append({'Titles': title, 'Vectors': title_vec}, ignore_index=True)
vectorized_titles.to_pickle("/Users/angelateng/Dropbox/AMBER/SharpestMinds/vectorized_titles.pkl")
224/24: import gensim
224/25:
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)

filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
dictionary = dict(zip(post_titles, filtered_word_list))
vectorized_titles = pd.DataFrame(columns=["Titles", "Vectors"])
for title in post_titles: 
    word_vecs = [model[word] for word in dictionary[title]]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = normalize(sum(word_vecs).reshape(1, -1))
    vectorized_titles = vectorized_titles.append({'Titles': title, 'Vectors': title_vec}, ignore_index=True)
vectorized_titles.to_pickle("/Users/angelateng/Dropbox/AMBER/SharpestMinds/vectorized_titles.pkl")
224/26: x
224/27: type(x)
224/28: x[1]
224/29: x[2]
224/30: x[3]
224/31: x[4]
224/32: x[10]
224/33: tweets = [x[1],x[10]]
224/34:

filtered_word_list = [[word for word in tweet if word in model.vocab] for tweet in tweets];
224/35: filtered_word_list
224/36: tweets
224/37: x[1] in tweets
224/38: x[1].split()
224/39: tweets=[x[1].split(),x[10].split()]
224/40:

filtered_word_list = [[word for word in tweet if word in model.vocab] for tweet in tweets];
224/41: filtered_word_list
224/42: model.vocab
224/43: model.vocab['rt']
225/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
225/2:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
225/3:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
from load_filter import load_filter
from sklearn.externals import joblib
226/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
from load_filter import load_filter
from sklearn.externals import joblib
226/2:

data = load_filter('testv.yaml')
fitted=[]
predicted=[]
models = [LogisticRegression(),GaussianNB()]
x=data.filter_data_x(True)
y=data.filter_data_y()
for j in range(len(models)):
    for i in range (0,len(x)):
        fitted.append(models[j].fit(X=x[i],y=y))
joblib.dump(fitted, 'fitted.pkl')
predict_model=joblib.load('fitted.pkl')
for i in range(len(predict_model)):
    for j in range (0,len(x)):
        predicted.append(predict_model[i].predict(x[j]))
226/3:

data = load_filter('testv.yaml')
fitted=[]
predicted=[]
models = [LogisticRegression(),GaussianNB()]
226/4:

data = load_filter('testv.yaml')
fitted=[]
predicted=[]
models = [LogisticRegression(),GaussianNB()]
226/5:
x=data.filter_data_x(True)
y=data.filter_data_y()
for j in range(len(models)):
    for i in range (0,len(x)):
        fitted.append(models[j].fit(X=x[i],y=y))
joblib.dump(fitted, 'fitted.pkl')
predict_model=joblib.load('fitted.pkl')
for i in range(len(predict_model)):
    for j in range (0,len(x)):
        predicted.append(predict_model[i].predict(x[j]))
226/6:
x=data.filter_data_x(True)
y=data.filter_data_y()
226/7:

data = load_filter('testv.yaml')
fitted=[]
predicted=[]
models = [LogisticRegression(),GaussianNB()]
226/8:
x=data.filter_data_x(True)
y=data.filter_data_y()
226/9:

data = load_filter('testv.yaml')
fitted=[]
predicted=[]
models = [LogisticRegression(),GaussianNB()]
226/10:
x=data.filter_data_x(True)
y=data.filter_data_y()
226/11:
x=data.filter_data_x(True)
y=data.filter_data_y()
226/12:

for j in range(len(models)):
    for i in range (0,len(x)):
        fitted.append(models[j].fit(X=x[i],y=y))
joblib.dump(fitted, 'fitted.pkl')
predict_model=joblib.load('fitted.pkl')
for i in range(len(predict_model)):
    for j in range (0,len(x)):
        predicted.append(predict_model[i].predict(x[j]))
226/13:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
from load_filter import load_filter
from sklearn.externals import joblib
226/14:

data = load_filter('testv.yaml')
fitted=[]
predicted=[]
models = [LogisticRegression(),GaussianNB()]
226/15:
x=data.filter_data_x(True)
y=data.filter_data_y()
226/16:
r=load_filter('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/testv.yaml')  
data = load_filter('testv.yaml')
fitted=[]
predicted=[]
models = [LogisticRegression(),GaussianNB()]
226/17:
x=r.filter_data_x(True)
y=r.filter_data_y()
227/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/pipe.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
230/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/pipe.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
230/2:
import argparse
import yaml
from sklearn.externals import joblib 
import model_storage



from load_filter import load_filter
from str_transform import str_to_bool,str_to_function
#from parameter_var import parameter_var
parser = argparse.ArgumentParser(description='pipeline')
parser.add_argument("-y", "--yaml", default='../testv.yaml',type=str,help='Location of yaml file')
parser.add_argument("-t", "--train_test", default=True,type=str, help="True if training, False if testing")
parser.add_argument("-m", "--model", default='RandomForestClassifierWrapper',
                    help="name of model,no parentheses. see model_storage.py for available models")
parser.add_argument("-p", "--params", default='', help="parameters for model")
parser.add_argument("-s",'--save_load_path',default='../fitted.pkl',type=str,help='path to saved models')
parser.add_argument("-mod",'--module_name',default='model_storage',type=str,help='module name for model')

args=parser.parse_args()


def function_importer(module_name,class_name):
    module = __import__(module_name)
    my_class = getattr(module, class_name)
    instance = my_class()
    return instance 


def main(path,train,model,save_load_path,*params):
    data=load_filter(path)
    fitted=[]
    predicted=[]
    
    #TF=str_to_bool(train)
    TF=str_to_bool(train)
    x=data.filter_data_x(TF)
    
    model=str_to_function(model_storage,model)
    #param=parameter_var(params)  
    #parameters= ",".join(param)    
    if TF==True:
        y=data.filter_data_y()
        for i in range (0,len(x)):
            fitted.append(model().fit(X=x[i],y=y))
        joblib.dump(fitted, save_load_path)   
    elif TF==False:     
        predict_model=joblib.load(save_load_path)
        for j in range (0,len(x)):
            predicted.append(predict_model[j].predict(x[j]))
        return predicted
    
    else: 
        print('train needs to be True or False')
    print('Done')


out=main(path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/testv.yaml',train='True',model='AdaBoostClassifierWrapper',
         save_load_path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/fitted.pkl')

#out=main(path='D:/MLCourses/ML1030/data/testv.yaml',train='True',
         #model='AdaBoostClassifierWrapper',save_load_path='D:/MLCourses/ML1030/models/fitted.pkl')
230/3:
import argparse
import yaml
from sklearn.externals import joblib 
import model_storage



from load_filter import load_filter
from str_transform import str_to_bool,str_to_function
#from parameter_var import parameter_var
parser = argparse.ArgumentParser(description='pipeline')
parser.add_argument("-y", "--yaml", default='../testv.yaml',type=str,help='Location of yaml file')
parser.add_argument("-t", "--train_test", default=True,type=str, help="True if training, False if testing")
parser.add_argument("-m", "--model", default='RandomForestClassifierWrapper',
                    help="name of model,no parentheses. see model_storage.py for available models")
parser.add_argument("-p", "--params", default='', help="parameters for model")
parser.add_argument("-s",'--save_load_path',default='../fitted.pkl',type=str,help='path to saved models')
parser.add_argument("-mod",'--module_name',default='model_storage',type=str,help='module name for model')

args=parser.parse_args()


def function_importer(module_name,class_name):
    module = __import__(module_name)
    my_class = getattr(module, class_name)
    instance = my_class()
    return instance 


def main(path,train,model,save_load_path,*params):
    data=load_filter(path)
    fitted=[]
    predicted=[]
    
    #TF=str_to_bool(train)
    TF=str_to_bool(train)
    x=data.filter_data_x(TF)
    
    model=str_to_function(model_storage,model)
    #param=parameter_var(params)  
    #parameters= ",".join(param)    
    if TF==True:
        y=data.filter_data_y()
        for i in range (0,len(x)):
            fitted.append(model().fit(X=x[i],y=y))
        joblib.dump(fitted, save_load_path)   
    elif TF==False:     
        predict_model=joblib.load(save_load_path)
        for j in range (0,len(x)):
            predicted.append(predict_model[j].predict(x[j]))
        return predicted
    
    else: 
        print('train needs to be True or False')
    print('Done')


out=main(path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/testv.yaml',train='True',model='AdaBoostClassifierWrapper',
         save_load_path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/fitted.pkl')
233/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
233/2:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df2 = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df2.copy()
##For my local computer
df2 = pd.read_hdf(path_to_data+'2.h5')
input_files=df2.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
233/3:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.001)
input_files = df[:index]
233/4:
##I only work with .1 percent of the data for the time being
index = ceil(len(input_files)*0.001)
input_files = df[:index]
233/5:
##I only work with .1 percent of the data for the time being
index = ceil(len(input_files)*0.001)
input_files = input_files[:index]
233/6: input_files
233/7: input_files.columns
233/8: list(input_files.columns)
235/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
235/2:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df2 = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df2.copy()
##For my local computer
df2 = pd.read_hdf(path_to_data+'2.h5')
input_files=df2.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
235/3:
##I only work with .1 percent of the data for the time being
index = ceil(len(input_files)*0.001)
input_files = input_files[:index]
235/4:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
235/5:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df2 = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df2.copy()
##For my local computer
df2 = pd.read_hdf(path_to_data+'2.h5')
input_files=df2.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
235/6:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df2 = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df2.copy()
##For my local computer
df2 = pd.read_hdf(path_to_data+'2.h5')
input_files=df2.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
235/7:
##I only work with .1 percent of the data for the time being
index = ceil(len(input_files)*0.001)
input_files = input_files[:index]
235/8:
##I only work with .1 percent of the data for the time being
index = ceil(len(input_files)*0.001)
input_files = input_files[:index]
input_files.head()
235/9:
##I only work with .1 percent of the data for the time being
index = ceil(len(input_files)*0.001)
input_files = input_files[:index]
input_files.head(10)
235/10:
##I only work with .1 percent of the data for the time being
index = ceil(len(input_files)*0.001)
input_files = input_files[:index]
input_files.head()
235/11:
##I only work with .1 percent of the data for the time being
index = ceil(len(input_files)*0.001)
input_files = input_files[:index]
input_files.shape
235/12:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df2 = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df2.copy()
##For my local computer
df2 = pd.read_hdf(path_to_data+'2.h5')
input_files=df2.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
235/13:
##I only work with .1 percent of the data for the time being
#index = ceil(len(input_files)*0.001)
#input_files = input_files[:index]
input_files.shape
235/14:
##I only work with .1 percent of the data for the time being
#index = ceil(len(input_files)*0.001)
#input_files = input_files[:index]
input_files.head()
235/15:
##I only work with .1 percent of the data for the time being
#index = ceil(len(input_files)*0.001)
#input_files = input_files[:index]
input_files.head(20)
235/16:
#Empyircal Metrics
#https://github.com/quantopian/empyrical/blob/badbdca75f5b293f28b5e947974894de041d6868/empyrical/__init__.py
empy_metrics=[aggregate_returns,
    alpha,
    alpha_aligned,
    alpha_beta,
    alpha_beta_aligned,
    annual_return,
    annual_volatility,
    beta,
    beta_aligned,
    cagr,
    calmar_ratio,
    capture,
    conditional_value_at_risk,
    cum_returns,
    cum_returns_final,
    down_alpha_beta,
    down_capture,
    downside_risk,
    excess_sharpe,
    max_drawdown,
    omega_ratio,
    roll_alpha,
    roll_alpha_aligned,
    roll_alpha_beta,
    roll_alpha_beta,
    roll_alpha_beta_aligned,
    roll_annual_volatility,
    roll_beta,
    roll_beta_aligned,
    roll_down_capture,
    roll_max_drawdown,
    roll_sharpe_ratio,
    roll_sortino_ratio,
    roll_up_capture,
    roll_up_down_capture,
    sharpe_ratio,
    simple_returns,
    sortino_ratio,
    stability_of_timeseries,
    tail_ratio,
    up_alpha_beta,
    up_capture,
    up_down_capture,
    value_at_risk,
]
235/17:
#Empyircal Metrics
#https://github.com/quantopian/empyrical/blob/badbdca75f5b293f28b5e947974894de041d6868/empyrical/__init__.py
empy_metrics=['aggregate_returns',
    'alpha',
    'alpha_aligned',
    'alpha_beta',
    'alpha_beta_aligned',
    'annual_return',
    'annual_volatility',
    'beta',
    'beta_aligned',
    'cagr',
    'calmar_ratio',
    'capture',
    'conditional_value_at_risk',
    'cum_returns',
   ' cum_returns_final',
    'down_alpha_beta',
    'down_capture',
    'downside_risk',
    'excess_sharpe',
    'max_drawdown',
    'omega_ratio',
    'roll_alpha',
    'roll_alpha_aligned',
    'roll_alpha_beta',
    'roll_alpha_beta',
    'roll_alpha_beta_aligned',
    'roll_annual_volatility',
    'roll_beta',
    'roll_beta_aligned',
    'roll_down_capture',
    'roll_max_drawdown',
    'roll_sharpe_ratio',
    'roll_sortino_ratio',
    'roll_up_capture',
    'roll_up_down_capture',
    'sharpe_ratio',
    'simple_returns',
    'sortino_ratio',
    'stability_of_timeseries',
    'tail_ratio',
    'up_alpha_beta',
    'up_capture',
    'up_down_capture'
]
len(empy_metrics)
235/18: import empyrical
235/19:
import empyrical
roll_up_capture?
235/20:
from  empyrical import (
    aggregate_returns,
    alpha,
    alpha_aligned,
    alpha_beta,
    alpha_beta_aligned,
    annual_return,
    annual_volatility,
    beta,
    beta_aligned,
    cagr,
    calmar_ratio,
    capture,
    conditional_value_at_risk,
    cum_returns,
    cum_returns_final,
    down_alpha_beta,
    down_capture,
    downside_risk,
    excess_sharpe,
    max_drawdown,
    omega_ratio,
    roll_alpha,
    roll_alpha_aligned,
    roll_alpha_beta,
    roll_alpha_beta,
    roll_alpha_beta_aligned,
    roll_annual_volatility,
    roll_beta,
    roll_beta_aligned,
    roll_down_capture,
    roll_max_drawdown,
    roll_sharpe_ratio,
    roll_sortino_ratio,
    roll_up_capture,
    roll_up_down_capture,
    sharpe_ratio,
    simple_returns,
    sortino_ratio,
    stability_of_timeseries,
    tail_ratio,
    up_alpha_beta,
    up_capture,
    up_down_capture,
    value_at_risk,
)
roll_up_capture?
235/21:
from  empyrical import (
    aggregate_returns,
    alpha,
    alpha_aligned,
    alpha_beta,
    alpha_beta_aligned,
    annual_return,
    annual_volatility,
    beta,
    beta_aligned,
    cagr,
    calmar_ratio,
    capture,
    conditional_value_at_risk,
    cum_returns,
    cum_returns_final,
    down_alpha_beta,
    down_capture,
    downside_risk,
    excess_sharpe,
    max_drawdown,
    omega_ratio,
    roll_alpha,
    roll_alpha_aligned,
    roll_alpha_beta,
    roll_alpha_beta,
    roll_alpha_beta_aligned,
    roll_annual_volatility,
    roll_beta,
    roll_beta_aligned,
    roll_down_capture,
    roll_max_drawdown,
    roll_sharpe_ratio,
    roll_sortino_ratio,
    roll_up_capture,
    roll_up_down_capture,
    sharpe_ratio,
    simple_returns,
    sortino_ratio,
    stability_of_timeseries,
    tail_ratio,
    up_alpha_beta,
    up_capture,
    up_down_capture,
    value_at_risk,
)
235/22: roll_up_capture?
235/23: roll_up_capture?
235/24: roll_up_capture?
235/25:  aggregate_returns?
237/1:
target_col = 'sig_label_sec_1'#target column
run_testing = False #if the data is for testing then run_tesiting=True, else it is for training
# columns to be used for training or testing
# ['MOVING_AVG',
#  'CCI',
#  'RSI',
#  'CMO',
#  'STOCH',
#  'BBI_2',
#  'BBI_3',
#  'INV_MOVING_AVG',
#  'INV_CCI',
#  'INV_RSI',
#  'INV_CMO',
#  'INV_STOCH']
feature = 'MOVING_AVG' #from list above
model_name = 'RF' #the model used for training to testing
237/2:
#from  Testing import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
from sklearn.base import clone
import yaml 
import itertools
from math import ceil
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
import pickle
from sklearn.base import clone
#---------------------------------------------------------------
#from classifierwrapper import * #we created it
237/3:
##For my local computer
df = pd.read_hdf('../1.h5')
#---------------------------------------------------------------------------------
input_files=df.copy()

#---------------------------------------------------------------------------------
237/4:
#creating directories we need, if they do not exsit already
os.makedirs("../saved_models", exist_ok=True) 

saved_folder = "../saved_models/"
#---------------------------------------------------
237/5:
#some column explanations to creat feature columns
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
237/6:
#we have 12 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

features_dic = {} #empty dictionary that will contain the sets of features
#dictionary of feature sets to be used for each model

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    features_dic[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    features_dic[inv_ti[i]] = lf
237/7: list(features_dic)
237/8:
with open('../features_dic.txt', 'wb') as handle:
    pickle.dump(features_dic, handle)
    
with open('../features_dic.txt', 'rb') as handle:
    b = pickle.loads(handle.read())
b==features_dic
237/9: b
235/26: import re
235/27:
import re
re.match(r'ss','ssssk')
235/28:
import re
re.match(r'd','ssssk')
235/29:
import re
z=re.match(r'd','ssssk')
z
235/30:
import re
z=re.match(r'k','ssssk')
z
235/31:
import re
z=re.match(r'ssss','ssssk')
z
235/32:
import re
z=re.match(r'ssss','ssssk')
z.group
235/33:
import re
z=re.match(r'ssss','ssssk')
z.group()
235/34:
import re
z=re.match(r'ssss','sssskjj')
z.group()
235/35: cols=list(input_files.columns)
235/36:
for col in cols:
    z=re.match(r"sig_")
    if not z
    print(z.group())
235/37:
for col in cols:
    z=re.match(r"sig_")
    if not z:
     print(z.group())
235/38:
for col in cols:
    z=re.match(r"sig_",col)
    if not z:
     print(z.group())
235/39:
for col in cols:
    z=re.match(r"sig_",col)
    if not z:
     print(z.groups())
235/40:
for col in cols:
    z=re.match(r"sig_",col)
    if  z:
     print(z.groups())
235/41:
for col in cols:
    if not re.match(r'^sig_.*', col_name):
        print(z.groups())
235/42:
for col in cols:
    if not re.match(r'^sig_.*', col):
        print(z.groups())
235/43:
for col in cols:
    if  re.match(r'^sig_.*', col):
        print(z.groups())
235/44:  metric = pd.DataFrame({"sig_name": ['kk'], "time": 'j'}, columns=['ddddd','dfgf'])
235/45:
metric = pd.DataFrame({"sig_name": ['kk'], "time": 'j'}, columns=['ddddd','dfgf'])
metric
235/46: cum_returns_final?
235/47:  sharpe_ratio?
235/48:  roll_alpha_beta?
235/49: alpha_beta?
236/1:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
236/2:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
236/3:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
#input_files=df.copy()
236/4: input_files=df.copy()
236/5: len(input_files)
236/6:
index = ceil(len(input_files)*0.8)
index
236/7:

index = ceil(len(input_files)*0.8)
training_index=ceil(index*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_valid_contracts = input_files[training_index:index]
input_files_test_contracts = input_files[index:]
236/8: input_files_train_contracts.shape
236/9:
index = ceil(len(input_files))
training_index=ceil(index*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_valid_contracts = input_files[training_index:index]
input_files_test_contracts = input_files[index:]
236/10: input_files_test_contracts.to_hdf(path_to_data+'test.h5', key='df', mode='w')
236/11:
input_files_test_contracts.to_hdf(path_to_data+'test.h5', key='df', mode='w')
test = pd.read_hdf(path_to_data+'test.h5')
test.shape
236/12:
index = ceil(len(input_files))
training_index=ceil(index*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
236/13:
input_files_test_contracts.to_hdf(path_to_data+'test.h5', key='df', mode='w')
test = pd.read_hdf(path_to_data+'test.h5')
test.shape
236/14: input_files_train_contracts.shape
236/15: df.shape
236/16: 647977+1511949
234/1: df = pd.read_hdf('../test.h5')
234/2:
import re
import pandas as pd
import numpy as np
df = pd.read_hdf('../test.h5')
234/3:
import re
import pandas as pd
import numpy as np
df = pd.read_hdf('../../test.h5')
234/4:
import re
import pandas as pd
import numpy as np
df = pd.read_hdf('../ /test.h5')
234/5:
import re
import pandas as pd
import numpy as np
df = pd.read_hdf('../../test.h5')
234/6:
import re
import pandas as pd
import numpy as np
df = pd.read_hdf('../test.h5')
234/7:
import re
import pandas as pd
import numpy as np
df = pd.read_hdf('../test.h5')
234/8:
import re
import pandas as pd
import numpy as np
df = pd.read_hdf('../test.h5')
234/9:
import re
import pandas as pd
import numpy as np
df = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/test.h5')
236/17:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
234/10: wd
234/11:
import os
print(os.getcwd())
234/12:
metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]
234/13: grid=1
234/14: df=pd.read_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/test.h5')
234/15: grids = [1]
234/16: metrics = pd.DataFrame(columns=metrics_columns)
234/17:
for col_name in df.columns:
    if not re.match(r'^sig_.*', col_name):
        continue
    sig = col_name
    for grid in grids:
        if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
            continue
        metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
        ret_col = "ret_sec_" + str(grid)
        df["ret"] = df[ret_col]
234/18: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
234/19: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
236/18: input_files=df.copy()
236/19: len(input_files)
236/20:
index = ceil(len(input_files))
training_index=ceil(index*0.9)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
236/21:
input_files_test_contracts.to_hdf(path_to_data+'test.h5', key='df', mode='w')
test = pd.read_hdf(path_to_data+'test.h5')
test.shape
236/22:
index = ceil(len(input_files))
training_index=ceil(index*0.9)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
236/23:
input_files_test_contracts.to_hdf(path_to_data+'test.h5', key='df', mode='w')
test = pd.read_hdf(path_to_data+'test.h5')
test.shape
236/24: input_files=df.copy()
236/25: len(input_files)
236/26:
index = ceil(len(input_files))
training_index=ceil(index*0.9)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
236/27:
input_files_test_contracts.to_hdf(path_to_data+'test.h5', key='df', mode='w')
test = pd.read_hdf(path_to_data+'test.h5')
test.shape
236/28: input_files=df.copy()
236/29: len(input_files)
236/30:
index = ceil(len(input_files))
training_index=ceil(index*0.99)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
236/31:
input_files_test_contracts.to_hdf(path_to_data+'test.h5', key='df', mode='w')
test = pd.read_hdf(path_to_data+'test.h5')
test.shape
236/32: input_files_test_contracts.shape
236/33: len(training_index)
236/34: training_index
238/1:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
238/2: X_test = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/test.h5')
238/3: forecast_period=1
238/4: metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
238/5:
metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]
238/6: grids = [forecast_period]
238/7:
metrics = pd.DataFrame(columns=metrics_columns)
metrics
238/8: data_test = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/test.h5')
238/9: df=data_test.copy
238/10: forecast_period=1
238/11:
metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]
238/12: grids = [forecast_period]
238/13: df
238/14: df.columns
238/15: df.columns
238/16: data_test.columns
238/17: df=data_test.copy
238/18: df=data_test.copy()
238/19: df
238/20: df.head(10)
238/21:
    for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/22:
import re
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
238/23:
    for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/24: df
238/25: df.head(10)
238/26:
    for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue
            print(sig)
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/27:
    for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                print(sig)
                continue
           
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/28:
    for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            print(col_name)
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid"
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/29:
    for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            metric
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/30:
    for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            metric
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/31: metric
238/32:
    for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            print(sig)
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/33:
metric = pd.DataFrame(columns=metrics_columns)
for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            print(sig)
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/34:
metric = pd.DataFrame(columns=metrics_columns)
for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            print(sig)
            me = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            metrics=metrics.append(me, ignore_index=True)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/35:
metric = pd.DataFrame(columns=metrics_columns)
for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            print(sig)
            me = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            metric=metric.append(me, ignore_index=True)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/36: metric
238/37:
for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            print(sig)
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/38: metric
238/39:
for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            print(ret_col)
            df["ret"] = df[ret_col]
238/40:
for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/41:
for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            #removes the columns
            #['price',
            # 'forward_price_sec_1',
            # 'ret_sec_1',
            # 'forward_price_sec_3',
            # 'ret_sec_3',
            # 'forward_price_sec_5',
            # 'ret_sec_5',
            # 'forward_price_sec_10',
            # 'ret_sec_10',
            # 'forward_price_sec_15',
            # 'ret_sec_15',
            # 'forward_price_sec_30',
            # 'ret_sec_30']
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue #removes all the other target values, except the "sig_label_sec_grid" from
                #the list # grids = [
                # 'sig_label_sec_1',
                # 'sig_label_sec_3',
                # 'sig_label_sec_5',
                # 'sig_label_sec_10',
                # 'sig_label_sec_15',
                # 'sig_label_sec_30 '   
                # ]
            print(sig)
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
238/42:
grid = forecast_period
sig = 'sig_label_sec_1'
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns
ret_col = "ret_sec_" + str(grid)
df["ret"] = df[ret_col]
238/43:
grid = forecast_period
sig = 'sig_label_sec_1'
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns
ret_col = "ret_sec_1" 
df["ret"] = df[ret_col]
238/44:
grid = forecast_period
sig = 'sig_label_sec_1'
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns
ret_col = "ret_sec_1" 
df["ret"] = df[ret_col]
238/45:
grid = forecast_period
sig = 'sig_label_sec_1'
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
ret_col = "ret_sec_" + str(grid)
df["ret"] = df[ret_col]
238/46:
def evaluate_model(df, forecast_period
                   ):
    metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]
    grids = [forecast_period]
    metrics = pd.DataFrame(columns=metrics_columns)

    for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
            metric, df = calculate_signal_evaluation_metrics(
                metric=metric,
                df=df,
                sig=sig
            )
            metrics = metrics.append(metric, ignore_index=True)
    metrics = metrics[metrics_columns]
    metrics['model_key'] = model_file
    metrics.sort_values(by=["sig_name", "time"], inplace=True)
    return metrics


def func_rate_pnl(win, loss, accuracy):
    return win * (accuracy / 100) + loss * (1 - (accuracy / 100))

def calculate_signal_evaluation_metrics(metric, df, sig):
    """
    This function calculates various metrics for signal performance evaluation for
    the input model against the market data.
    """
    ret_col = "ret"
    df["up"] = np.where(df[ret_col] > 0, 1, 0)
    df["down"] = np.where(df[ret_col] < 0, 1, 0)
    df["neutral"] = np.where(df[ret_col] == 0, 1, 0)
    df["move"] = np.where(df[ret_col] == 0, 0, 1)
    df["sig_up"] = np.where(df[sig] == 1, 1, 0)
    df["sig_down"] = np.where(df[sig] == -1, 1, 0)
    df["trade_ret"] = df["sig_up"] * df[ret_col] - df["sig_down"] * df[ret_col]
    df["lose"] = (df["sig_up"] * df["down"] + df["sig_down"] * df["up"]) > 0
    df["win"] = (df["sig_up"] * df["up"] + df["sig_down"] * df["down"]) > 0
    df["win_lose"] = df["lose"] | df["win"]
    accuracy_denom = sum(df["sig_up"] * df["move"] + df["sig_down"] * df["move"])
    accuracy_numer = sum(df["sig_up"] * df["up"] + df["sig_down"] * df["down"])
    metric["accuracy"] = 0 if accuracy_denom == 0 else accuracy_numer / accuracy_denom
    metric["accuracy"] = round(metric["accuracy"] * 100, 2)
    accuracy_up_denom = sum(df["sig_up"] * df["move"])
    metric["accuracy_up"] = 0 if accuracy_up_denom == 0 else sum(df["sig_up"] * df["up"]) / accuracy_up_denom
    metric["accuracy_up"] = round(metric["accuracy_up"] * 100, 2)
    accuracy_down_denom = sum(df["sig_down"] * df["move"])
    metric["accuracy_down"] = 0 if accuracy_down_denom == 0 else sum(df["sig_down"] * df["down"]) / accuracy_down_denom
    metric["accuracy_down"] = round(metric["accuracy_down"] * 100, 2)
    hit_denom = sum(df["sig_up"] + df["sig_down"])
    metric["hit"] = 0 if hit_denom == 0 else sum(df["sig_up"] * df["up"] + df["sig_down"] * df["down"]) / hit_denom
    metric["hit"] = round(metric["hit"] * 100, 2)
    hit_denom = sum(df["sig_up"])
    metric["hit_up"] = 0 if hit_denom == 0 else sum(df["sig_up"] * df["up"]) / hit_denom
    metric["hit_up"] = round(metric["hit_up"] * 100, 2)
    hit_denom = sum(df["sig_down"])
    metric["hit_down"] = 0 if hit_denom == 0 else sum(df["sig_down"] * df["down"]) / hit_denom
    metric["hit_down"] = round(metric["hit_down"] * 100, 2)
    metric["num_sig_ups"] = sum(df["sig_up"])
    metric["num_sig_downs"] = sum(df["sig_down"])
    coverage_denom = df["sig_down"].count()
    metric["coverage"] = 0 if coverage_denom == 0 else sum(df["sig_up"] + df["sig_down"]) / coverage_denom
    metric["avg_win"] = df[df["win"]]["trade_ret"].median()
    metric["avg_win"].fillna(value=0, inplace=True)
    metric["mean_win"] = df[df["win"]]["trade_ret"].mean()
    metric["mean_win"].fillna(value=0, inplace=True)
    metric["avg_loss"] = df[df["lose"]]["trade_ret"].median()
    metric["avg_loss"].fillna(value=0, inplace=True)
    metric["mean_loss"] = df[df["lose"]]["trade_ret"].mean()
    metric["mean_loss"].fillna(value=0, inplace=True)
    metric["avg_tot_PL"] = func_rate_pnl(metric["avg_win"], metric["avg_loss"], metric["accuracy"])
    metric["mean_tot_PL"] = func_rate_pnl(metric["mean_win"], metric["mean_loss"], metric["accuracy"])
    metric["sdev_win"] = df[df["win"]]["trade_ret"].std()
    metric["sdev_loss"] = df[df["lose"]]["trade_ret"].std()
    metric["sdev_tot_PL"] = df[df["win_lose"]]["trade_ret"].std()
    sharpe_denom = (df[df["win_lose"]]["trade_ret"].std())
    metric["sharpe_ratio"] = 0 if sharpe_denom == 0 else (df[df["win_lose"]]["trade_ret"].mean()) / sharpe_denom
    return metric, df
238/47:
metric, df = calculate_signal_evaluation_metrics(
            metric=metric,
            df=df,
            sig=sig
        )
238/48: metric
238/49:
for col_name in df.columns:
        if not re.match(r'^sig_.*', col_name):
            continue
        sig = col_name
        for grid in grids:
            if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
                continue
            metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
            ret_col = "ret_sec_" + str(grid)
            df["ret"] = df[ret_col]
            metric, df = calculate_signal_evaluation_metrics(
                metric=metric,
                df=df,
                sig=sig
            )
            metrics = metrics.append(metric, ignore_index=True)
    metrics = metrics[metrics_columns]
    metrics['model_key'] = model_file
    metrics.sort_values(by=["sig_name", "time"], inplace=True)
238/51:
for col_name in df.columns:
    if not re.match(r'^sig_.*', col_name):
        continue
    sig = col_name
    for grid in grids:
        if sig.startswith("sig_label_sec") & (sig.split("_")[-1] != str(grid)):
            continue
        metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
        ret_col = "ret_sec_" + str(grid)
        df["ret"] = df[ret_col]
        metric, df = calculate_signal_evaluation_metrics(
            metric=metric,
            df=df,
            sig=sig
        )
        metrics = metrics.append(metric, ignore_index=True)
metrics = metrics[metrics_columns]
metrics['model_key'] = model_file
metrics.sort_values(by=["sig_name", "time"], inplace=True)
238/52: metrics.sort_values(by=["sig_name", "time"], inplace=True)
238/53: metrics
238/54:
#Empyircal Metrics
#https://github.com/quantopian/empyrical/blob/badbdca75f5b293f28b5e947974894de041d6868/empyrical/__init__.py
empy_metrics=['aggregate_returns',
    'alpha',
    'alpha_aligned',
    'alpha_beta',
    'alpha_beta_aligned',
    'annual_return',
    'annual_volatility',
    'beta',
    'beta_aligned',
    'cagr',
    'calmar_ratio',
    'capture',
    'conditional_value_at_risk',
    'cum_returns',
   ' cum_returns_final',
    'down_alpha_beta',
    'down_capture',
    'downside_risk',
    'excess_sharpe',
    'max_drawdown',
    'omega_ratio',
    'roll_alpha',
    'roll_alpha_aligned',
    'roll_alpha_beta',
    'roll_alpha_beta',
    'roll_alpha_beta_aligned',
    'roll_annual_volatility',
    'roll_beta',
    'roll_beta_aligned',
    'roll_down_capture',
    'roll_max_drawdown',
    'roll_sharpe_ratio',
    'roll_sortino_ratio',
    'roll_up_capture',
    'roll_up_down_capture',
    'sharpe_ratio',
    'simple_returns',
    'sortino_ratio',
    'stability_of_timeseries',
    'tail_ratio',
    'up_alpha_beta',
    'up_capture',
    'up_down_capture'
]
len(empy_metrics)
238/55:
from  empyrical import (
    aggregate_returns,
    alpha,
    alpha_aligned,
    alpha_beta,
    alpha_beta_aligned,
    annual_return,
    annual_volatility,
    beta,
    beta_aligned,
    cagr,
    calmar_ratio,
    capture,
    conditional_value_at_risk,
    cum_returns,
    cum_returns_final,
    down_alpha_beta,
    down_capture,
    downside_risk,
    excess_sharpe,
    max_drawdown,
    omega_ratio,
    roll_alpha,
    roll_alpha_aligned,
    roll_alpha_beta,
    roll_alpha_beta,
    roll_alpha_beta_aligned,
    roll_annual_volatility,
    roll_beta,
    roll_beta_aligned,
    roll_down_capture,
    roll_max_drawdown,
    roll_sharpe_ratio,
    roll_sortino_ratio,
    roll_up_capture,
    roll_up_down_capture,
    sharpe_ratio,
    simple_returns,
    sortino_ratio,
    stability_of_timeseries,
    tail_ratio,
    up_alpha_beta,
    up_capture,
    up_down_capture,
    value_at_risk,
)
238/56: from  empyrical import *
238/57:   sharpe_ratio?
238/58: beta?
238/59: df.head(3)
238/60: df[['ret_sec_1']]['ret_sec_3']
238/61: x=(df[['ret_sec_1']]['ret_sec_3'].std())
238/62: df.columns
238/63:
metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
ret_col = "ret_sec_1"
sig='sig_label_sec_1'
df["up"] = np.where(df[ret_col] > 0, 1, 0)
df["down"] = np.where(df[ret_col] < 0, 1, 0)
df["neutral"] = np.where(df[ret_col] == 0, 1, 0)
df["move"] = np.where(df[ret_col] == 0, 0, 1)
df["sig_up"] = np.where(df[sig] == 1, 1, 0)
df["sig_down"] = np.where(df[sig] == -1, 1, 0)
df["trade_ret"] = df["sig_up"] * df[ret_col] - df["sig_down"] * df[ret_col]
df["lose"] = (df["sig_up"] * df["down"] + df["sig_down"] * df["up"]) > 0
df["win"] = (df["sig_up"] * df["up"] + df["sig_down"] * df["down"]) > 0
df["win_lose"] = df["lose"] | df["win"]
accuracy_denom = sum(df["sig_up"] * df["move"] + df["sig_down"] * df["move"])
accuracy_numer = sum(df["sig_up"] * df["up"] + df["sig_down"] * df["down"])
metric["accuracy"] = 0 if accuracy_denom == 0 else accuracy_numer / accuracy_denom
metric["accuracy"] = round(metric["accuracy"] * 100, 2)
accuracy_up_denom = sum(df["sig_up"] * df["move"])
metric["accuracy_up"] = 0 if accuracy_up_denom == 0 else sum(df["sig_up"] * df["up"]) / accuracy_up_denom
metric["accuracy_up"] = round(metric["accuracy_up"] * 100, 2)
accuracy_down_denom = sum(df["sig_down"] * df["move"])
metric["accuracy_down"] = 0 if accuracy_down_denom == 0 else sum(df["sig_down"] * df["down"]) / accuracy_down_denom
metric["accuracy_down"] = round(metric["accuracy_down"] * 100, 2)
hit_denom = sum(df["sig_up"] + df["sig_down"])
metric["hit"] = 0 if hit_denom == 0 else sum(df["sig_up"] * df["up"] + df["sig_down"] * df["down"]) / hit_denom
metric["hit"] = round(metric["hit"] * 100, 2)
hit_denom = sum(df["sig_up"])
metric["hit_up"] = 0 if hit_denom == 0 else sum(df["sig_up"] * df["up"]) / hit_denom
metric["hit_up"] = round(metric["hit_up"] * 100, 2)
hit_denom = sum(df["sig_down"])
metric["hit_down"] = 0 if hit_denom == 0 else sum(df["sig_down"] * df["down"]) / hit_denom
metric["hit_down"] = round(metric["hit_down"] * 100, 2)
metric["num_sig_ups"] = sum(df["sig_up"])
metric["num_sig_downs"] = sum(df["sig_down"])
coverage_denom = df["sig_down"].count()
metric["coverage"] = 0 if coverage_denom == 0 else sum(df["sig_up"] + df["sig_down"]) / coverage_denom
metric["avg_win"] = df[df["win"]]["trade_ret"].median()
metric["avg_win"].fillna(value=0, inplace=True)
metric["mean_win"] = df[df["win"]]["trade_ret"].mean()
metric["mean_win"].fillna(value=0, inplace=True)
metric["avg_loss"] = df[df["lose"]]["trade_ret"].median()
metric["avg_loss"].fillna(value=0, inplace=True)
metric["mean_loss"] = df[df["lose"]]["trade_ret"].mean()
metric["mean_loss"].fillna(value=0, inplace=True)
metric["avg_tot_PL"] = func_rate_pnl(metric["avg_win"], metric["avg_loss"], metric["accuracy"])
metric["mean_tot_PL"] = func_rate_pnl(metric["mean_win"], metric["mean_loss"], metric["accuracy"])
metric["sdev_win"] = df[df["win"]]["trade_ret"].std()
metric["sdev_loss"] = df[df["lose"]]["trade_ret"].std()
metric["sdev_tot_PL"] = df[df["win_lose"]]["trade_ret"].std()
sharpe_denom = (df[df["win_lose"]]["trade_ret"].std())
metric["sharpe_ratio"] = 0 if sharpe_denom == 0 else (df[df["win_lose"]]["trade_ret"].mean()) / sharpe_denom
return metric, df
238/64:
metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
ret_col = "ret_sec_1"
sig='sig_label_sec_1'
df["up"] = np.where(df[ret_col] > 0, 1, 0)
df["down"] = np.where(df[ret_col] < 0, 1, 0)
df["neutral"] = np.where(df[ret_col] == 0, 1, 0)
df["move"] = np.where(df[ret_col] == 0, 0, 1)
df["sig_up"] = np.where(df[sig] == 1, 1, 0)
df["sig_down"] = np.where(df[sig] == -1, 1, 0)
df["trade_ret"] = df["sig_up"] * df[ret_col] - df["sig_down"] * df[ret_col]
df["lose"] = (df["sig_up"] * df["down"] + df["sig_down"] * df["up"]) > 0
df["win"] = (df["sig_up"] * df["up"] + df["sig_down"] * df["down"]) > 0
df["win_lose"] = df["lose"] | df["win"]
accuracy_denom = sum(df["sig_up"] * df["move"] + df["sig_down"] * df["move"])
accuracy_numer = sum(df["sig_up"] * df["up"] + df["sig_down"] * df["down"])
metric["accuracy"] = 0 if accuracy_denom == 0 else accuracy_numer / accuracy_denom
metric["accuracy"] = round(metric["accuracy"] * 100, 2)
accuracy_up_denom = sum(df["sig_up"] * df["move"])
metric["accuracy_up"] = 0 if accuracy_up_denom == 0 else sum(df["sig_up"] * df["up"]) / accuracy_up_denom
metric["accuracy_up"] = round(metric["accuracy_up"] * 100, 2)
accuracy_down_denom = sum(df["sig_down"] * df["move"])
metric["accuracy_down"] = 0 if accuracy_down_denom == 0 else sum(df["sig_down"] * df["down"]) / accuracy_down_denom
metric["accuracy_down"] = round(metric["accuracy_down"] * 100, 2)
hit_denom = sum(df["sig_up"] + df["sig_down"])
metric["hit"] = 0 if hit_denom == 0 else sum(df["sig_up"] * df["up"] + df["sig_down"] * df["down"]) / hit_denom
metric["hit"] = round(metric["hit"] * 100, 2)
hit_denom = sum(df["sig_up"])
metric["hit_up"] = 0 if hit_denom == 0 else sum(df["sig_up"] * df["up"]) / hit_denom
metric["hit_up"] = round(metric["hit_up"] * 100, 2)
hit_denom = sum(df["sig_down"])
metric["hit_down"] = 0 if hit_denom == 0 else sum(df["sig_down"] * df["down"]) / hit_denom
metric["hit_down"] = round(metric["hit_down"] * 100, 2)
metric["num_sig_ups"] = sum(df["sig_up"])
metric["num_sig_downs"] = sum(df["sig_down"])
coverage_denom = df["sig_down"].count()
metric["coverage"] = 0 if coverage_denom == 0 else sum(df["sig_up"] + df["sig_down"]) / coverage_denom
metric["avg_win"] = df[df["win"]]["trade_ret"].median()
metric["avg_win"].fillna(value=0, inplace=True)
metric["mean_win"] = df[df["win"]]["trade_ret"].mean()
metric["mean_win"].fillna(value=0, inplace=True)
metric["avg_loss"] = df[df["lose"]]["trade_ret"].median()
metric["avg_loss"].fillna(value=0, inplace=True)
metric["mean_loss"] = df[df["lose"]]["trade_ret"].mean()
metric["mean_loss"].fillna(value=0, inplace=True)
metric["avg_tot_PL"] = func_rate_pnl(metric["avg_win"], metric["avg_loss"], metric["accuracy"])
metric["mean_tot_PL"] = func_rate_pnl(metric["mean_win"], metric["mean_loss"], metric["accuracy"])
metric["sdev_win"] = df[df["win"]]["trade_ret"].std()
metric["sdev_loss"] = df[df["lose"]]["trade_ret"].std()
metric["sdev_tot_PL"] = df[df["win_lose"]]["trade_ret"].std()
sharpe_denom = (df[df["win_lose"]]["trade_ret"].std())
metric["sharpe_ratio"] = 0 if sharpe_denom == 0 else (df[df["win_lose"]]["trade_ret"].mean()) / sharpe_denom
238/65: metric
238/66: df[df["win_lose"]]
238/67: df["win_lose"]
238/68: df[df["win_lose"]]
238/69: df["lose"]
238/70: df["win"]
238/71: df["lose"] | df["win"]
238/72:   sharpe_ratio?
238/73: beta?
238/74:   sharpe_ratio?
238/75:   simple_returns?
238/76: value_at_risk?
238/77:
forecast_periods=[1,3,5,10,10,15,30]

df = pd.dataframe()
for grid in forecast_period:
    df= df.append(data[sig] if sig= 'sig_label_sec_'+str(grid))
238/78: df=data_test.copy()
238/79:
forecast_periods=[1,3,5,10,10,15,30]

d = pd.dataframe()
for grid in forecast_period:
    d['sig_label_sec_'+str(grid)] = df['sig_label_sec_'+str(grid)]
238/80:
forecast_periods=[1,3,5,10,10,15,30]

d = pd.DataFrame()
for grid in forecast_period:
    d['sig_label_sec_'+str(grid)] = df['sig_label_sec_'+str(grid)]
238/81:
forecast_periods=[1,3,5,10,10,15,30]

d = pd.DataFrame()
for grid in forecast_periods:
    d['sig_label_sec_'+str(grid)] = df['sig_label_sec_'+str(grid)]
238/82: d.head(10)
238/83: metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
238/84:
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
metric
238/85:
forecast_periods=[1,3,5,10,10,15,30]

d = pd.DataFrame()
for grid in forecast_periods:
    d['sig_label_sec_'+str(grid)] = df['sig_label_sec_'+str(grid)]
238/86: d
238/87: df=d.copy()
238/88: target_df=d.copy()
238/89:
def evaluate_model(X,target_df,forecast_periods):
     df = pd.DataFrame()
     metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]

     metrics = pd.DataFrame(columns=metrics_columns)
     for grid in forecast_periods:
       sig = 'sig_label_sec_'+str(grid)
       y_pred=stacking(X,grid)
       df[sig] = target_df['sig_label_sec_'+str(grid)]
       df[sig+'pred'] = y_pred
       metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
       metric ,df = calculate_signal_evaluation_metrics(metric=metric, df=df, sig=sig)
     metrics = metrics.append(metric, ignore_index=True)
     metrics = metrics[metrics_columns]
     metrics.sort_values(by=["sig_name", "time"], inplace=True)
     return metrics
238/90:
def evaluate_model(X,target_df,forecast_periods):
     df = pd.DataFrame()
     metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]

     metrics = pd.DataFrame(columns=metrics_columns)
     for grid in forecast_periods:
        sig = 'sig_label_sec_'+str(grid)
       #y_pred=stacking(X,grid)
        y_pred=target_df[sig]
        df[sig] = target_df[sig]
        df[sig+'pred'] = y_pred
        metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
        metric ,df = calculate_signal_evaluation_metrics(metric=metric, df=df, sig=sig)
     metrics = metrics.append(metric, ignore_index=True)
     metrics = metrics[metrics_columns]
     metrics.sort_values(by=["sig_name", "time"], inplace=True)
     return metrics
238/91:
forecast_periods=[1,3,5,10,10,15,30]
metrics = evaluate_model(X,target_df, forecast_periods)

metrics.to_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/metrics.h5', key='df', mode='w')
238/92: data_test = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/test.h5')
238/93: df=data_test.copy()
238/94: df = pd.read_hdf(path_to_data+'2.h5')
238/95:
#data_test = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/test.h5')
df = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/2.h5')
238/96: df.shape
238/97:
#let's use 1 percent of data for the time being
index = ceil(len(df)*0.01)
input_files = df[:index]
238/98: df=input_files.copy()
238/99: df.shape
238/100:
forecast_periods=[1,3,5,10,10,15,30]
metrics = evaluate_model(df, forecast_periods)

metrics.to_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/metrics.h5', key='df', mode='w')
238/101:
def evaluate_model(data,forecast_periods):
     df = pd.DataFrame()
     metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]

     metrics = pd.DataFrame(columns=metrics_columns)
     for grid in forecast_periods:
       sig = 'sig_label_sec_'+str(grid)
       #y_pred = stacking(data,grid)
       y_pred = data[sig]
       df[sig] = data[sig]
       df[sig+'pred'] = y_pred
       metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
       metric ,df = calculate_signal_evaluation_metrics(metric=metric, df=df, sig=sig)
     metrics = metrics.append(metric, ignore_index=True)
     metrics = metrics[metrics_columns]
     metrics.sort_values(by=["sig_name", "time"], inplace=True)
     return metrics
238/102:
forecast_periods=[1,3,5,10,10,15,30]
metrics = evaluate_model(df, forecast_periods)

metrics.to_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/metrics.h5', key='df', mode='w')
238/103: df["ret_sec_" + str(grid)]
238/104: df[1]
238/105: df[[1]]
238/106: df.columns
238/107: df['sig_label_sec_1']+1
238/108:
def evaluate_model(data,forecast_periods):
     df = pd.DataFrame()
     metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]

     metrics = pd.DataFrame(columns=metrics_columns)
     for grid in forecast_periods:
       sig = 'sig_label_sec_'+str(grid)
       #y_pred = stacking(data,grid)-1
       y_pred = data[sig]
       df['ret'] = data["ret_sec_" + str(grid)]
       df[sig+'pred'] = y_pred
       metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
       metric ,df = calculate_signal_evaluation_metrics(metric=metric, df=df, sig=sig)
     metrics = metrics.append(metric, ignore_index=True)
     metrics = metrics[metrics_columns]
     metrics.sort_values(by=["sig_name", "time"], inplace=True)
     return metrics
238/109:
forecast_periods=[1,3,5,10,10,15,30]
metrics = evaluate_model(df, forecast_periods)

metrics.to_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/metrics.h5', key='df', mode='w')
238/110: df.columns
238/111: df['sig_label_sec_1']
238/112:
sig = 'sig_label_sec_'+str(grid)
sig
238/113:
def evaluate_model(data,forecast_periods):
     df = pd.DataFrame()
     metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]

     metrics = pd.DataFrame(columns=metrics_columns)
     for grid in forecast_periods:
       sig = 'sig_label_sec_'+str(grid)
       #y_pred = stacking(data,grid)-1
       y_pred = data[sig]
       df['ret'] = data["ret_sec_" + str(grid)]
       df[sig] = y_pred
       metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
       metric ,df = calculate_signal_evaluation_metrics(metric=metric, df=df, sig=sig)
     metrics = metrics.append(metric, ignore_index=True)
     metrics = metrics[metrics_columns]
     metrics.sort_values(by=["sig_name", "time"], inplace=True)
     return metrics
238/114:
forecast_periods=[1,3,5,10,10,15,30]
metrics = evaluate_model(df, forecast_periods)

metrics.to_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/metrics.h5', key='df', mode='w')
238/115: metrics
234/20: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
234/21: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
234/22: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
234/23: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
234/24: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
234/25: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
234/26: grid=1
234/27:
sig = 'sig_label_sec_'+str(grid)
#y_pred = stacking(data,grid)-1 # I do not have it yet
df['ret'] = data["ret_sec_" + str(grid)]
#df[sig] = y_pred
df[sig] = data[sig]
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
metric ,df = calculate_signal_evaluation_metrics(metric=metric, df=df, sig=sig)
234/28:
data = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/2.h5')
index = ceil(len(data)*0.01)
data = data[:index]
234/29:
sig = 'sig_label_sec_'+str(grid)
#y_pred = stacking(data,grid)-1 # I do not have it yet
df['ret'] = data["ret_sec_" + str(grid)]
#df[sig] = y_pred
df[sig] = data[sig]
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
234/30: df = pd.DataFrame()
234/31:
sig = 'sig_label_sec_'+str(grid)
#y_pred = stacking(data,grid)-1 # I do not have it yet
df['ret'] = data["ret_sec_" + str(grid)]
#df[sig] = y_pred
df[sig] = data[sig]
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
234/32:
metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                  "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                  "mean_tot_PL", "sharpe_ratio"]
234/33: metrics = pd.DataFrame(columns=metrics_columns)
234/34:
sig = 'sig_label_sec_'+str(grid)
#y_pred = stacking(data,grid)-1 # I do not have it yet
df['ret'] = data["ret_sec_" + str(grid)]
#df[sig] = y_pred
df[sig] = data[sig]
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
234/35: metric ,df = calculate_signal_evaluation_metrics(metric=metric, df=df, sig=sig)
234/36: metrics = metrics.append(metric, ignore_index=True)
234/37:
metrics = metrics[metrics_columns]
metrics.sort_values(by=["sig_name", "time"], inplace=True)
234/38: grid=3
234/39:
sig = 'sig_label_sec_'+str(grid)
#y_pred = stacking(data,grid)-1 # I do not have it yet
df['ret'] = data["ret_sec_" + str(grid)]
#df[sig] = y_pred
df[sig] = data[sig]
metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
metric ,df = calculate_signal_evaluation_metrics(metric=metric, df=df, sig=sig)
234/40:
metrics = metrics.append(metric, ignore_index=True)
metrics = metrics[metrics_columns]
metrics.sort_values(by=["sig_name", "time"], inplace=True)
238/116:
def evaluate_model(data,forecast_periods):
     df = pd.DataFrame()
     metrics_columns = ["sig_name", "time", "accuracy", "accuracy_up", "accuracy_down", "hit", "hit_up", "hit_down",
                       "num_sig_ups", "num_sig_downs", "coverage", "avg_win", "avg_loss", "avg_tot_PL",
                       "mean_tot_PL", "sharpe_ratio"]

     metrics = pd.DataFrame(columns=metrics_columns)
     for grid in forecast_periods:
       sig = 'sig_label_sec_'+str(grid)
       #y_pred = stacking(data,grid)-1
       y_pred = data[sig]
       df['ret'] = data["ret_sec_" + str(grid)]
       df[sig] = y_pred
       metric = pd.DataFrame({"sig_name": [sig], "time": grid}, columns=metrics_columns)
       metric ,df = calculate_signal_evaluation_metrics(metric=metric, df=df, sig=sig)
       metrics = metrics.append(metric, ignore_index=True)
       metrics = metrics[metrics_columns]
       metrics.sort_values(by=["sig_name", "time"], inplace=True)
     return metrics
238/117:
forecast_periods=[1,3,5,10,10,15,30]
metrics = evaluate_model(df, forecast_periods)

metrics.to_hdf('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/metrics.h5', key='df', mode='w')
238/118: metrics
240/1: from tweepy import Stream
240/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
239/1:
import pandas as pd
import json
from tweepy.streaming import StreamListener
import twitter
import tweepy
239/2:
df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
df.shape
239/3: df[df.duplicated()]
239/4: df[df.duplicated()].shape
239/5:
import pandas as pd
import json
from tweepy.streaming import StreamListener
import twitter
import tweepy
239/6:


# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    credentials = json.load(file)
239/7:

#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
            except:
                tweet=status.retweeted_status.text
        else:
        #-------------------
            try:
                tweet=status.extended_tweet["full_text"]
                return True
            except BaseException as e:
                tweet=status.text
            return True
        print(tweet)
        df2.loc[len(df2)]=[tweet] 
        df= df.append(df2,ignore_index=True)
        df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
239/8:

auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
api = tweepy.API(auth)
239/9:
# #creating a dataframe once, I already created it
# df = pd.DataFrame(columns=['tweet'])
# df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
239/10:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
240/3: from tweepy import Stream
240/4:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
240/5:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
240/6:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
240/7:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
240/8:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
240/9:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
240/10: df.shape
240/11: sum(df.duplicated())
240/12: (df.duplicated())
240/13: df.duplicated()
240/14: df.duplicated()
240/15: df.created_at
240/16: df.created_at(1:10)
240/17: df.created_at[1:10]
240/18: data.index
240/19:
#keep English languages
data=df[df.lang=='en']
240/20: data.index
240/21: tweets=pd.DataFrame(columns=['time''tweet'],index=data.index)
240/22:
tweets=pd.DataFrame(columns=['time''tweet'],index=data.index)
tweets
240/23:
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
tweets
240/24: data.created_at
240/25: tweets=pd.DataFrame(columns=['time','tweet'])
240/26: data.created_at[1:2]
240/27: data.created_at[1:3]
240/28: data.created_at[0:3]
240/29: data.retweeted_status[2].keys()
240/30: data.retweeted_status[2]
240/31: data.retweeted_status[3].keys()
240/32:
for i in (data.index & range(10)):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweets.tweet[i] = data.text[i]
                    tweets.time[i] = data.created_at[i]
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                     tweets.tweet[i]=data.extended_tweet[i]["full_text"]
                     tweets.time[i] = data.created_at[i]
                    
                else:
                     tweets.tweet[i]=data.text[i] 
                     tweets.time[i] = data.created_at[i]
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    tweets.time[i] = data.created_at[i]
            else:
                 tweets.tweet[i] = data.retweeted_status[i]['text']
                 tweets.time[i] = data.created_at[i]
                    
       print( tweets.tweet[i])
       print('----------------\n')
240/33:
for i in (data.index & range(10)): 
    print(i)
240/34: tweets.tweet[0]='ddd'
240/35: tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
240/36:
for i in (data.index & range(10)):   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweets.tweet[i] = data.text[i]
                    tweets.time[i] = data.created_at[i]
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                     tweets.tweet[i]=data.extended_tweet[i]["full_text"]
                     tweets.time[i] = data.created_at[i]
                    
                else:
                     tweets.tweet[i]=data.text[i] 
                     tweets.time[i] = data.created_at[i]
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    tweets.time[i] = data.created_at[i]
            else:
                 tweets.tweet[i] = data.retweeted_status[i]['text']
                 tweets.time[i] = data.created_at[i]
                    
       print( tweets.tweet[i])
       print('----------------\n')
240/37: tweets
240/38: tweets.tweet[2]
240/39: tweets.duplicated
240/40:
for i in data.index:   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweets.tweet[i] = data.text[i]
                    tweets.time[i] = data.created_at[i]
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                     tweets.tweet[i]=data.extended_tweet[i]["full_text"]
                     tweets.time[i] = data.created_at[i]
                    
                else:
                     tweets.tweet[i]=data.text[i] 
                     tweets.time[i] = data.created_at[i]
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    tweets.time[i] = data.created_at[i]
            else:
                 tweets.tweet[i] = data.retweeted_status[i]['text']
                 tweets.time[i] = data.created_at[i]
                    
       print( tweets.tweet[i])
       print('----------------\n')
240/41: tweets.duplicated
240/42: sum(tweets.duplicated)
240/43: len(tweets.duplicated)
240/44: tweets.duplicated
240/45: tweets.duplicated()
240/46: sum(tweets.duplicated())
240/47: tweets = tweets.sort_values('time', ascending=False)
240/48: sum(tweets.duplicated())
240/49: tweets=tweets.drop_duplicates()
240/50: data.retweeted_status[0]['text']
240/51: sum(tweets.duplicates())
240/52: sum(tweets.duplicated())
240/53: print('The time range tweet written is from', min(data.created_at),'to', max(data.created_at))
240/54: print('The time range tweet written is from', min(tweets.created_at),'to', max(tweets.created_at))
240/55: print('The time range tweet written is from', min(tweets.time),'to', max(tweets.time))
240/56:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
240/57:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
240/58: df.shape
240/59:
list_columns=list(df.columns)
list_columns
240/60: df.created_at[1:10]
240/61:
#keep English languages
data=df[df.lang=='en']
240/62: data.shape
240/63: data.truncated[21]
240/64:

data.text[21]
240/65: data.retweeted_status[31]
240/66: data.retweeted_status[21]['extended_tweet']['full_text']
240/67: data.text[31]
240/68: data.extended_tweet[31]
240/69: data.extended_tweet[31]['full_text']
240/70: data.created_at[0:3]
240/71: tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
240/72:
for i in data.index:   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweets.tweet[i] = data.text[i]
                    tweets.time[i] = data.created_at[i]
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                     tweets.tweet[i]=data.extended_tweet[i]["full_text"]
                     tweets.time[i] = data.created_at[i]
                    
                else:
                     tweets.tweet[i]=data.text[i] 
                     tweets.time[i] = data.created_at[i]
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    tweets.time[i] = data.created_at[i]
            else:
                 tweets.tweet[i] = data.retweeted_status[i]['text']
                 tweets.time[i] = data.created_at[i]
                    
       #print( tweets.tweet[i])
       #print('----------------\n')
240/73: tweets = tweets.sort_values('time', ascending=False)
240/74: tweets=tweets.drop_duplicates()
240/75: sum(tweets.duplicated())
240/76: print('The time range tweet written is from', min(tweets.time),'to', max(tweets.time))
240/77:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
240/78:
import pandas as pd
df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
240/79: df.shape
240/80:
list_columns=list(df.columns)
list_columns
240/81: df.created_at[1:10]
240/82:
#keep English languages
data=df[df.lang=='en']
240/83: data.shape
240/84: data.truncated[21]
240/85:

data.text[21]
240/86: data.retweeted_status[31]
240/87: data.retweeted_status[21]['extended_tweet']['full_text']
240/88: data.text[31]
240/89: data.extended_tweet[31]
240/90: data.extended_tweet[31]['full_text']
240/91: data.created_at[0:3]
240/92: tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
240/93:
for i in data.index:   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweets.tweet[i] = data.text[i]
                    tweets.time[i] = data.created_at[i]
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                     tweets.tweet[i]=data.extended_tweet[i]["full_text"]
                     tweets.time[i] = data.created_at[i]
                    
                else:
                     tweets.tweet[i]=data.text[i] 
                     tweets.time[i] = data.created_at[i]
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    tweets.time[i] = data.created_at[i]
            else:
                 tweets.tweet[i] = data.retweeted_status[i]['text']
                 tweets.time[i] = data.created_at[i]
                    
       #print( tweets.tweet[i])
       #print('----------------\n')
240/94: tweets = tweets.sort_values('time', ascending=False)
240/95: tweets=tweets.drop_duplicates()
240/96: sum(tweets.duplicated())
240/97: print('The time range tweets written is from', min(tweets.time),'to', max(tweets.time))
240/98: tweets.to_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5', key='df', mode='w')
240/99: tweets = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5')
240/100: tweets.head()
240/101: tweets.isnull().sum().sum()
240/102:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en #install it in the terminal

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
240/103:
wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I)
    doc = doc.lower()
    doc = doc.strip()
    # tokenize document
    tokens = wpt.tokenize(doc)
    # filter stopwords out of document
    filtered_tokens = [token for token in tokens if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(normalize_document)
240/104: df.text[2]
240/105: normalize_corpus(df.text)
240/106: import tweet-preprocessor
240/107: import tweet-preprocessor
240/108: from tweet-preprocessor import *
240/109: from tweet_preprocessor import *
240/110: from tweet-preprocessor import *
240/111: from twitter_preprocessor import TwitterPreprocessor
243/1: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/requirements_installer.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds')
240/112: from twitter_preprocessor import TwitterPreprocessor
240/113: import TwitterPreprocessor
240/114: from twitter_preprocessor import TwitterPreprocessor
240/115: from twitter_preprocessor import TwitterPreprocessor
240/116: from twitter_preprocessor import TwitterPreprocessor
240/117: import preprocessor as p
240/118: tweets.tweet[0]
240/119: tweets.tweet[1]
240/120: tweets.tweet[3]
240/121: tweets.tweet[10]
240/122: p.clean(tweets.tweet[10])
240/123: import tweet-preprocessor as p
240/124:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
240/125: tweets.tweet[10]
240/126:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
240/127:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
240/128: tweets.tweet[10]
240/129:
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text

sample = denoise_text(tweets.tweet[10])
print(sample)
240/130: tweets.tweet[11]
240/131:
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text

sample = denoise_text(tweets.tweet[11])
print(sample)
240/132:
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[10])
x
240/133:
from text_normalizer import normalize_corpus
x=normalize_corpus(df.text)
243/2:
import string

import nltk
from nltk.corpus import stopwords
from nltk import re
import regex, utils
from profanity_filter import ProfanityFilter
243/3:
import string

import nltk
from nltk.corpus import stopwords
from nltk import re
243/4: import regex, utils
243/5: from profanity_filter import ProfanityFilter
243/6: from profanity_filter import ProfanityFilter
243/7: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/twitter_preprocessor.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds')
243/8: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/twitter_preprocessor.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds')
240/134:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('Some @ptwist text to be preprocessed. It contains 2 sentences. Best text 2019!')
240/135: p
240/136:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('Some @ptwist text to be preprocessed. It contains 2 sentences. Best text 2019!')
240/137: p
240/138:
p = TwitterPreprocessor('RT @ptwist This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
240/139:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('Some @ptwist text to be preprocessed. It contains 2 sentences. Best text 2019!')

p.remove_mentions().remove_punctuation().remove_numbers(preserve_years=True).remove_blank_spaces()
print(p.text)
243/9: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/twitter_preprocessor.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds')
240/140:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('Some @ptwist text to be preprocessed. It contains 2 sentences. Best text 2019!')

p.remove_mentions().remove_punctuation().remove_numbers(preserve_years=True).remove_blank_spaces()
print(p.text)
240/141:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('Some @ptwist text to be preprocessed. It contains 2 sentences. Best text 2019!')

p.remove_punctuation().remove_numbers(preserve_years=True).remove_blank_spaces()
print(p.text)
240/142:
import re
x=re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=tweets.tweet[11])
240/143: x
240/144:
import re
def remove_url(text)
return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
240/145:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
240/146: remove_url(tweets.tweet[11])
240/147: remove_mentions(remove_url(tweets.tweet[11]))
240/148:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
240/149:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
240/150: remove_mentions(remove_url(tweets.tweet[11]))
240/151:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
240/152: remove_mentions(remove_url(tweets.tweet[11]))
240/153: remove_mentions(remove_url(tweets.tweet[11]))
240/154:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
240/155: remove_mentions(remove_url(tweets.tweet[11]))
240/156: tweets.tweet[12]
240/157: remove_mentions(remove_url(tweets.tweet[12]))
240/158:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@#\w*'), repl='', string=text)
240/159: remove_mentions(remove_url(tweets.tweet[12]))
240/160:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
240/161: remove_mentions(remove_url(tweets.tweet[12]))
240/162:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#\w*'),repl='',string=text)
240/163: remove_hashtags(remove_mentions(remove_url(tweets.tweet[12])))
240/164:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
240/165: remove_hashtags(remove_mentions(remove_url(tweets.tweet[12])))
240/166:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text)
    return text.translate(str.maketrans('', '', string.punctuation))
240/167:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
240/168: remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[12]))))
240/169:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl='',string=text)
240/170: remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[12]))))
240/171: tweets.tweet[13]
240/172: remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[13]))))
240/173: tweets.tweet[14]
240/174: remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[14]))))
240/175: tweets.tweet[15]
240/176: remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[145]))))
240/177: tweets.tweet[145]
240/178: tweets.tweet[150]
240/179: remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))
240/180:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl='',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl='',string=text)
240/181: remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))
240/182:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl='',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl='',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),,repl='',string=text)
240/183:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl='',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl='',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
240/184: remove_twitter_reserved_words(remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))
240/185: remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))
240/186: remove_twitter_reserved_words(remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150])))))
240/187:
remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))
240/188:
remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))
240/189:
remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))
240/190:
remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150])))))
240/191:
remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150])))))))
240/192:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl='',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl='',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        text = ' '.join(new_sentence)
        return text
240/193:
remove_stopwords(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150])))))))
240/194:
remove_stopwords(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))))
240/195:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl='',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl='',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(''+w)
        text = ' '.join(new_sentence)
        return text
240/196:
remove_stopwords(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))))
240/197:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl='',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl='',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
240/198:
remove_stopwords(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))))
240/199:
(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))))
240/200:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl='',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl='',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
240/201:
(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))))
240/202:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl='',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
240/203:
(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))))
240/204:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
240/205:
(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))))
240/206:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', ' ', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
240/207:
(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))))
240/208:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
240/209:
(remove_blank_spaces(remove_single_letter_words(remove_twitter_reserved_words
                           (remove_punctuation(remove_hashtags(remove_mentions(remove_url(tweets.tweet[150]))))))))
240/210: normalize_corpus(tweets.tweet[145])
240/211:
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text

sample = denoise_text(tweets.tweet[11])
print(sample)
240/212: tweets.tweet[11]
240/213:
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text

sample = denoise_text(tweets.tweet[12])
print(sample)
240/214: tweets.tweet[12]
243/10: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/twitter_preprocessor.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds')
243/11: runfile('/Users/shahla/Dropbox/MLCourse/SharpestMinds/twitter_preprocessor.py', wdir='/Users/shahla/Dropbox/MLCourse/SharpestMinds')
240/215:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
240/216:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
240/217:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
244/1:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
244/2:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ www.ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
244/3:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
244/4:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some 
                        Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
244/5:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
244/6:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor(tweets.tweet[12])

p.fully_preprocess()
print(p.text)
244/7: tweets = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5')
244/8:
from tweepy import Stream
import pandas as pd
244/9: tweets = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5')
244/10:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor(tweets.tweet[12])

p.fully_preprocess()
print(p.text)
244/11: tweets.tweet[12]
244/12: #normalize_corpus(tweets.tweet[145])
244/13:
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[12])
244/14:
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[12])
244/15:
import re
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[12])
245/1: tweets = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5')
245/2:
from tweepy import Stream
import pandas as pd
245/3: tweets = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5')
245/4:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[12])
246/1:
from tweepy import Stream
import pandas as pd
246/2: tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
246/3:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[12])
246/4: tweets = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5')
246/5: tweets = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5')
246/6:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[12])
246/7: x
246/8: type(x)
246/9: type(tweets.tweet[12])
246/10: x
246/11:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet)
246/12:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[1:10])
246/13: x
246/14:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[1:12])
246/15: x[12]
246/16: x[11]
246/17: x[0]
246/18: x[1]
246/19: x
246/20: tweets.tweet[1:12]
246/21: tweets.tweet[12]
246/22:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[1:13])
246/23: x
246/24:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[1:20])
246/25: tweets.tweet[1]
246/26:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[1:1])
246/27: x
246/28:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[0:1])
246/29: x
246/30: tweets.tweet[0:1]
246/31: tweets.tweet[12]
246/32:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[12:13])
246/33: x
246/34:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[11:12])
246/35: x
246/36:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[11:13])
246/37: x
246/38:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[11])
246/39: x
246/40:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[11:11])
246/41: x
246/42:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[11:12])
246/43: x
246/44: tweets.tweet[11:12]
246/45: tweets.tweet[11:12][0]
246/46: tweets.tweet[11:12][1]
246/47: tweets.tweet[11:12]
246/48: tweets.tweet[11:13]
246/49: print(tweets.tweet[11:12])
241/1:
from  empyrical import (
    aggregate_returns,
    alpha,
    alpha_aligned,
    alpha_beta,
    alpha_beta_aligned,
    annual_return,
    annual_volatility,
    beta,
    beta_aligned,
    cagr,
    calmar_ratio,
    capture,
    conditional_value_at_risk,
    cum_returns,
    cum_returns_final,
    down_alpha_beta,
    down_capture,
    downside_risk,
    excess_sharpe,
    max_drawdown,
    omega_ratio,
    roll_alpha,
    roll_alpha_aligned,
    roll_alpha_beta,
    roll_alpha_beta,
    roll_alpha_beta_aligned,
    roll_annual_volatility,
    roll_beta,
    roll_beta_aligned,
    roll_down_capture,
    roll_max_drawdown,
    roll_sharpe_ratio,
    roll_sortino_ratio,
    roll_up_capture,
    roll_up_down_capture,
    sharpe_ratio,
    simple_returns,
    sortino_ratio,
    stability_of_timeseries,
    tail_ratio,
    up_alpha_beta,
    up_capture,
    up_down_capture,
    value_at_risk,
)
241/2:   sharpe_ratio?
241/3: beta?
241/4: value_at_risk?
241/5:   sharpe_ratio?
241/6:   sharpe_ratio?
241/7: beta?
241/8:   simple_returns?
241/9: value_at_risk?
241/10:  roll_beta?
241/11:  up_down_capture?
241/12:  up_down_capture?
241/13: value_at_risk?
243/12: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
243/13: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
243/14: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/14: sharpe_ratio?
241/15: up_capture?
241/16:  up_alpha_beta?
241/17: tail_ratio?
243/15: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/18: stability_of_timeseries?
243/16: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/19: sortino_ratio?
243/17: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/20: 1/0
241/21: inf?
241/22:  simple_returns?
241/23:  roll_up_down_capture?
241/24: roll_up_capture?
241/25: roll_sortino_ratio?
241/26: roll_max_drawdown?
241/27:  roll_beta?
241/28: beta?
241/29: roll_annual_volatility?
241/30:  omega_ratio?
243/18: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/31:   max_drawdown?
243/19: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
243/20: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
243/21: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
243/22: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
243/23:
len(returns
)
241/32: excess_sharpe?
241/33: downside_risk?
243/24: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/34: down_capture?
241/35: down_alpha_beta?
241/36:  cum_returns_final?
243/25: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/37:  cum_returns?
243/26: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/38: conditional_value_at_risk?
243/27: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/39:  capture?
241/40:  calmar_ratio?
243/28: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/41: cagr?
243/29: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/42: beta_aligned?
241/43: beta?
241/44: annual_volatility?
243/30: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/45: annual_return?
243/31: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
241/46: alpha_beta_aligned?
241/47: alpha_beta?
241/48: alpha_aligned?
241/49: alpha?
241/50: aggregate_returns?
243/32: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
243/33: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
243/34: x=aggregate_returns(returns, "weekly")
243/35: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/evaluation.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
247/1: pd.read_excel('grades-completed', index_col=0)
247/2: import pandas as pd
247/3: pd.read_excel('grades-completed', index_col=0)
247/4: pd.read_excel('grades-completed.xlsx', index_col=0)
247/5: df=pd.read_excel('grades-completed.xlsx', index_col=0)
247/6: df.head()
247/7: df.columns
247/8: df.Max Score
247/9: import pandas as pd
247/10: df=pd.read_excel('grades-completed.xlsx', index_col=0)
247/11: df.head()
247/12: df.columns
247/13: df.MaxScore
247/14: df.MaxScore[0]
247/15: df.MaxScore
247/16: df.MaxScore[0]
247/17: df.MaxScore
247/18: df.MaxScore.dropna()
247/19: max=df.MaxScore.dropna()
247/20: max.mean()
247/21: max[max>89].mean()
247/22: max[max>89].sum()
247/23: len(max[max>89])
247/24: Aplus=max[max>89])
247/25: Aplus=max[max>89]
247/26: len(Aplus)
247/27: A=max[79<max<=89]
247/28: A=max[max<=89| max>79]
247/29: A=max[max<=89 or max>79]
247/30: A=max[(max<=89) | (max>79)]
247/31: A
247/32: final=df.MaxScore.dropna()
247/33: max.mean()
247/34: final.mean()
247/35: final=df.MaxScore.dropna()
247/36: final.mean()
247/37: Aplus=final[final>89]
247/38: len(Aplus)
247/39: A=final[(final<=89) | (final>79)]
247/40: A
247/41: A=final[(final<=89) & (final>79)]
247/42: A
247/43: Bplus=final[(final<=79) & (final>75)]
247/44: Aplus=final[final>=89]
247/45: len(Aplus)
247/46: Aplus=final[final>89]
247/47: A=final[(final<=89) & (final>79)]
247/48: Bplus=final[(final<=79) & (final>74)]
247/49: B=final[(final<=74) & (final>69)]
247/50: Cplus=final[(final<=69) & (final>64)]
247/51: C=final[(final<=60) & (final>59)]
247/52: Dplus=final[(final<=59) & (final>54)]
247/53: D=final[(final<=54) & (final>49)]
247/54: E=final[(final<=49) & (final>40)]
247/55: F=final[(final<=40)]
247/56: df=pd.read_excel('grades-completed.xlsx', index_col=0)
247/57: df.head()
247/58: df.columns
247/59: final=df.MaxScore.dropna()
247/60: final.mean()
247/61: Aplus=final[final>89]
247/62: A=final[(final<=89) & (final>79)]
247/63: Bplus=final[(final<=79) & (final>74)]
247/64: B=final[(final<=74) & (final>69)]
247/65: Cplus=final[(final<=69) & (final>64)]
247/66: C=final[(final<=64) & (final>59)]
247/67: Dplus=final[(final<=59) & (final>54)]
247/68: D=final[(final<=54) & (final>47)]
247/69: E=final[(final<=47) & (final>40)]
247/70: F=final[(final<=40)]
247/71: AplusPer=(len(Aplus)/len(final))*100
247/72:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/73:
APer=(len(A)/len(final))*100
APer
247/74:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/75:
BPer=(len(B)/len(final))*100
BPer
247/76:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/77:
CPer=(len(C)/len(final))*100
CPer
247/78:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/79:
DPer=(len(D)/len(final))*100
DPer
247/80:
EPer=(len(E)/len(final))*100
EPer
247/81:
FPer=(len(F)/len(final))*100
FPer
247/82: F
247/83: E
247/84: len(final)
247/85: D=final[(final<=54) & (final>44)]
247/86: E=final[(final<=44) & (final>40)]
247/87: F=final[(final<=40)]
247/88:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/89:
APer=(len(A)/len(final))*100
APer
247/90:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/91:
BPer=(len(B)/len(final))*100
BPer
247/92:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/93:
CPer=(len(C)/len(final))*100
CPer
247/94:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/95:
DPer=(len(D)/len(final))*100
DPer
247/96:
EPer=(len(E)/len(final))*100
EPer
247/97:
FPer=(len(F)/len(final))*100
FPer
247/98: F
247/99: E
247/100: len(final)
247/101: E=final[(final<=44) & (final>37)]
247/102: F=final[(final<=37)]
247/103:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/104:
APer=(len(A)/len(final))*100
APer
247/105:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/106:
BPer=(len(B)/len(final))*100
BPer
247/107:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/108:
CPer=(len(C)/len(final))*100
CPer
247/109:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/110:
DPer=(len(D)/len(final))*100
DPer
247/111:
EPer=(len(E)/len(final))*100
EPer
247/112:
FPer=(len(F)/len(final))*100
FPer
247/113: F
247/114: E
247/115:
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
objects={'A+','A','B+','B','C+','C','D+','D','E','F'}
y_pos = np.arange(len(objects))
performance = [AplusPer, APer,BplusPer, BPer, CplusPer,
               CPer,DplusPer,DPer, E, F]
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('percentage')
plt.title('marks')
 
plt.show()
247/116:
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
objects={'A+','A','B+','B','C+','C','D+','D','E','F'}
y_pos = np.arange(len(objects))
performance = [AplusPer, APer,BplusPer, BPer, CplusPer,
               CPer,DplusPer,DPer, E, F]
247/117:
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('percentage')
plt.title('marks')
 
plt.show()
247/118:
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
 
objects = ('Python', 'C++', 'Java', 'Perl', 'Scala', 'Lisp')
y_pos = np.arange(len(objects))
performance = [10,8,6,4,2,1]
 
plt.barh(y_pos, performance, align='center', alpha=0.5)
plt.yticks(y_pos, objects)
plt.xlabel('Usage')
plt.title('Programming language usage')
 
plt.show()
247/119:
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
objects=('A+','A','B+','B','C+','C','D+','D','E','F')
y_pos = np.arange(len(objects))
performance = [AplusPer, APer,BplusPer, BPer, CplusPer,
               CPer,DplusPer,DPer, E, F]
247/120:
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('percentage')
plt.title('marks')
 
plt.show()
247/121:
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
 
objects =('A+','A','B+','B','C+','C')
y_pos = np.arange(len(objects))
performance = [10,8,6,4,2,1]
 
plt.barh(y_pos, performance, align='center', alpha=0.5)
plt.yticks(y_pos, objects)
plt.xlabel('Usage')
plt.title('Programming language usage')
 
plt.show()
247/122:
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
objects=('A+','A','B+','B','C+','C','D+','D','E','F')
y_pos = np.arange(len(objects))
# performance = [AplusPer, APer,BplusPer, BPer, CplusPer,
#                CPer,DplusPer,DPer, E, F]
performance =[1,3,4,4,4,4,5,6,7,3]
247/123:
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('percentage')
plt.title('marks')
 
plt.show()
247/124:
len(final)
from math import ceil
ceil(1)
247/125:
len(final)
from math import ceil
ceil(1.3)
247/126:
len(final)
from math import ceil
ceil(1.5)
247/127:
len(final)
from math import ceil
ceil(1.6)
247/128:
len(final)
from math import floor
floor(1.6)
247/129:
len(final)
from math import floor
floor([AplusPer, APer,BplusPer, BPer, CplusPer,
#                CPer,DplusPer,DPer, E, F])
247/130:
len(final)
from math import floor
floor([AplusPer, APer,BplusPer, BPer, CplusPer,
               CPer,DplusPer,DPer, E, F])
247/131:
len(final)
from math import floor
ls =[AplusPer, APer,BplusPer, BPer, CplusPer,
               CPer,DplusPer,DPer, E, F]
performance = [floor(item) for item in ls]
247/132: type(APer)
247/133: APer.info()
247/134:
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
objects=('A+','A','B+','B','C+','C','D+','D','E','F')
y_pos = np.arange(len(objects))
performance = [AplusPer, APer,BplusPer, BPer, CplusPer,
               CPer,DplusPer,DPer, EPer, FPer]
247/135:
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('percentage')
plt.title('marks')
 
plt.show()
247/136: ddf=df.dropna()
247/137: final=df.MaxScore
247/138: final.mean()
247/139: Aplus=final[final>89]
247/140: A=final[(final<=89) & (final>79)]
247/141: Bplus=final[(final<=79) & (final>74)]
247/142: B=final[(final<=74) & (final>69)]
247/143: Cplus=final[(final<=69) & (final>64)]
247/144: C=final[(final<=64) & (final>59)]
247/145: Dplus=final[(final<=59) & (final>54)]
247/146: D=final[(final<=54) & (final>44)]
247/147: E=final[(final<=44) & (final>37)]
247/148: F=final[(final<=37)]
247/149:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/150:
APer=(len(A)/len(final))*100
APer
247/151:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/152:
BPer=(len(B)/len(final))*100
BPer
247/153:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/154:
CPer=(len(C)/len(final))*100
CPer
247/155:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/156:
DPer=(len(D)/len(final))*100
DPer
247/157:
EPer=(len(E)/len(final))*100
EPer
247/158:
FPer=(len(F)/len(final))*100
FPer
247/159: F
247/160: E
247/161: APer
247/162:
len(final)
from math import floor
ls =[AplusPer, APer,BplusPer, BPer, CplusPer,
               CPer,DplusPer,DPer, E, F]
performance = [floor(item) for item in ls]
247/163:
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
objects=('A+','A','B+','B','C+','C','D+','D','E','F')
y_pos = np.arange(len(objects))
performance = [AplusPer, APer,BplusPer, BPer, CplusPer,
               CPer,DplusPer,DPer, EPer, FPer]
247/164:
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('percentage')
plt.title('marks')
 
plt.show()
247/165: ddf['marks']='A+' if final>89
247/166: ddf['marks']=['A+' if final>89 else 'felan']
247/167: ddf=df.dropna()
247/168: final=ddf.MaxScore
247/169: final.mean()
247/170: Aplus=final[final>89]
247/171: A=final[(final<=89) & (final>79)]
247/172: Bplus=final[(final<=79) & (final>74)]
247/173: B=final[(final<=74) & (final>69)]
247/174: Cplus=final[(final<=69) & (final>64)]
247/175: C=final[(final<=64) & (final>59)]
247/176: Dplus=final[(final<=59) & (final>54)]
247/177: D=final[(final<=54) & (final>44)]
247/178: E=final[(final<=44) & (final>37)]
247/179: F=final[(final<=37)]
247/180:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/181:
APer=(len(A)/len(final))*100
APer
247/182:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/183:
BPer=(len(B)/len(final))*100
BPer
247/184:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/185:
CPer=(len(C)/len(final))*100
CPer
247/186:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/187:
DPer=(len(D)/len(final))*100
DPer
247/188:
EPer=(len(E)/len(final))*100
EPer
247/189:
FPer=(len(F)/len(final))*100
FPer
247/190: F
247/191: E
247/192: APer
247/193:
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
objects=('A+','A','B+','B','C+','C','D+','D','E','F')
y_pos = np.arange(len(objects))
performance = [AplusPer, APer,BplusPer, BPer, CplusPer,
               CPer,DplusPer,DPer, EPer, FPer]
247/194:
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('percentage')
plt.title('marks')
 
plt.show()
247/195: ddf['marks']=['A+' if final>89 else 'felan']
247/196: D=final[(final<=54) & (final>49)]
247/197: E=final[(final<=49) & (final>37)]
247/198: F=final[(final<=37)]
247/199:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/200:
APer=(len(A)/len(final))*100
APer
247/201:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/202:
BPer=(len(B)/len(final))*100
BPer
247/203:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/204:
CPer=(len(C)/len(final))*100
CPer
247/205:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/206:
DPer=(len(D)/len(final))*100
DPer
247/207:
EPer=(len(E)/len(final))*100
EPer
247/208:
FPer=(len(F)/len(final))*100
FPer
247/209:
EPer=(len(E)/len(final))*100
EPer
247/210:
FPer=(len(F)/len(final))*100
FPer
247/211: D=final[(final<=54) & (final>48)]
247/212: E=final[(final<=49) & (final>37)]
247/213: F=final[(final<=37)]
247/214:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/215:
APer=(len(A)/len(final))*100
APer
247/216:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/217:
BPer=(len(B)/len(final))*100
BPer
247/218:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/219:
CPer=(len(C)/len(final))*100
CPer
247/220:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/221:
DPer=(len(D)/len(final))*100
DPer
247/222:
EPer=(len(E)/len(final))*100
EPer
247/223:
FPer=(len(F)/len(final))*100
FPer
247/224: D=final[(final<=54) & (final>49)]
247/225: E=final[(final<=49) & (final>37)]
247/226: F=final[(final<=37)]
247/227:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/228:
APer=(len(A)/len(final))*100
APer
247/229:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/230:
BPer=(len(B)/len(final))*100
BPer
247/231:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/232:
CPer=(len(C)/len(final))*100
CPer
247/233:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/234:
DPer=(len(D)/len(final))*100
DPer
247/235:
EPer=(len(E)/len(final))*100
EPer
247/236:
FPer=(len(F)/len(final))*100
FPer
247/237: ddf['marks']=['A+' if ddf.MaxScore>89 else 'felan']
247/238: ddf['marks']=  (ddf.MaxScore>89)
247/239: ddf['marks']
247/240: ap= (ddf.MaxScore>89)
247/241: ddf['marks']='A+' if ap
247/242: ddf['marks']='A+'
247/243: ddf['marks']
247/244:
df.loc[(df['Set']=="Z"), 'Color'] = "green"
ddf['marks']=ddf.loc[(ddf.MaxScore<=89) & (ddf.MaxScore>79),'B+']
247/245:
df.loc[(df['Set']=="Z"), 'Color'] = "green"
ddf.loc[(ddf.MaxScore<=89) & (ddf.MaxScore>79),'marks']='B+'
247/246:
#df.loc[(df['Set']=="Z"), 'Color'] = "green"
ddf.loc[(ddf.MaxScore<=89) & (ddf.MaxScore>79),'marks']='B+'
247/247: ddf.marks
247/248: import pandas as pd
247/249: df=pd.read_excel('grades-completed.xlsx', index_col=0)
247/250: df.head()
247/251: df.columns
247/252: ddf=df.dropna()
247/253: final=ddf.MaxScore
247/254: ddf=df.dropna()
247/255: final=ddf.MaxScore
247/256: df=pd.read_excel('grades-completed.xlsx', index_col=0)
247/257: df.head()
247/258: df.columns
247/259: ddf=df.dropna()
247/260: final=ddf.MaxScore
247/261: final.mean()
247/262: Aplus=final[final>89]
247/263: A=final[(final<=89) & (final>79)]
247/264: Bplus=final[(final<=79) & (final>74)]
247/265: B=final[(final<=74) & (final>69)]
247/266: Cplus=final[(final<=69) & (final>64)]
247/267: C=final[(final<=64) & (final>59)]
247/268: Dplus=final[(final<=59) & (final>54)]
247/269: D=final[(final<=54) & (final>49)]
247/270: E=final[(final<=49) & (final>37)]
247/271: F=final[(final<=37)]
247/272:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/273:
APer=(len(A)/len(final))*100
APer
247/274:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/275:
BPer=(len(B)/len(final))*100
BPer
247/276:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/277:
CPer=(len(C)/len(final))*100
CPer
247/278:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/279:
DPer=(len(D)/len(final))*100
DPer
247/280:
EPer=(len(E)/len(final))*100
EPer
247/281:
FPer=(len(F)/len(final))*100
FPer
247/282:
EPer=(len(E)/len(final))*100
EPer
247/283:
FPer=(len(F)/len(final))*100
FPer
247/284: F
247/285: ddf.columns
247/286: ddf
247/287: ddf[1]
247/288: ddf[[1]
247/289: ddf[[1]]
247/290: ddf.columns[7]
247/291: ddf.columns[6]
247/292: ddf[ddf.columns[6]]
247/293: ddf[ddf.columns[6]][205]
247/294: ddf[ddf.columns[6]](205)
247/295: ddf[ddf.columns[6]].loc(2)
247/296: ddf[ddf.columns[6]]
247/297: ddf[ddf.columns[6,7]]
247/298: ddf[ddf.columns[6,7]]
247/299: ddf[ddf.columns[6]]
247/300:
ddf[ddf.columns[6]]
ddf['Denis']
247/301:
ddf[ddf.columns[6]]
ddf.Surname=='Denis'
247/302:
ddf[ddf.columns[6]]
ddf[ddf.Surname=='Denis']
247/303:
ddf[ddf.columns[6]]
ddf[ddf.Surname=='Deaconu']
247/304: import pandas as pd
247/305: df=pd.read_excel('grades-completed.xlsx', index_col=0)
247/306: df.head()
247/307: df.columns
247/308: ddf=df.dropna()
247/309: final=ddf.MaxScore
247/310: final.mean()
247/311: Aplus=final[final>89]
247/312: A=final[(final<=89) & (final>79)]
247/313: Bplus=final[(final<=79) & (final>74)]
247/314: B=final[(final<=74) & (final>69)]
247/315: Cplus=final[(final<=69) & (final>64)]
247/316: C=final[(final<=64) & (final>59)]
247/317: Dplus=final[(final<=59) & (final>54)]
247/318: D=final[(final<=54) & (final>49)]
247/319: E=final[(final<=49) & (final>40)]
247/320: F=final[(final<=37)]
247/321:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
247/322:
APer=(len(A)/len(final))*100
APer
247/323:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
247/324:
BPer=(len(B)/len(final))*100
BPer
247/325:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
247/326:
CPer=(len(C)/len(final))*100
CPer
247/327:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
247/328:
DPer=(len(D)/len(final))*100
DPer
247/329:
EPer=(len(E)/len(final))*100
EPer
247/330:
FPer=(len(F)/len(final))*100
FPer
248/1: import pandas as pd
248/2: df=pd.read_excel('grades-completed.xlsx', index_col=0)
248/3: df.head()
248/4: df.columns
248/5: ddf=df.dropna()
248/6: final=ddf.MaxScore
248/7: final.mean()
248/8: Aplus=final[final>89]
248/9: A=final[(final<=89) & (final>79)]
248/10: Bplus=final[(final<=79) & (final>74)]
248/11: B=final[(final<=74) & (final>69)]
248/12: Cplus=final[(final<=69) & (final>64)]
248/13: C=final[(final<=64) & (final>59)]
248/14: Dplus=final[(final<=59) & (final>54)]
248/15: D=final[(final<=54) & (final>49)]
248/16: E=final[(final<=49) & (final>40)]
248/17: F=final[(final<=37)]
248/18:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
248/19:
APer=(len(A)/len(final))*100
APer
248/20:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
248/21:
BPer=(len(B)/len(final))*100
BPer
248/22:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
248/23:
CPer=(len(C)/len(final))*100
CPer
248/24:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
248/25:
DPer=(len(D)/len(final))*100
DPer
248/26:
EPer=(len(E)/len(final))*100
EPer
248/27:
FPer=(len(F)/len(final))*100
FPer
249/1: final[(final<=49) & (final>45)]=final[(final<=49) & (final>45)]+5
249/2: import pandas as pd
249/3: df=pd.read_excel('grades-completed.xlsx', index_col=0)
249/4: df.head()
249/5: df.columns
249/6: ddf=df.dropna()
249/7: final=ddf.MaxScore
249/8: final.mean()
249/9: Aplus=final[final>89]
249/10: A=final[(final<=89) & (final>79)]
249/11: Bplus=final[(final<=79) & (final>74)]
249/12: B=final[(final<=74) & (final>69)]
249/13: Cplus=final[(final<=69) & (final>64)]
249/14: C=final[(final<=64) & (final>59)]
249/15: Dplus=final[(final<=59) & (final>54)]
249/16: D=final[(final<=54) & (final>49)]
249/17: E=final[(final<=49) & (final>40)]
249/18: F=final[(final<=37)]
249/19: final[(final<=49) & (final>45)]=final[(final<=49) & (final>45)]+5
249/20: F=final[(final<=37)]
249/21:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
249/22:
APer=(len(A)/len(final))*100
APer
249/23:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
249/24:
BPer=(len(B)/len(final))*100
BPer
249/25:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
249/26:
CPer=(len(C)/len(final))*100
CPer
249/27:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
249/28:
DPer=(len(D)/len(final))*100
DPer
249/29:
EPer=(len(E)/len(final))*100
EPer
249/30:
FPer=(len(F)/len(final))*100
FPer
249/31: Aplus=final[final>89]
249/32: A=final[(final<=89) & (final>79)]
249/33: Bplus=final[(final<=79) & (final>74)]
249/34: B=final[(final<=74) & (final>69)]
249/35: Cplus=final[(final<=69) & (final>64)]
249/36: C=final[(final<=64) & (final>59)]
249/37: Dplus=final[(final<=59) & (final>54)]
249/38: D=final[(final<=54) & (final>49)]
249/39: E=final[(final<=49) & (final>40)]
249/40: F=final[(final<=37)]
249/41:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
249/42:
APer=(len(A)/len(final))*100
APer
249/43:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
249/44:
BPer=(len(B)/len(final))*100
BPer
249/45:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
249/46:
CPer=(len(C)/len(final))*100
CPer
249/47:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
249/48:
DPer=(len(D)/len(final))*100
DPer
249/49:
EPer=(len(E)/len(final))*100
EPer
249/50:
FPer=(len(F)/len(final))*100
FPer
249/51: final.mean()
249/52: final[(final<=50) & (final>49)]=final[(final<=50) & (final>49)]+5
249/53: Aplus=final[final>89]
249/54: A=final[(final<=89) & (final>79)]
249/55: Bplus=final[(final<=79) & (final>74)]
249/56: B=final[(final<=74) & (final>69)]
249/57: Cplus=final[(final<=69) & (final>64)]
249/58: C=final[(final<=64) & (final>59)]
249/59: Dplus=final[(final<=59) & (final>54)]
249/60: D=final[(final<=54) & (final>49)]
249/61: E=final[(final<=49) & (final>40)]
249/62: F=final[(final<=37)]
249/63:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
249/64:
APer=(len(A)/len(final))*100
APer
249/65:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
249/66:
BPer=(len(B)/len(final))*100
BPer
249/67:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
249/68:
CPer=(len(C)/len(final))*100
CPer
249/69:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
249/70:
DPer=(len(D)/len(final))*100
DPer
249/71:
EPer=(len(E)/len(final))*100
EPer
249/72:
FPer=(len(F)/len(final))*100
FPer
249/73: final.mean()
250/1: import pandas as pd
250/2: df=pd.read_excel('grades-completed.xlsx', index_col=0)
250/3: df.head()
250/4: df.columns
250/5:
df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]=df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]+5
df.MaxScore[(df.MaxScore<=50) & (df.MaxScore>49)]=df.MaxScore[(df.MaxScore<=50) & (df.MaxScore>49)]+5
250/6: final=df.MaxScore
250/7: final.mean()
251/1: import pandas as pd
251/2: df=pd.read_excel('grades-completed.xlsx', index_col=0)
251/3: df.head()
251/4: df.columns
251/5:
df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]=df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]+5
df.MaxScore[(df.MaxScore<=50) & (df.MaxScore>49)]=df.MaxScore[(df.MaxScore<=50) & (df.MaxScore>49)]+5
251/6: #ddf=df.dropna()
251/7: final=df.MaxScore
251/8: final.mean()
251/9: Aplus=final[final>89]
251/10: A=final[(final<=89) & (final>79)]
251/11: Bplus=final[(final<=79) & (final>74)]
251/12: B=final[(final<=74) & (final>69)]
251/13: Cplus=final[(final<=69) & (final>64)]
251/14: C=final[(final<=64) & (final>59)]
251/15: Dplus=final[(final<=59) & (final>54)]
251/16: D=final[(final<=54) & (final>49)]
251/17: E=final[(final<=49) & (final>40)]
251/18: F=final[(final<=37)]
251/19:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
251/20: ddf=df.dropna()
251/21: final=ddf.MaxScore
251/22: final.mean()
251/23: Aplus=final[final>89]
251/24: A=final[(final<=89) & (final>79)]
251/25: Bplus=final[(final<=79) & (final>74)]
251/26: B=final[(final<=74) & (final>69)]
251/27: Cplus=final[(final<=69) & (final>64)]
251/28: C=final[(final<=64) & (final>59)]
251/29: Dplus=final[(final<=59) & (final>54)]
251/30: D=final[(final<=54) & (final>49)]
251/31: E=final[(final<=49) & (final>40)]
251/32: F=final[(final<=37)]
251/33:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
251/34:
APer=(len(A)/len(final))*100
APer
251/35:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
251/36:
BPer=(len(B)/len(final))*100
BPer
251/37:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
251/38:
CPer=(len(C)/len(final))*100
CPer
251/39:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
251/40:
DPer=(len(D)/len(final))*100
DPer
251/41:
EPer=(len(E)/len(final))*100
EPer
251/42:
FPer=(len(F)/len(final))*100
FPer
251/43:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/44: df.loc[0:37]
251/45: df.loc[0:37,[1]]
251/46: df.loc[0,[1]]
251/47: df.iloc[0,[1]]
251/48: df.iloc[0:37,[1]]
251/49: df.iloc[0:38,[1]]
251/50: df.iloc[0:36,[1]]
251/51: df.iloc[0:36,[0:]
251/52: len(list(df.columns))
251/53: df.iloc[0:36,[0:21]
251/54: df.iloc[0:36,[0:20]
251/55: df.iloc[0:36][0:20]
251/56: df.iloc[36][0:20]
251/57: df.iloc[0][0:20]
251/58: df.iloc[37][0:20]
251/59: df.iloc[36][0:20]
251/60: df.iloc[35][0:20]
251/61: df=pd.read_excel('grades-completed.xlsx', index_col=0)
251/62: df.iloc[35][0:20]
251/63: cols=list(df.columns)
251/64:
cols=list(df.columns)
cols
251/65:
cols=cols.remove('Surname',
 'ID number',
 'Email address')
251/66:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
251/67:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
cols
251/68: df=pd.read_excel('g.xlsx', index_col=0)
251/69: df.iloc[35][0:20]
251/70: df.head()
251/71: df=pd.read_excel('g.xlsx', index_col=0)
251/72:
for col in df:
    print df[col].value_counts(dropna=False)
251/73:
for col in df:
    print(df[col].value_counts(dropna=False))
251/74:
for col in df:
    s=df[col]
    print(col,s.isna().sum())
251/75:
for col in df:
    df[col]
    #print(col,s.isna().sum())
251/76:
for col in df:
    print(df[col])
    #print(col,s.isna().sum())
251/77: df.iloc[35][0:20]
251/78: df.iloc[:][0:20]
251/79: df.iloc[:][0:40]
251/80:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/81: df=pd.read_excel('g.xlsx', index_col=0)
251/82: df.iloc[:][0:40]
251/83: df.iloc[39][0:40]
251/84: df.iloc[40][0:40]
251/85: df.iloc[40][0:40]
251/86: df.iloc[40][0:40]
251/87: df.iloc[40][-1:40]
251/88: df.iloc[40][1:40]
251/89: df.iloc[40][0:40]
251/90: df.iloc[40][0:41]
251/91:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/92: df=pd.read_excel('g.xlsx', index_col=0)
251/93: df.iloc[40][0:41]
251/94: df.iloc[40][0:38]
251/95: df.iloc[40][0:39]
251/96: df.iloc[40][0:40]
251/97: df.iloc[36][0:40]
251/98: df.iloc[38][0:40]
251/99: df.iloc[39][0:40]
251/100: df.iloc[37][0:40]
251/101: df.iloc[38][0:40]
251/102:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/103: df=pd.read_excel('g.xlsx', index_col=0)
251/104: df.iloc[38][0:40]
251/105: ddf=df.dropna()
251/106: df.shape
251/107: ddf.shape()
251/108: ddf.shape
251/109: final=ddf.MaxScore
251/110: final.mean()
251/111:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/112: df=pd.read_excel('g.xlsx', index_col=0)
251/113: #df.iloc[38][0:40]
251/114: df.head()
251/115:
cols=list(df.columns)
cols
251/116:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
cols
251/117: ddf=df.dropna()
251/118: df.shape
251/119: ddf.shape
251/120: final=ddf.MaxScore
251/121: final.mean()
251/122: df.MaxScore.mean()
251/123:
df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]=df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]+5
df.MaxScore[(df.MaxScore<=50) & (df.MaxScore>49)]=df.MaxScore[(df.MaxScore<=50) & (df.MaxScore>49)]+5
df.MaxScore[(df.MaxScore<=64) & (df.MaxScore>59)]=df.MaxScore[(df.MaxScore<=64) & (df.MaxScore>59)]+5
251/124:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/125: df=pd.read_excel('g.xlsx', index_col=0)
251/126: #df.iloc[38][0:40]
251/127: df.head()
251/128:
cols=list(df.columns)
cols
251/129:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
cols
251/130:
df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]=df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]+5
df.MaxScore[(df.MaxScore<=55) & (df.MaxScore>49)]=df.MaxScore[(df.MaxScore<=50) & (df.MaxScore>49)]+5
df.MaxScore[(df.MaxScore<=64) & (df.MaxScore>59)]=df.MaxScore[(df.MaxScore<=64) & (df.MaxScore>59)]+5
251/131:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/132: df=pd.read_excel('g.xlsx', index_col=0)
251/133: #df.iloc[38][0:40]
251/134: df.head()
251/135:
cols=list(df.columns)
cols
251/136:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
cols
251/137:
df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]=df.MaxScore[(df.MaxScore<=49) & (df.MaxScore>45)]+5
df.MaxScore[(df.MaxScore<=55) & (df.MaxScore>49)]=df.MaxScore[(df.MaxScore<=55) & (df.MaxScore>49)]+5
df.MaxScore[(df.MaxScore<=64) & (df.MaxScore>59)]=df.MaxScore[(df.MaxScore<=64) & (df.MaxScore>59)]+5
251/138: final=df.MaxScore
251/139: final.mean()
251/140:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/141: df=pd.read_excel('g.xlsx', index_col=0)
251/142: #df.iloc[38][0:40]
251/143: df.head()
251/144:
cols=list(df.columns)
cols
251/145:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
cols
251/146:

df.MaxScore[(df.MaxScore<=70) & (df.MaxScore>45)]=df.MaxScore[(df.MaxScore<=70) & (df.MaxScore>45)]+5
251/147: final=df.MaxScore
251/148: final.mean()
251/149:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/150: df=pd.read_excel('g.xlsx', index_col=0)
251/151: #df.iloc[38][0:40]
251/152: df.head()
251/153:
cols=list(df.columns)
cols
251/154:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
cols
251/155:

df.MaxScore[(df.MaxScore<=80) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=80) & (df.MaxScore>40)]+5
251/156: df.shape
251/157: ddf=df.dropna()
251/158: ddf.shape
251/159: final=df.MaxScore
251/160: final.mean()
251/161:

df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>36)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>36)]+5
251/162: ddf=df.dropna()
251/163: df.shape
251/164: ddf.shape
251/165: final=df.MaxScore
251/166: final.mean()
251/167:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/168: df=pd.read_excel('g.xlsx', index_col=0)
251/169: #df.iloc[38][0:40]
251/170: df.head()
251/171:
cols=list(df.columns)
cols
251/172:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
cols
251/173:

df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+5
251/174: ddf=df.dropna()
251/175: df.shape
251/176: ddf.shape
251/177: final=df.MaxScore
251/178: final.mean()
251/179:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/180: df=pd.read_excel('g.xlsx', index_col=0)
251/181: #df.iloc[38][0:40]
251/182: df.head()
251/183:
cols=list(df.columns)
cols
251/184:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
cols
251/185:

df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
251/186: ddf=df.dropna()
251/187: df.shape
251/188: ddf.shape
251/189: final=df.MaxScore
251/190: final.mean()
251/191: df.MaxScore.mean()
251/192: Aplus=final[final>89]
251/193: A=final[(final<=89) & (final>79)]
251/194: Bplus=final[(final<=79) & (final>74)]
251/195: B=final[(final<=74) & (final>69)]
251/196: Cplus=final[(final<=69) & (final>64)]
251/197: C=final[(final<=64) & (final>59)]
251/198: Dplus=final[(final<=59) & (final>54)]
251/199: D=final[(final<=54) & (final>49)]
251/200: E=final[(final<=49) & (final>40)]
251/201: F=final[(final<=37)]
251/202:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
251/203:
APer=(len(A)/len(final))*100
APer
251/204:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
251/205:
BPer=(len(B)/len(final))*100
BPer
251/206:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
251/207:
CPer=(len(C)/len(final))*100
CPer
251/208:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
251/209:
DPer=(len(D)/len(final))*100
DPer
251/210:
EPer=(len(E)/len(final))*100
EPer
251/211:
FPer=(len(F)/len(final))*100
FPer
251/212: submit
251/213:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
df.MaxScore[(df.MaxScore<=75) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=75) & (df.MaxScore>40)]+7
251/214: ddf=df.dropna()
251/215:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/216: df=pd.read_excel('g.xlsx', index_col=0)
251/217: #df.iloc[38][0:40]
251/218: df.head()
251/219:
cols=list(df.columns)
cols
251/220:
cols = [e for e in cols if e not in ('Surname',
 'ID number',
 'Email address')]
cols
251/221:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
df.MaxScore[(df.MaxScore<=75) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=75) & (df.MaxScore>40)]+7
251/222: ddf=df.dropna()
251/223: df.shape
251/224: ddf.shape
251/225: final=df.MaxScore
251/226: final.mean()
251/227: df.MaxScore.mean()
251/228: Aplus=final[final>89]
251/229: A=final[(final<=89) & (final>79)]
251/230: Bplus=final[(final<=79) & (final>74)]
251/231: B=final[(final<=74) & (final>69)]
251/232: Cplus=final[(final<=69) & (final>64)]
251/233: C=final[(final<=64) & (final>59)]
251/234: Dplus=final[(final<=59) & (final>54)]
251/235: D=final[(final<=54) & (final>49)]
251/236: E=final[(final<=49) & (final>40)]
251/237: F=final[(final<=37)]
251/238:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
251/239:
APer=(len(A)/len(final))*100
APer
251/240:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
251/241:
BPer=(len(B)/len(final))*100
BPer
251/242:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
251/243:
CPer=(len(C)/len(final))*100
CPer
251/244:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
251/245:
DPer=(len(D)/len(final))*100
DPer
251/246:
EPer=(len(E)/len(final))*100
EPer
251/247:
FPer=(len(F)/len(final))*100
FPer
251/248: sub=df.copy()
251/249:
sub.drop(['Surname',
 'ID number',
 'Email address',
 'Test1 (Real)',
 'Test2 (Real)',
 'Test3 (Real)',
 'Final (Real)',
 'MaxScore'],1)
251/250:
data=sub.drop(['Surname',
 'ID number',
 'Email address',
 'Test1 (Real)',
 'Test2 (Real)',
 'Test3 (Real)',
 'Final (Real)',
 'MaxScore'],1)
251/251:
data=sub.drop(['Surname',
 'ID number',
 'Email address',
 'Test1 (Real)',
 'Test2 (Real)',
 'Test3 (Real)',
 'Final (Real)',
 'MaxScore'],1)
data
251/252:
data=sub.drop([
 'Course total (Real)',
 'Scheme 1',
 'Scheme 2',
 'Scheme 3',
 'Scheme 4',
 'Scheme 5',
 'Scheme 6',
 'Scheme 7',
 'Scheme 8'],1)
251/253:
data=sub.drop([
 'Course total (Real)',
 'Scheme 1',
 'Scheme 2',
 'Scheme 3',
 'Scheme 4',
 'Scheme 5',
 'Scheme 6',
 'Scheme 7',
 'Scheme 8'],1)
data
251/254:
data=sub.drop([
 'Course total (Real)',
 'Scheme 1',
 'Scheme 2',
 'Scheme 3',
 'Scheme 4',
 'Scheme 5',
 'Scheme 6',
 'Scheme 7',
 'Scheme 8'],1)
list(data.columns)
251/255:
data.rename({'Surname',
 'ID number',
 'Email address',
 'Test1 (Real)':'Test1',
 'Test2 (Real)':'Test2',
 'Test3 (Real)':"Test3",
 'Final (Real)':'Final',
 'MaxScore':'Maximum'},inplace=True)
251/256:
data.rename(columns={'Surname',
 'ID number',
 'Email address',
 'Test1 (Real)':'Test1',
 'Test2 (Real)':'Test2',
 'Test3 (Real)':"Test3",
 'Final (Real)':'Final',
 'MaxScore':'Maximum'},inplace=True)
251/257:
data.columns=['Surname',
 'ID number',
 'Email address','Test1','Test2',"Test3",
 'Final',
 'MaxScore']
251/258:
data.columns=['Surname',
 'ID number',
 'Email address','Test1','Test2',"Test3",
 'Final',
 'MaxScore']
data
251/259:
data.columns=['Surname',
 'ID number',
 'Email address','Test1','Test2',"Test3",
 'Final',
 'MaxScore']
data.MaxScore=max(data.MaxScore,data.Final)
251/260: data
251/261:
data.columns=['Surname',
 'ID number',
 'Email address','Test1','Test2',"Test3",
 'Final',
 'MaxScore']
df[["A", "B"]].max(axis=1)
data.MaxScore=data[['MaxScore','Final']].max(axis=1)
251/262:
data.columns=['Surname',
 'ID number',
 'Email address','Test1','Test2',"Test3",
 'Final',
 'MaxScore']

data.MaxScore=data[['MaxScore','Final']].max(axis=1)
251/263:
data.columns=['Surname',
 'ID number',
 'Email address','Test1','Test2',"Test3",
 'Final',
 'MaxScore']

data.MaxScore=data[['MaxScore','Final']].max(axis=1)
data
251/264:
writer = ExcelWriter('submit.xlsx')
data.to_excel(writer,'Sheet1',index=False)
writer.save()
251/265: df=pd.read_excel('gg.xlsx', index_col=0)
251/266: df.iloc[38][0:40]
251/267: df.iloc[37][0:40]
251/268: df.iloc[0:37][0:40]
251/269: df.iloc[0:38][0:40]
251/270: x=df.iloc[0:38][0:40]
251/271: x
251/272: x.columns
251/273:
x=x.drop([
 'Course total (Real)',
 'Scheme 1',
 'Scheme 2',
 'Scheme 3',
 'Scheme 4',
 'Scheme 5',
 'Scheme 6',
 'Scheme 7',
 'Scheme 8'],1)
list(x.columns)
251/274:
x.columns=['Surname',
 'ID number',
 'Email address','Test1','Test2',"Test3",
 'Final',
 'MaxScore']
251/275:
x.columns=['Surname',
 'ID number',
 'Email address','Test1','Test2',"Test3",
 'Final',
 'MaxScore']
x
251/276:
data2=data.append(x)
data2
251/277:
writer = ExcelWriter('submit2.xlsx')
data2.to_excel(writer,'Sheet1',index=False)
writer.save()
251/278: submit2=pd.read_excel('submit2.xlsx', index_col=0)
251/279: submit2.MaxScore
251/280: submit2.MaxScore.mean()
251/281: Aplus2=submit2.MaxScore[submit2.MaxScore>89]
251/282: r=submit2.dropna()
251/283: r.MaxScore.mean()
251/284:
AplusPer=(len(Aplus2)/len(r.MaxScore))*100
AplusPer
251/285:
m=r.MaxScore
Aplus=m[m>89]
A=m[(m<=89) & (m>79)]
Bplus=m[(m<=79) & (m>74)]
B=m[(m<=74) & (m>69)]
Cplus=m[(m<=69) & (m>64)]
C=m[(m<=64) & (m>59)]
Dplus=m[(m<=59) & (,m>54)]
D=m[(m<=54) & (m>49)]
E=m[(m<=49) & (m>40)]
F=m[(m<=37)]
251/286:
m=r.MaxScore
Aplus=m[m>89]
A=m[(m<=89) & (m>79)]
Bplus=m[(m<=79) & (m>74)]
B=m[(m<=74) & (m>69)]
Cplus=m[(m<=69) & (m>64)]
C=m[(m<=64) & (m>59)]
Dplus=m[(m<=59) & (m>54)]
D=m[(m<=54) & (m>49)]
E=m[(m<=49) & (m>40)]
F=m[(m<=37)]
251/287:
AplusPer=(len(Aplus2)/len(m))*100
Aper=(len(A)/len(m))*100
Bplusper=(len(Bplus)/len(m))*100
Bper=(len(B)/len(m))*100
Cplusper=(len(Cplus)/len(m))*100
Cper=(len(C)/len(m))*100
Dplusper=(len(Dplus)/len(m))*100
Dper=(len(D)/len(m))*100
Eper=(len(E)/len(m))*100
Fper=(len(F)/len(m))*100
251/288: Aper
251/289: Aplus
251/290: Aplusper
251/291: AplusPer
251/292: Bplusper
251/293: Bper
251/294: Cplusper
251/295: Eper
251/296: Fper
251/297: submit2.to_csv('submit3.csv')
251/298:
with open('submit3.csv') as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
251/299:
import csv
with open('submit3.csv') as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
251/300: readCSV
251/301: csvfile
251/302: submit4=pd.read_excel('submit4.xlsx', index_col=0)
251/303: submit4=pd.read_excel('submit4.xlsx', index_col=0)
251/304:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/305: df=pd.read_excel('gg.xlsx', index_col=0)
251/306: submit4=pd.read_excel('submit4.xlsx', index_col=0)
251/307: submit4=pd.read_excel('submit4.xlsb', index_col=0)
251/308: submit4=pd.read_excel('submit4.xlsm', index_col=0)
251/309: submit4.columns
251/310: Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
251/311:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.columns
251/312:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.columns[[0]]
251/313:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[][0]
251/314:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[:][0]
251/315:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[0:10][0]
251/316:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[0:10][1]
251/317:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[1:10][1]
251/318:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[1:10][1:2]
251/319:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[1:10][1:1]
251/320:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[1:10][2:2]
251/321:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[1:10][2:3]
251/322:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.iloc[1:10][2:3]
251/323:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.letter
251/324:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.columns
251/325: sum(submit4.LetterGrafe==Math.letter)
251/326: sum(submit4.LetterGrade==Math.letter)
251/327: Math.index=df.index
251/328: len(df.index)
251/329: Math.inex=submit4.index
251/330: sum(submit4.LetterGrade==Math.letter)
251/331: sum(submit4.LetterGrade==Math.letter)
251/332: Math.inex=submit4.index
251/333: sum(submit4.LetterGrade==Math.letter)
251/334: Math.inex=submit4.index
251/335: sum(submit4.LetterGrade==Math.letter)
251/336: Math.index=submit4.index
251/337: len(Math.index)
251/338: len(submit4.index)
251/339:
submit4=pd.read_excel('submit4.xlsm', index_col=0)
submit.shape
251/340:
submit4=pd.read_excel('submit4.xlsm', index_col=0)
submit4.shape
251/341:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.shape
251/342: len(submit4.index)
251/343:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/344: df=pd.read_excel('gg.xlsx', index_col=0)
251/345:
submit4=pd.read_excel('submit4.xlsm', index_col=0)
submit4.shape
251/346:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.shape
251/347:
submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
251/348:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
251/349:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.shape
251/350: Math.columns
251/351: df.index=Math.index
251/352: sum(df.Surname==Math.Surname)
251/353: df.Surname==Math.Surname
251/354: df.Surname
251/355: Math.Surname
251/356: Math.id
251/357: Math.index
251/358: df.columns
251/359:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
251/360: df=pd.read_excel('gg.xlsx', index_col=0)
251/361:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
251/362:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.shape
251/363: Math.columns
251/364: df.index=Math.index
251/365: df.columns
251/366: Math.index==df.ID
251/367: sum(Math.index==df.ID)
251/368: df[df[Math.index==df.ID]].Surname
251/369: df[Math.index==df.ID].Surname
251/370: df[Math.index!=df.ID].Surname
251/371:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.shape
251/372: df=pd.read_excel('rich.xlsx', index_col=0)
251/373:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
251/374: Math.columns
251/375: df.columns
251/376: df[df.Surname!=Math.Surname]
251/377: df.index=Math.index
251/378: df.columns
251/379: df[df.Surname!=Math.Surname]
251/380: df[df.Surname!=Math.Surname].Surname
252/1:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
252/2: df=pd.read_excel('rich.xlsx', index_col=0)
252/3:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
252/4:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.shape
252/5: Math.columns
252/6: df.index=Math.index
252/7: df.columns
252/8:
Math=pd.read_excel('2018MATH1013M.xlsm', index_col=0)
Math.shape
252/9: Math.columns
252/10:
Math=pd.read_excel('2018MATH1013M.xls', index_col=0)
Math.shape
252/11: Math.columns
252/12:
Math=pd.read_excel('2018MATH1013M.xls', index_col=0)
Math.shape
252/13: Math.columns
252/14:
Math=pd.read_excel('2018MATH1013M.xls', index_col=0)
Math.shape
252/15: Math.columns
252/16:
Math=pd.read_excel('MATH1013M.xls', index_col=0)
Math.shape
252/17: Math.columns
252/18:
Math=pd.read_excel('MATH.xls', index_col=0)
Math.shape
252/19: Math.columns
252/20: Math.ID=Math.index
252/21: Math.columns
252/22: Math['ID']=Math.index
252/23: Math.columns
252/24: df[df.ID!=Math.ID].Surname
252/25: df.ID!=Math.ID
252/26: sum(df.ID!=Math.ID)
252/27: df=pd.read_excel('rich.xlsx', index_col=0)
252/28:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
252/29:
Math=pd.read_excel('MATH.xls', index_col=0)
Math.shape
252/30: Math['ID']=Math.index
252/31: Math.columns
252/32: df.columns
252/33: df=pd.read_excel('rich.xlsx', index_col=0)
252/34:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
252/35:
Math=pd.read_excel('MATH.xls', index_col=0)
Math.shape
252/36: Math['ID']=Math.index
252/37: Math.columns
252/38: df.columns
252/39: final=df.MaxScore
252/40: final.mean()
252/41: df.MaxScore.mean()
252/42: Aplus=final[final>89]
252/43: A=final[(final<=89) & (final>79)]
252/44: Bplus=final[(final<=79) & (final>74)]
252/45: B=final[(final<=74) & (final>69)]
252/46: Cplus=final[(final<=69) & (final>64)]
252/47: C=final[(final<=64) & (final>59)]
252/48: Dplus=final[(final<=59) & (final>54)]
252/49: D=final[(final<=54) & (final>49)]
252/50: E=final[(final<=49) & (final>40)]
252/51: F=final[(final<=37)]
252/52:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
252/53:
APer=(len(A)/len(final))*100
APer
252/54:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
252/55:
BPer=(len(B)/len(final))*100
BPer
252/56:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
252/57:
CPer=(len(C)/len(final))*100
CPer
252/58:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
252/59:
DPer=(len(D)/len(final))*100
DPer
252/60:
EPer=(len(E)/len(final))*100
EPer
252/61:
FPer=(len(F)/len(final))*100
FPer
252/62:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
df.Test1[(df.Test1<=75) & (df.Test1>40)]=df.Test1[(df.Test1<=75) & (df.Test1>40)]+7
252/63: df.columns
252/64: df.Test1
252/65: df.columns
252/66: df=pd.read_excel('rich.xlsx', index_col=0)
252/67:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
252/68:
Math=pd.read_excel('MATH.xls', index_col=0)
Math.shape
252/69: Math['ID']=Math.index
252/70: Math.columns
252/71: df.columns
252/72:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
df.Test1[(df.Test1<=75) & (df.Test1>40)]=df.Test1[(df.Test1<=75) & (df.Test1>40)]+7
252/73:
writer = ExcelWriter('rich.xlsx')
df.to_excel(writer,'Sheet1',index=False)
writer.save()
252/74:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
252/75: df=pd.read_excel('rich.xlsx', index_col=0)
252/76:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
252/77:
Math=pd.read_excel('MATH.xls', index_col=0)
Math.shape
252/78: Math['ID']=Math.index
252/79: Math.columns
252/80: df.columns
252/81: df.Test1[0:10]
252/82:
# x.columns=['Surname',
#  'ID number',
#  'Email address','Test1','Test2',"Test3",
#  'Final',
#  'MaxScore']
252/83:
# x=x.drop([
#  'Course total (Real)',
#  'Scheme 1',
#  'Scheme 2',
#  'Scheme 3',
#  'Scheme 4',
#  'Scheme 5',
#  'Scheme 6',
#  'Scheme 7',
#  'Scheme 8'],1)
# list(x.columns)
252/84:
# cols = [e for e in cols if e not in ('Surname',
#  'ID number',
#  'Email address')]
# cols
252/85:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
df.Test1[(df.Test1<=75) & (df.Test1>40)]=df.Test1[(df.Test1<=75) & (df.Test1>40)]+7
252/86: df.Test1[0:10]
252/87:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
T=(df.Test1<=75) & (df.Test1>40)
df.Test1[T]=df.Test1[(df.Test1<=75) & (df.Test1>40)]+7
252/88: df.Test1[0:10]
252/89: df=pd.read_excel('rich.xlsx', index_col=0)
252/90: df=pd.read_excel('rich1.xlsx', index_col=0)
252/91:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
252/92:
Math=pd.read_excel('MATH.xls', index_col=0)
Math.shape
252/93:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
T=(df.Test1<=100) & (df.Test1>150)
df.Test1[T]=df.Test1[(df.Test1<=75) & (df.Test1>40)]+10
#df.Test2[T]
252/94:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
252/95: df=pd.read_excel('rich1.xlsx', index_col=0)
252/96:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
252/97:
Math=pd.read_excel('MATH.xls', index_col=0)
Math.shape
252/98: Math['ID']=Math.index
252/99:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
T=(df.Test1<=150) & (df.Test1>40)
df.Test1[T]=df.Test1[(df.Test1<=150) & (df.Test1>40)]+10
#df.Test2[T]
252/100:
writer = ExcelWriter('rich.xlsx')
df.to_excel(writer,'Sheet1',index=False)
writer.save()
252/101: final=df.MaxScore
252/102: final.mean()
252/103: data=pd.read_excel('rich1.xlsx', index_col=0)
252/104: final=data.MaxScore
252/105: final.mean()
252/106:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
T=(df.Test1<=150) & (df.Test1>40)
df.Test1[T]=df.Test1[(df.Test1<=150) & (df.Test1>40)]+10
#df.Test2[T]
252/107:
writer = ExcelWriter('rich.xlsx')
df.to_excel(writer,'Sheet1',index=False)
writer.save()
252/108:
writer = ExcelWriter('rich.xlsx')
df.to_excel(writer,'Sheet1',index=False)
writer.save()
252/109: data=pd.read_excel('rich1.xlsx', index_col=0)
252/110: ddf.shape
252/111: final=data.MaxScore
252/112: final.mean()
252/113: df.MaxScore.mean()
252/114:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
T=(df.Test2<=160) & (df.Test2>40)
df.Test2[T]=df.Test2[(df.Test2<=160) & (df.Test2>40)]+20
#df.Test2[T]
252/115:
writer = ExcelWriter('rich.xlsx')
df.to_excel(writer,'Sheet1',index=False)
writer.save()
252/116: data=pd.read_excel('rich1.xlsx', index_col=0)
252/117: final=data.MaxScore
252/118: final.mean()
252/119:
#df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]=df.MaxScore[(df.MaxScore<=85) & (df.MaxScore>40)]+7
T=(df.Test2<=160) & (df.Test2>40)
df.Test2[T]=df.Test2[(df.Test2<=160) & (df.Test2>40)]+10
#df.Test2[T]
252/120:
writer = ExcelWriter('rich.xlsx')
df.to_excel(writer,'Sheet1',index=False)
writer.save()
252/121: data=pd.read_excel('rich1.xlsx', index_col=0)
252/122: final=data.MaxScore
252/123: final.mean()
252/124: data=pd.read_excel('rich1.xlsx', index_col=0)
252/125: final=data.MaxScore
252/126: final.mean()
252/127: data.MaxScore.mean()
252/128: Aplus=final[final>89]
252/129: A=final[(final<=89) & (final>79)]
252/130: Bplus=final[(final<=79) & (final>74)]
252/131: B=final[(final<=74) & (final>69)]
252/132: Cplus=final[(final<=69) & (final>64)]
252/133: C=final[(final<=64) & (final>59)]
252/134: Dplus=final[(final<=59) & (final>54)]
252/135: D=final[(final<=54) & (final>49)]
252/136: E=final[(final<=49) & (final>40)]
252/137: F=final[(final<=37)]
252/138:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
252/139:
APer=(len(A)/len(final))*100
APer
252/140:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
252/141:
BPer=(len(B)/len(final))*100
BPer
252/142:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
252/143:
CPer=(len(C)/len(final))*100
CPer
252/144:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
252/145:
DPer=(len(D)/len(final))*100
DPer
252/146:
EPer=(len(E)/len(final))*100
EPer
252/147:
FPer=(len(F)/len(final))*100
FPer
252/148: df.MaxScore=max(df.MaxScore,df.Final)
252/149: data=pd.read_excel('rich1.xlsx', index_col=0)
252/150: final=data.MaxScore
252/151: final.mean()
252/152: data=pd.read_excel('rich1.xlsx', index_col=0)
252/153: data.columns
252/154: Math['letter']=data.Letter
252/155: Math.index=data.index
252/156: Math['letter']=data.Letter
252/157:
writer = ExcelWriter('elena.xlsx')
Math.to_excel(writer,'Sheet1',index=False)
writer.save()
252/158: data.Letter
252/159: data=pd.read_excel('rich1.xlsx', index_col=0)
252/160: data.columns
252/161: data.Letter
252/162: final=data.MaxScore
252/163: final.mean()
252/164: data.MaxScore.mean()
252/165: Aplus=final[final>89]
252/166: A=final[(final<=89) & (final>79)]
252/167: Bplus=final[(final<=79) & (final>74)]
252/168: B=final[(final<=74) & (final>69)]
252/169: Cplus=final[(final<=69) & (final>64)]
252/170: C=final[(final<=64) & (final>59)]
252/171: Dplus=final[(final<=59) & (final>54)]
252/172: D=final[(final<=54) & (final>49)]
252/173: E=final[(final<=49) & (final>40)]
252/174: F=final[(final<=37)]
252/175:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
252/176:
APer=(len(A)/len(final))*100
APer
252/177:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
252/178:
BPer=(len(B)/len(final))*100
BPer
252/179:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
252/180:
CPer=(len(C)/len(final))*100
CPer
252/181:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
252/182:
DPer=(len(D)/len(final))*100
DPer
252/183:
EPer=(len(E)/len(final))*100
EPer
252/184:
FPer=(len(F)/len(final))*100
FPer
253/1:
import pandas as pd
import json
from tweepy.streaming import StreamListener
import twitter
import tweepy
253/2:


# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    credentials = json.load(file)
253/3:

#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
            except:
                tweet=status.retweeted_status.text
        else:
        #-------------------
            try:
                tweet=status.extended_tweet["full_text"]
                return True
            except BaseException as e:
                tweet=status.text
            return True
        print(tweet)
        df2.loc[len(df2)]=[tweet] 
        df= df.append(df2,ignore_index=True)
        df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
253/4:

auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
api = tweepy.API(auth)
253/5:
# #creating a dataframe once, I already created it
# df = pd.DataFrame(columns=['tweet'])
# df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
253/6:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
254/1:
import pandas as pd
import json
from tweepy.streaming import StreamListener
import twitter
import tweepy
254/2:


# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
    credentials = json.load(file)
254/3:

#from tweepy import StreamListener, Stream, OAuthHandler
#from virtualornithology.analysis.importer import Importer


class MyStreamListener(StreamListener):
    

    def on_status(self, status):
        df = pd.read_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        df2=pd.DataFrame(columns=['tweet'])
        if hasattr(status, 'retweeted_status'):
            try:
                tweet= status.retweeted_status.extended_tweet["full_text"]
            except:
                tweet=status.retweeted_status.text
        else:
        #-------------------
            try:
                tweet=status.extended_tweet["full_text"]
                return True
            except BaseException as e:
                tweet=status.text
            return True
        print(tweet)
        df2.loc[len(df2)]=[tweet] 
        df= df.append(df2,ignore_index=True)
        df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
        return True

    def on_error(self, status_code):
        if status_code == 420:
            print("Error 420")
            #returning False in on_error disconnects the stream
            return False
254/4:

auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
api = tweepy.API(auth)
254/5:
# #creating a dataframe once, I already created it
# df = pd.DataFrame(columns=['tweet'])
# df.to_pickle('/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/file_name')
254/6:
from tweepy import Stream
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
twitter_stream = Stream(auth, MyStreamListener(),tweet_mode='extended')
twitter_stream.filter(track=[query])
255/1:
from tweepy import Stream
import pandas as pd
255/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
255/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
255/4:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
255/5:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
255/6:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
255/7:

df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
255/8: df.shape
255/9:
list_columns=list(df.columns)
list_columns
255/10: df.created_at[1:10]
255/11:
#keep English languages
data=df[df.lang=='en']
255/12: data.shape
255/13: data.truncated[21]
255/14:

data.text[21]
255/15: data.retweeted_status[31]
255/16:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
255/17:

df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
255/18: df.shape
255/19:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
255/20:

df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
255/21: df.shape
255/22:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
256/1:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
256/2: df=pd.read_excel('rich1.xlsx', index_col=0)
256/3: data=pd.read_excel('rich1.xlsx', index_col=0)
256/4: data.columns
256/5: final=data.MaxScore
256/6: final.mean()
256/7: data.MaxScore.mean()
256/8: Aplus=final[final>89]
256/9: A=final[(final<=89) & (final>79)]
256/10: Bplus=final[(final<=79) & (final>74)]
256/11: B=final[(final<=74) & (final>69)]
256/12: Cplus=final[(final<=69) & (final>64)]
256/13: C=final[(final<=64) & (final>59)]
256/14: Dplus=final[(final<=59) & (final>54)]
256/15: D=final[(final<=54) & (final>49)]
256/16: E=final[(final<=49) & (final>40)]
256/17: F=final[(final<=37)]
256/18:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
256/19:
APer=(len(A)/len(final))*100
APer
256/20:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
256/21:
BPer=(len(B)/len(final))*100
BPer
256/22:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
256/23:
CPer=(len(C)/len(final))*100
CPer
256/24:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
256/25:
DPer=(len(D)/len(final))*100
DPer
256/26:
EPer=(len(E)/len(final))*100
EPer
256/27:
FPer=(len(F)/len(final))*100
FPer
256/28: data.dropna()
256/29: data=data.dropna()
256/30: final=data.MaxScore
256/31: final.mean()
256/32:
T=(df.Letter!=F)
df[T]
256/33:

noF=df[T].MaxScore
256/34:
T=(df.Letter!=F)
T
256/35:
T=(data.Letter!=F)
df[T]
256/36:
T=(data.Letter!='F')
df[T]
256/37: T=(data.Letter!='F')
256/38:
T=(data.Letter!='F')
T
256/39:

noF=data.MaxScore[T]
256/40: noF.mean()
256/41: noF.mean()
256/42: final=data.MaxScore
256/43: final.mean()
256/44: data.MaxScore.mean()
256/45: len(A)
256/46:
Aplus=final[final>89]
len(Aplus)
256/47:
Bplus=final[(final<=79) & (final>74)]
len(Bplus)
256/48:
B=final[(final<=74) & (final>69)]
len(B)
256/49: BB=data.letter[data.Letter=='B']
256/50: BB=data.Letter[data.Letter=='B']
256/51:
BB=data.Letter[data.Letter=='B']
len(BB)
256/52:
Cplus=final[(final<=69) & (final>64)]
len(Cplus)
256/53:
C=final[(final<=64) & (final>59)]
len(C)
256/54:
Dplus=final[(final<=59) & (final>54)]
len(Dplus)
256/55:
D=final[(final<=54) & (final>49)]
len(D)
256/56:
E=final[(final<=49) & (final>40)]
len(E)
256/57:
F=final[(final<=37)]
len(F)
256/58: T=(data.Letter!='F')
256/59: data=pd.read_excel('rich1.xlsx', index_col=0)
256/60: data.shape
256/61: data=data.dropna()
256/62: data.shape
256/63: data=pd.read_excel('rich1.xlsx', index_col=0)
256/64: data.shape
256/65: data.columns
256/66: data=data.dropna(subset=['MaxScore'])
256/67: data.shape
256/68: T=(data.Letter!='F')
256/69:

noF=data.MaxScore[T]
256/70: noF.mean()
256/71: final=data.MaxScore
256/72: final.mean()
256/73: data.MaxScore.mean()
256/74:
Aplus=final[final>89
            
len(Aplus)
256/75:
Aplus=final[final>89]
len(Aplus)
256/76: A=final[(final<=89) & (final>79)]
256/77: len(A)
256/78:
Bplus=final[(final<=79) & (final>74)]
len(Bplus)
256/79:
B=final[(final<=74) & (final>69)]
len(B)
256/80:
BB=data.Letter[data.Letter=='B']
len(BB)
256/81:
Cplus=final[(final<=69) & (final>64)]
len(Cplus)
256/82:
C=final[(final<=64) & (final>59)]
len(C)
256/83:
Dplus=final[(final<=59) & (final>54)]
len(Dplus)
256/84:
D=final[(final<=54) & (final>49)]
len(D)
256/85:
E=final[(final<=49) & (final>40)]
len(E)
256/86:
F=final[(final<=37)]
len(F)
256/87:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
256/88: data=pd.read_excel('rich1.xlsx', index_col=0)
256/89: data.shape
256/90: data=data.dropna(subset=['MaxScore'])
256/91: data.shape
256/92: data=pd.read_excel('rich1.xlsx', index_col=0)
256/93: data.drona()
256/94: data.dropna()
256/95: data.dropna().shape
256/96:
F=final[(final<=24)]
len(F)
256/97:
F=data.MaxScore[(data.MaxScore<=24)]
len(F)
256/98:
F=data.MaxScore[(data.MaxScore<=24)]
len(F)
FF=data.Letter[data.Letter=='F']
len(FF)
256/99: data.MaxScore.mean()
256/100: data=data.dropna()
256/101: data.shape
256/102: data.MaxScore.mean()
256/103:
Aplus=final[final=='A+']
len(Aplus)
256/104: final=data.Letter
256/105:
Aplus=final[final=='A+']
len(Aplus)
256/106: A=final[final='A']
256/107: A=final[final=='A']
256/108: len(A)
256/109:
Bplus=final[final=='B+']
len(Bplus)
256/110:
B=final[final='B']
len(B)
256/111:
B=final[final=='B']
len(B)
256/112:
Cplus=final[final=='C+']
len(Cplus)
256/113:
C=final[final='C']
len(C)
256/114:
C=final[final=='C']
len(C)
256/115:
Dplus=final[final='D+']
len(Dplus)
256/116:
Dplus=final[final=='D+']
len(Dplus)
256/117:
D=final[final=='D']
len(D)
256/118:
E=final[final=='E']
len(E)
256/119:
F=data.MaxScore[(data.MaxScore<=24)]
len(F)
FF=data.Letter[data.Letter=='F']
len(FF)
256/120:
F=data.MaxScore[(data.MaxScore<=24)]
len(F)
FF=data.Letter[data.Letter=='F']
len(FF)
F=final[final=='F']
len(F)
256/121:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
256/122:
APer=(len(A)/len(final))*100
APer
256/123:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
256/124:
BPer=(len(B)/len(final))*100
BPer
256/125:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
256/126:
CPer=(len(C)/len(final))*100
CPer
256/127:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
256/128:
DPer=(len(D)/len(final))*100
DPer
256/129:
EPer=(len(E)/len(final))*100
EPer
256/130:
FPer=(len(F)/len(final))*100
FPer
256/131:
FPer=(len(F)/len(final))*100
FPer
256/132: data=pd.read_excel('rich1.xlsx', index_col=0)
256/133: data.shape
256/134: data=data.dropna()
256/135: data.shape
256/136: T=(data.Letter!='F')
256/137:

noF=data.MaxScore[T]
256/138: noF.mean()
256/139: final=data.Letter
256/140: data.MaxScore.mean()
256/141: data.MaxScore.mean()
256/142:
Aplus=final[final=='A+']
len(Aplus)
256/143: A=final[final=='A']
256/144: len(A)
256/145:
Bplus=final[final=='B+']
len(Bplus)
256/146:
B=final[final=='B']
len(B)
256/147:
BB=data.Letter[data.Letter=='B']
len(BB)
256/148:
Cplus=final[final=='C+']
len(Cplus)
256/149:
C=final[final=='C']
len(C)
256/150:
Dplus=final[final=='D+']
len(Dplus)
256/151:
D=final[final=='D']
len(D)
256/152:
E=final[final=='E']
len(E)
256/153:

F=final[final=='F']
len(F)
256/154:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
256/155:
APer=(len(A)/len(final))*100
APer
256/156:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
256/157:
BPer=(len(B)/len(final))*100
BPer
256/158:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
256/159:
CPer=(len(C)/len(final))*100
CPer
256/160:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
256/161:
DPer=(len(D)/len(final))*100
DPer
256/162:
EPer=(len(E)/len(final))*100
EPer
256/163:
FPer=(len(F)/len(final))*100
FPer
256/164: data=pd.read_excel('rich1.xlsx', index_col=0)
256/165: data.shape
256/166: data=data.dropna()
256/167: Math['letter']=data.Letter
256/168:
writer = ExcelWriter('elena.xlsx')
Math.to_excel(writer,'Sheet1',index=False)
writer.save()
256/169: T=(data.Letter!='F')
256/170:

noF=data.MaxScore[T]
256/171: noF.mean()
256/172: final=data.Letter
256/173: data.MaxScore.mean()
256/174: data.MaxScore.mean()
256/175:
Aplus=final[final=='A+']
len(Aplus)
256/176: A=final[final=='A']
256/177: len(A)
256/178:
Bplus=final[final=='B+']
len(Bplus)
256/179:
B=final[final=='B']
len(B)
256/180:
BB=data.Letter[data.Letter=='B']
len(BB)
256/181:
Cplus=final[final=='C+']
len(Cplus)
256/182:
C=final[final=='C']
len(C)
256/183:
Dplus=final[final=='D+']
len(Dplus)
256/184:
D=final[final=='D']
len(D)
256/185:
E=final[final=='E']
len(E)
256/186:

F=final[final=='F']
len(F)
256/187:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
256/188:
APer=(len(A)/len(final))*100
APer
256/189:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
256/190:
BPer=(len(B)/len(final))*100
BPer
256/191:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
256/192:
CPer=(len(C)/len(final))*100
CPer
256/193:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
256/194:
DPer=(len(D)/len(final))*100
DPer
256/195:
EPer=(len(E)/len(final))*100
EPer
256/196:
FPer=(len(F)/len(final))*100
FPer
256/197: data=pd.read_excel('rich1.xlsx', index_col=0)
256/198: data.shape
256/199: data=data.dropna()
256/200: T=(data.Letter!='F')
256/201:

noF=data.MaxScore[T]
256/202: noF.mean()
256/203: final=data.Letter
256/204: data.MaxScore.mean()
256/205: data.MaxScore.mean()
256/206:
Aplus=final[final=='A+']
len(Aplus)
256/207: A=final[final=='A']
256/208: len(A)
256/209:
Bplus=final[final=='B+']
len(Bplus)
256/210:
B=final[final=='B']
len(B)
256/211:
BB=data.Letter[data.Letter=='B']
len(BB)
256/212:
Cplus=final[final=='C+']
len(Cplus)
256/213:
C=final[final=='C']
len(C)
256/214:
Dplus=final[final=='D+']
len(Dplus)
256/215:
D=final[final=='D']
len(D)
256/216:
E=final[final=='E']
len(E)
256/217:

F=final[final=='F']
len(F)
256/218:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
256/219:
APer=(len(A)/len(final))*100
APer
256/220:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
256/221:
BPer=(len(B)/len(final))*100
BPer
256/222:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
256/223:
CPer=(len(C)/len(final))*100
CPer
256/224:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
256/225:
DPer=(len(D)/len(final))*100
DPer
256/226:
EPer=(len(E)/len(final))*100
EPer
256/227:
FPer=(len(F)/len(final))*100
FPer
256/228:
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import numpy as np
256/229: df=pd.read_excel('rich1.xlsx', index_col=0)
256/230:
#submit4=pd.read_excel('submit4.xlsm', index_col=0)
df.shape
256/231:
Math=pd.read_excel('MATH.xls', index_col=0)
Math.shape
256/232: Math.index=data.index
256/233: data=pd.read_excel('rich1.xlsx', index_col=0)
256/234: data.shape
256/235: data=data.dropna()
256/236: # Math['letter']=data.Letter
256/237:
# writer = ExcelWriter('elena.xlsx')
# Math.to_excel(writer,'Sheet1',index=False)
# writer.save()
256/238: data=data.dropna(subset=['MaxScore'])
256/239: data.shape
256/240: T=(data.Letter!='F')
256/241:

noF=data.MaxScore[T]
256/242: noF.mean()
256/243: data.MaxScore.mean()
256/244: data.MaxScore.mean()
256/245:
Aplus=final[final=='A+']
len(Aplus)
256/246: A=final[final=='A']
256/247: len(A)
256/248:
Bplus=final[final=='B+']
len(Bplus)
256/249:
B=final[final=='B']
len(B)
256/250:
BB=data.Letter[data.Letter=='B']
len(BB)
256/251:
Cplus=final[final=='C+']
len(Cplus)
256/252:
C=final[final=='C']
len(C)
256/253:
Dplus=final[final=='D+']
len(Dplus)
256/254:
D=final[final=='D']
len(D)
256/255:
E=final[final=='E']
len(E)
256/256:

F=final[final=='F']
len(F)
256/257:
AplusPer=(len(Aplus)/len(final))*100
AplusPer
256/258:
APer=(len(A)/len(final))*100
APer
256/259:
BplusPer=(len(Bplus)/len(final))*100
BplusPer
256/260:
BPer=(len(B)/len(final))*100
BPer
256/261:
CplusPer=(len(Cplus)/len(final))*100
CplusPer
256/262:
CPer=(len(C)/len(final))*100
CPer
256/263:
DplusPer=(len(Dplus)/len(final))*100
DplusPer
256/264:
DPer=(len(D)/len(final))*100
DPer
256/265:
EPer=(len(E)/len(final))*100
EPer
256/266:
FPer=(len(F)/len(final))*100
FPer
256/267: Aplusper+Aper+Bplusper+B+Dplusper+Dper+Eper+Fper
256/268:
Aplusper=(len(Aplus)/len(final))*100
Aplusper
256/269: Aplusper+Aper+Bplusper+B+Dplusper+Dper+Eper+Fper
256/270:
Aper=(len(A)/len(final))*100
Aper
256/271:
Bplusper=(len(Bplus)/len(final))*100
BplusPper
256/272:
Bplusper=(len(Bplus)/len(final))*100
Bplusper
256/273:
Bper=(len(B)/len(final))*100
Bper
256/274:
Cplusper=(len(Cplus)/len(final))*100
Cplusper
256/275:
Cper=(len(C)/len(final))*100
Cper
256/276:
Dplusper=(len(Dplus)/len(final))*100
Dplusper
256/277:
Dper=(len(D)/len(final))*100
Dper
256/278:
Eper=(len(E)/len(final))*100
Eper
256/279:
Fper=(len(F)/len(final))*100
Fper
256/280: Aplusper+Aper+Bplusper+B+Dplusper+Dper+Eper+Fper
256/281: x=Aplusper+Aper+Bplusper+B+Dplusper+Dper+Eper+Fper
256/282: x=Aplusper+Aper
256/283: x=Aplusper+Aper+Bplusper+Bper+Dplusper+Dper+Eper+Fper
256/284:
x=Aplusper+Aper+Bplusper+Bper+Dplusper+Dper+Eper+Fper
x
256/285:
x=Aplusper+Aper+Bplusper+Bper+Dplusper+Dper+Eper+Fper
x+17
256/286:
x=Aplusper+Aper+Bplusper+Bper+Dplusper+Dper+Eper+Fper
x+17.07
255/23:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
255/24:

df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
255/25: df.shape
258/1:
from tweepy import Stream
import pandas as pd
258/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
258/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
258/4:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
258/5:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
258/6:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
257/1: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/level1_main.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
257/2: runfile('/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline/level1_main.py', wdir='/Users/shahla/Dropbox/MLCourse/ML1030/Project/forstack_york/pipeline')
257/3: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/4: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/5: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/6: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/7: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/8: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/9: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/10: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/11: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/12: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/13: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/14:
import glob
print(glob.glob("/Users/shahla/Desktop/1013grades/*.xlsx"))
257/15: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/16: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/17: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
258/7:

df = pd.read_json("/Users/shahla/Dropbox/MLCourse/SharpestMinds/stream/data/stream_politics.json",lines=True)
258/8:
from tweepy import Stream
import pandas as pd
258/9:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
258/10:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
258/11:
from tweepy import Stream
import pandas as pd
258/12:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
258/13:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
258/14:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
258/15:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
258/16:
query = 'politics'
data_dir = '/Users/shahla/Documents/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
258/17:

df = pd.read_json("/Users/shahla/Documents/SharpestMinds/stream/data/stream_politics.json",lines=True)
258/18: df.shape
258/19:
query = 'politics'
data_dir = '/Users/shahla/Documents/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
257/18: cwd
257/19:
import os
print(os.getcwd())
257/20: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/21: runfile('/Users/shahla/Desktop/untitled4.py', wdir='/Users/shahla/Desktop')
257/22:
if not os.path.exists('/newdir'):
    os.makedirs('/tmp/newdir')
257/23:
if not os.path.exists('/newdir'):
    os.makedirs('/newdir')
258/20:

df = pd.read_json("/Users/shahla/Documents/SharpestMinds/stream/data/stream_politics.json",lines=True)
258/21: df.shape
258/22:

data.text[14800]
258/23:
#keep English languages
data=df[df.lang=='en']
258/24: data.shape
258/25: data.truncated[21]
258/26:

data.text[32270]
258/27: data.retweeted_status[32270]
258/28: data.retweeted_status[32270]['extended_tweet']['full_text']
258/29: data.retweeted_status[21]['extended_tweet']['full_text']
258/30:

data.text[21]
257/24:
wd=os.getcwd()
#from parameter_var import parameter_var
parser = argparse.ArgumentParser(description='pipeline')
257/25: import argparse
257/26:
wd=os.getcwd()

#from parameter_var import parameter_var
parser = argparse.ArgumentParser(description='pipeline')
257/27: wd
257/28:
if not os.path.exists(wd+'/saved_models_level1'):
    os.makedirs(wd+'/saved_models_level1')
257/29:
if os.path.exists('../saved_models_level1'):
    print('error')
257/30:
if not os.path.exists('../saved_models_level1'):
    print('error')
257/31: predict_model=joblib.load(save_load_path)
257/32: import importlib
257/33: predict_model=joblib.load(save_load_path)
257/34: from sklearn.externals import joblib
257/35: predict_model=joblib.load(save_load_path)
257/36: x=2
257/37:
if x>1 & x=2:
    print(x
    )
257/38:
if x>1 and
 x=2:
    print(x
    )
257/39:
if x>1 and
 x==
2:
    print(x
    )
257/40:
if x>1 and
 x==


2:
    print(x

        )
257/41:
if x>2 and x==3:
    print(x)
257/42:
if x>1 and x==2:
    print(x)
257/43:
if x>1 and x!
=2:
    print(x)
257/44:
if x>1 and x!=2:
    print(x)
260/1:
from tweepy import Stream
import pandas as pd
260/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
260/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
260/4:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
260/5:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
260/6:
query = 'politics'
data_dir = '/Users/shahla/Documents/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
260/7: df.shape
260/8:

df = pd.read_json("/Users/shahla/Documents/SharpestMinds/stream/data/stream_politics.json",lines=True)
260/9: df.shape
260/10:
query = 'politics'
data_dir = '/Users/shahla/Documents/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
260/11:

df = pd.read_json("/Users/shahla/Documents/SharpestMinds/stream/data/stream_politics.json",lines=True)
260/12: df.shape
263/1: import gensim #downloaded pretrained model
266/1:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
266/2:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
266/3:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
266/4: lookbacks['100ms']
266/5: lookbacks['300ms']
266/6:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
266/7:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
266/8:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
#input_files=df.copy()
266/9:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
266/10:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
266/11:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
266/12: input_files.info()
272/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
272/2: from sklearn.base import clone
272/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
272/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
272/5:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
272/6:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
272/7:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'2.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.001)
df = df[:index]
272/8:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
#input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
272/9:
list_columns=list(input_files)
#list_columns
272/10:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
272/11:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
272/12:
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
272/13:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
272/14:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
272/15:
for feature, feature_set in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)

    ##the rest of the codes below will be here!
272/16: features_dic['MOVING_AVG']
272/17: input_files_train_contracts[features_dic['MOVING_AVG']]
272/18: features_dic['MOVING_AVG']
272/19: features_dic['CCI']
272/20: features_dic['MOVING_AVG']
272/21: features_dic['RSI']
272/22: features_dic['CMO']
272/23: features_dic['STOCH']
272/24: features_dic['BBI']
272/25: features_dic['BBI_2']
273/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
273/2: from sklearn.base import clone
273/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
273/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
273/5:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
273/6:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
273/7:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'2.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.001)
df = df[:index]
273/8:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
#input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
273/9:
list_columns=list(input_files)
#list_columns
273/10:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
273/11:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
273/12:
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
273/13:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
273/14:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
273/15:
for feature, feature_set in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)

    ##the rest of the codes below will be here!
273/16: features_dic['INV_MOVING_AVG']
275/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
275/2: lookbacks['100ms']
275/3: lookbacks['300ms']
275/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
275/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
275/6:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
275/7:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
275/8: input_files.info()
275/9:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
275/10: input_files.info()
275/11: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
275/12:
list_columns=list(input_files)
list_columns
275/13: input_files.dtypes
275/14:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
275/15:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
275/16: input_files['ret_sec_1'].head()
275/17:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
275/18:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
275/19: input_files.dtypes
275/20: min(input_files.index)
275/21: max(input_files.index)
275/22:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['price'].plot(grid=True)

# Show the plot
plt.show()
275/23: x=input_files['price'].index if input_files['price'].index<=2019-01-01
275/24: dtype(input_files['price'].index)
275/25: input_files['price'].index.type
275/26: input_files['2019-02-01' :'2019-02-10']
275/27: input_files['2019-02-01' :'2019-02-10'].price
277/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
277/2: lookbacks['100ms']
277/3: lookbacks['300ms']
277/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
277/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
277/6:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
277/7:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
277/8: input_files.info()
277/9: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
277/10:
list_columns=list(input_files)
list_columns
277/11: input_files.dtypes
277/12:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
277/13:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
277/14: input_files['ret_sec_1'].head()
277/15:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
277/16:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
277/17: input_files.dtypes
277/18:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
277/19: min(input_files.index)
277/20: max(input_files.index)
277/21: input_files['2019-02-01' :'2019-02-10'].price
277/22: input_files['2019-02-01' :'2019-02-10'].price
277/23: input_files['2019-02-01 00:00:00.222000' :'2019-02-5 00:00:00.222000'].price
277/24: input_files..price
277/25: input_files.price
277/26: input_files.price[1:3]
277/27: input_files.price[1:300]
277/28: input_files.price[1:1000]
277/29: input_files.price[1:2000]
277/30: input_files.price[1:4000]
277/31: input_files.price[1:10000]
277/32: input_files.price[1:40000]
277/33: input_files.price[1:100000]
277/34: input_files.price[1:200000]
277/35: input_files.index['2019-02-01 16:59:56.371    1.145625']
277/36: input_files['2019-02-01 16:59:56.371    1.145625'].price
277/37: input_files['2019-02-01 16:59:56.371    1.145625']
277/38: input_files['2019-02-01 16:59:56.371']
277/39: input_files.index=='2019-02-01 16:59:56.371'
277/40: sum(input_files.index=='2019-02-01 16:59:56.371')
277/41: input_files.loc[input_files.index == '2019-02-01 16:59:56.371']
277/42: input_files.loc[input_files.index == '2019-02-01 16:59:56.371'].price
277/43: input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/44:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/45:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('some numbers')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/46:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/47:
import matplotlib.pyplot as plt
import matplotlib.dates as md
import numpy as np
import datetime as dt
import time

n=20
duration=1000
now=time.mktime(time.localtime())
timestamps=np.linspace(now,now+duration,n)
dates=[dt.datetime.fromtimestamp(ts) for ts in timestamps]
values=np.sin((timestamps-now)/duration*2*np.pi)
plt.subplots_adjust(bottom=0.2)
plt.xticks( rotation=25 )
ax=plt.gca()
xfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
plt.plot(dates,values)
plt.show()
277/48:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')
xfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/49:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
xfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/50:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/51:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/52:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/53:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/54:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 00:00:00'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/55:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:50'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/56:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371' & input_files.index <= '2019-02-01 16:59:56'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/57:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371' and input_files.index <= '2019-02-01 16:59:56'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/58:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371' and input_files.index <= '2019-02-01 16:59:00'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/59:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371' and input_files.index <= '2019-02-01 12:59:00'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/60: max(input_files.price[1:200000])
277/61: input_files.index[input_files.price==max(input_files.price[1:200000])]
277/62:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371' and input_files.index >= '2019-02-01 11:24:32.731000'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/63:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index >= '2019-02-01 11:24:32.731000'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/64:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371' and input_files.index >= '2019-02-01 11:24:32.731000'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/65:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
#ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371' and input_files.index >= '2019-02-01 11:24:32.731000'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/66:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
#xfmt = md.DateFormatter('%H:%M:%S')
#ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371' and input_files.index >= '2019-02-01 11:24:32.731000'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/67:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
#ax=plt.gca()
#xfmt = md.DateFormatter('%H:%M:%S')
#ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371' and input_files.index >= '2019-02-01 11:24:32.731000'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/68:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
#ax=plt.gca()
#xfmt = md.DateFormatter('%H:%M:%S')
#ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index <= '2019-02-01 16:59:56.371') & (input_files.index >= '2019-02-01 11:24:32.731000')].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/69:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index <= '2019-02-01 16:59:56.371') & (input_files.index >= '2019-02-01 11:24:32.731000')].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/70:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index <= '2019-02-01 16:59:56.371') & (input_files.index >= '2019-02-01 12:24:32.731000')].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/71:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index <= '2019-02-01 12:59:56.371') & (input_files.index >= '2019-02-01 11:24:32.731000')].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/72:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index <= '2019-02-01 12:59:56.371') & (input_files.index >= '2019-02-01 10:24:32.731000')].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/73:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
savefig('sample.pdf')
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/74:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')

# Show the plot
plt.show()
plt.savefig('sample.pdf')
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/75:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')
plt.savefig('sample.pdf')
# Show the plot
plt.show()
plt.savefig('sample.pdf')
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/76:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')
plt.savefig('sample.pdf')
# Show the plot
plt.show()

#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/77:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 1, 2019')
plt.savefig('Feb1.pdf')
# Show the plot
plt.show()

#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
277/78:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index <= '2019-02-01 12:59:56.371') & (input_files.index >= '2019-02-01 10:24:32.731000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 1, 2019, maximum price occurs at time 11:24')
plt.savefig('Feb1max.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
279/2: lookbacks['100ms']
279/3: lookbacks['300ms']
279/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
279/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
279/6:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
279/7:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
279/8: input_files.info()
279/9: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
279/10:
list_columns=list(input_files)
list_columns
279/11: input_files.dtypes
279/12:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
279/13:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
279/14: input_files['ret_sec_1'].head()
279/15:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
279/16:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
279/17: input_files.dtypes
279/18:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
279/19: min(input_files.index)
279/20: max(input_files.index)`
279/21: max(input_files.index)
279/22:

# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >'2019-02-01 16:59:56.371') & (input_files.index <= '2019-02-03 23:59:58.667')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 3,2019')
plt.savefig('Feb3.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/23:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >'2019-02-01 16:59:56.371') & (input_files.index <= '2019-02-03 23:59:58.667')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 3,2019')
plt.savefig('Feb3.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/24: input_files.price
279/25: input_files.index<='2019-02-05 16:59:56.086000'
279/26: input_files.index<='2019-02-05 00:00:00.0000'
279/27:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >'2019-02-03 23:59:58.667') & (input_files.index < '2019-02-05 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 4,2019')
plt.savefig('Feb4.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/28:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-02-05 00:00:00.0000') & (input_files.index < '2019-02-06 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 5,2019')
plt.savefig('Feb5.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/29:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-02-06 00:00:00.0000') & (input_files.index < '2019-02-07 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 6,2019')
plt.savefig('Feb6.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/30:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-02-07 00:00:00.0000') & (input_files.index < '2019-02-08 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 7,2019')
plt.savefig('Feb7.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/31:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-02-08 00:00:00.0000') & (input_files.index < '2019-02-09 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 8,2019')
plt.savefig('Feb8.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/32:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February, 2019')
plt.savefig('Feb.pdf')
# Show the plot
plt.show()

#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/33:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-02-09 00:00:00.0000') & (input_files.index < '2019-02-10 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 9,2019')
plt.savefig('Feb9.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/34:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-02-09 00:00:00.0000') & (input_files.index < '2019-02-10 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('February 8,2019')
plt.savefig('Feb8.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
279/35: input_files.price
281/1:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
281/2:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'2.h5')
input_files=df.copy()
281/3:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
281/4:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
input_files=df.copy()
281/5:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
281/6: lookbacks['100ms']
281/7:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
281/8: lookbacks['100ms']
281/9: lookbacks['300ms']
281/10:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
281/11:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
281/12:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
input_files=df.copy()
281/13:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
input_files=df.copy()
281/14:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
input_files=df.copy()
281/15:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
281/16: input_files.info()
281/17: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
281/18: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
281/19:
list_columns=list(input_files)
list_columns
281/20: input_files.dtypes
281/21:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
281/22:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
281/23: input_files['ret_sec_1'].head()
281/24:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
281/25:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
281/26: input_files.dtypes
281/27:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
281/28: min(input_files.index)
281/29: max(input_files.index)
281/30:

input_files.loc[(input_files.index >='2019-02-09 00:00:00.0000') & (input_files.index < '2019-02-10 00:00:00.0000')].price
281/31:

input_files.loc[(input_files.index >='2019-01-01 00:00:00.0000') & (input_files.index < '2019-01-2 00:00:00.0000')].price
281/32:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-01 00:00:00.0000') & (input_files.index < '2019-01-02 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 1,2019')
plt.savefig('Jan1.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/33:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-02 00:00:00.0000') & (input_files.index < '2019-01-03 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 2,2019')
plt.savefig('Jan2.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/34:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-03 00:00:00.0000') & (input_files.index < '2019-01-04 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 2,2019')
plt.savefig('Jan2.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/35:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-02 00:00:00.0000') & (input_files.index < '2019-01-03 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 2,2019')
plt.savefig('Jan2.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/36:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-03 00:00:00.0000') & (input_files.index < '2019-01-04 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 3,2019')
plt.savefig('Jan3.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/37:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-04 00:00:00.0000') & (input_files.index < '2019-01-05 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 4,2019')
plt.savefig('Jan4.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/38:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-05 00:00:00.0000') & (input_files.index < '2019-01-06 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 5,2019')
plt.savefig('Jan5.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/39:

input_files.loc[(input_files.index >='2019-01-05 00:00:00.0000') & (input_files.index < '2019-01-6 00:00:00.0000')].price
281/40:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-06 00:00:00.0000') & (input_files.index < '2019-01-07 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 6,2019')
plt.savefig('Jan6.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/41:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-07 00:00:00.0000') & (input_files.index < '2019-01-08 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 6,2019')
plt.savefig('Jan6.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/42:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-06 00:00:00.0000') & (input_files.index < '2019-01-07 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 6,2019')
plt.savefig('Jan6.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/43:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-07 00:00:00.0000') & (input_files.index < '2019-01-08 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 7,2019')
plt.savefig('Jan7.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/44:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-08 00:00:00.0000') & (input_files.index < '2019-01-09 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 8,2019')
plt.savefig('Jan8.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/45:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-09 00:00:00.0000') & (input_files.index < '2019-01-10 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 9,2019')
plt.savefig('Jan9.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/46:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-10 00:00:00.0000') & (input_files.index < '2019-01-11 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 1o,2019')
plt.savefig('Jan10.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/47:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-11 00:00:00.0000') & (input_files.index < '2019-01-12 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 11,2019')
plt.savefig('Jan11.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/48:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-12 00:00:00.0000') & (input_files.index < '2019-01-13 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 12,2019')
plt.savefig('Jan12.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/49:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-13 00:00:00.0000') & (input_files.index < '2019-01-14 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 13,2019')
plt.savefig('Jan13.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/50:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-14 00:00:00.0000') & (input_files.index < '2019-01-15 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 14,2019')
plt.savefig('Jan14.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/51:
import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-15 00:00:00.0000') & (input_files.index < '2019-01-16 00:00:00.0000')].price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 15,2019')
plt.savefig('Jan15.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/52:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January, 2019')
plt.savefig('Jan.pdf')
# Show the plot
plt.show()

#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/53:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 2019')
plt.savefig('Jan.pdf')
# Show the plot
plt.show()

#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/54:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%Y-%m-%d')
ax.xaxis.set_major_formatter(xfmt)
input_files.price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 2019')
plt.savefig('Jan.pdf')
# Show the plot
plt.show()

#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/55:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%Y-%m-%d')
ax.xaxis.set_major_formatter(xfmt)
input_files.price.plot(grid=True)
plt.ylabel('price')
plt.xlabel('January 2019')
plt.savefig('Jan.pdf')
# Show the plot
plt.show()

#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/56:
# Import matplotlib
import matplotlib.pyplot as plt

# Plot the distribution of `
input_files[['price']].hist(bins=50)

# Show the plot
plt.show()

# Pull up summary statistics
print(input_files[['price']].describe())
281/57:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['forward_price_sec_1'].plot(grid=True)

# Show the plot
plt.show()
281/58:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index <= '2019-01-01 00:00:00.0000') & (input_files.index >= '2019-01-02 00:00:00.0000')].forward_price_sec_1.plot(grid=True)
plt.ylabel('forward_price_sec_1')
#plt.xlabel('')
plt.savefig('Jan1-sec1.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
# Import Matplotlib's `pyplot` module as `plt`
281/59:


import matplotlib.dates as md
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
input_files.loc[(input_files.index >='2019-01-01 00:00:00.0000') & (input_files.index < '2019-01-02 00:00:00.0000')].forward_price_sec_1.plot(grid=True)
plt.ylabel('forward_price_sec_1')
plt.xlabel('January 1,2019')
plt.savefig('Jan1-1sec.pdf')
# Show the plot
plt.show()
#input_files.loc[input_files.index <= '2019-02-01 16:59:56.371'].price
281/60:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['forward_price_sec_3'].plot(grid=True)

# Show the plot
plt.show()
281/61:
# Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt

# Plot the closing prices for `aapl`
input_files['ret_sec_1'].plot(grid=True)

# Show the plot
plt.show()
281/62:

sum((input_files.index >='2019-01-01 00:00:00.0000') & (input_files.index < '2019-01-2 00:00:00.0000'))
281/63:

sum((input_files.index >='2019-01-02 00:00:00.0000') & (input_files.index < '2019-01-3 00:00:00.0000'))
281/64:

sum((input_files.index >='2019-01-03 00:00:00.0000') & (input_files.index < '2019-01-4 00:00:00.0000'))
281/65:

sum((input_files.index >='2019-01-06 00:00:00.0000') & (input_files.index < '2019-01-07 00:00:00.0000'))
281/66:

sum((input_files.index >='2019-01-13 00:00:00.0000') & (input_files.index < '2019-01-14 00:00:00.0000'))
281/67:

sum((input_files.index >='2019-01-14 00:00:00.0000') & (input_files.index < '2019-01-15 00:00:00.0000'))
284/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
284/2: lookbacks['100ms']
284/3: lookbacks['300ms']
284/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
284/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
284/6:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
input_files=df.copy()
284/7:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
284/8: input_files.info()
284/9: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
284/10:
list_columns=list(input_files)
list_columns
284/11: input_files.dtypes
284/12:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
284/13:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
284/14: input_files['ret_sec_1'].head()
284/15:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
284/16:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
284/17: input_files.dtypes
284/18:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
284/19: min(input_files.index)
284/20: max(input_files.index)
284/21:

sum((input_files.index >='2019-01-14 00:00:00.0000') & (input_files.index < '2019-01-15 00:00:00.0000'))
284/22:

input_files.loc[(input_files.index >='2019-01-01 00:00:00.0000') & (input_files.index < '2019-01-2 00:00:00.0000')].price
284/23: input_files.index[input_files.price==max(input_files.price[1:200000])]
284/24: df['sig_label_sec_1'].value_counts()
284/25: df['sig_label_sec_3'].value_counts()
290/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
290/2: lookbacks['100ms']
290/3: lookbacks['300ms']
290/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
290/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
290/6:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
input_files=df.copy()
290/7:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
290/8: input_files.info()
290/9: print("We have ",input_files.shape[1],"columns and", input_files.shape[0],"rows.")
290/10:
list_columns=list(input_files)
list_columns
290/11: input_files.dtypes
290/12:
g = input_files.columns.to_series().groupby(df.dtypes).groups
g
290/13:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
290/14: input_files['ret_sec_1'].head()
290/15: df['sig_label_sec_3'].value_counts()
290/16:
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
290/17:
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
290/18: input_files.dtypes
290/19:
sig = list(input_files.select_dtypes(include=['int8']).columns)
sig
290/20: min(input_files.index)
290/21:

sum((input_files.index >='2019-01-14 00:00:00.0000') & (input_files.index < '2019-01-15 00:00:00.0000'))
290/22:

sum((input_files.index >='2019-01-15 00:00:00.0000') & (input_files.index < '2019-01-16 00:00:00.0000'))
290/23:

sum((input_files.index >='2019-01-16 00:00:00.0000') & (input_files.index < '2019-01-17 00:00:00.0000'))
290/24:

sum((input_files.index >='2019-01-17 00:00:00.0000') & (input_files.index < '2019-01-18 00:00:00.0000'))
290/25:

sum((input_files.index >='2019-01-18 00:00:00.0000') & (input_files.index < '2019-01-19 00:00:00.0000'))
290/26:

sum((input_files.index >='2019-01-19 00:00:00.0000') & (input_files.index < '2019-01-20 00:00:00.0000'))
290/27:

sum((input_files.index >='2019-01-20 00:00:00.0000') & (input_files.index < '2019-01-21 00:00:00.0000'))
290/28:

sum((input_files.index >='2019-01-21 00:00:00.0000') & (input_files.index < '2019-01-22 00:00:00.0000'))
290/29:

sum((input_files.index >='2019-01-22 00:00:00.0000') & (input_files.index < '2019-01-23 00:00:00.0000'))
290/30:

sum((input_files.index >='2019-01-23 00:00:00.0000') & (input_files.index < '2019-01-24 00:00:00.0000'))
290/31:

sum((input_files.index >='2019-01-24 00:00:00.0000') & (input_files.index < '2019-01-25 00:00:00.0000'))
290/32:

sum((input_files.index >='2019-01-25 00:00:00.0000') & (input_files.index < '2019-01-26 00:00:00.0000'))
290/33:

sum((input_files.index >='2019-01-26 00:00:00.0000') & (input_files.index < '2019-01-27 00:00:00.0000'))
290/34:

sum((input_files.index >='2019-01-26 00:00:00.0000') & (input_files.index < '2019-01-27 00:00:00.0000'))
290/35:

sum((input_files.index >='2019-01-26 00:00:00.0000') & (input_files.index < '2019-01-27 00:00:00.0000'))
290/36:

sum((input_files.index >='2019-01-25 00:00:00.0000') & (input_files.index < '2019-01-26 00:00:00.0000'))
290/37:

sum((input_files.index >='2019-01-26 00:00:00.0000') & (input_files.index < '2019-01-27 00:00:00.0000'))
290/38:

sum((input_files.index >='2019-01-27 00:00:00.0000') & (input_files.index < '2019-01-28 00:00:00.0000'))
290/39:

sum((input_files.index >='2019-01-28 00:00:00.0000') & (input_files.index < '2019-01-29 00:00:00.0000'))
290/40:

sum((input_files.index >='2019-01-29 00:00:00.0000') & (input_files.index < '2019-01-30 00:00:00.0000'))
290/41:

sum((input_files.index >='2019-01-30 00:00:00.0000') & (input_files.index < '2019-01-31 00:00:00.0000'))
290/42:

sum((input_files.index >='2019-01-31 00:00:00.0000') )
290/43:

sum((input_files.index >='2019-01-30 00:00:00.0000') & (input_files.index < '2019-01-31 00:00:00.0000'))
290/44:

input_files.loc[(input_files.index >='2019-01-06 00:00:00.0000') & (input_files.index < '2019-01-7 00:00:00.0000')].price
290/45:

input_files.loc[(input_files.index >='2019-01-06 00:00:00.0000') & (input_files.index < '2019-01-07 00:00:00.0000')].price
290/46:

sum((input_files.index >='2019-01-07 00:00:00.0000') & (input_files.index < '2019-01-08 00:00:00.0000'))
290/47:

sum((input_files.index >='2019-01-05 00:00:00.0000') & (input_files.index < '2019-01-06 00:00:00.0000'))
290/48: df['sig_label_sec_3'].count()
290/49: df.shape()
290/50: df.shape
292/1:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
292/2: lookbacks['100ms']
292/3: lookbacks['300ms']
292/4:
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
292/5:
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler



import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
292/6:
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
input_files=df.copy()
292/7:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
292/8: input_files.info()
292/9:
#input_files = sorted(glob.glob(csv_filepath))
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
input_files.head(30)
292/10: df.price[0]
292/11: df.sig_label_sec_1[0]
293/1:

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
293/2: from sklearn.base import clone
293/3:
import pandas as pd
import numpy as np
import yaml 
import itertools
293/4:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
293/5:
##for my computer
path='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
293/6:
from sklearn import metrics,preprocessing
import pandas as pd
import re
import numpy as np
import os
import glob
import pickle
293/7:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'2.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.001)
df = df[:index]
293/8:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
#input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
293/9:
list_columns=list(input_files)
#list_columns
293/10:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
293/11:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.001)
df = df[:index]
293/12:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
#input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
293/13:
list_columns=list(input_files)
#list_columns
293/14:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.001)
input_files = df[:index]
293/15: input_files.shap
293/16: input_files.shape
293/17:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.01)
input_files = df[:index]
293/18: input_files.shape
293/19:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.1)
input_files = df[:index]
293/20: input_files.shape
293/21:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.001)
df = df[:index]
293/22:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
#input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
293/23:
list_columns=list(input_files)
#list_columns
293/24:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.1)
input_files = df[:index]
293/25: input_files.shape
293/26:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.1)
df = df[:index]
293/27: df.shape
293/28:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.05)
df = df[:index]
293/29: df.shape
293/30:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.05)
df = df[:12524]
293/31: df.shape
293/32:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.05)
df = df[:12525]
293/33: df.shape
293/34:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
#input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
293/35:
list_columns=list(input_files)
#list_columns
293/36:
##I only work with .1 percent of the data for the time being
from math import ceil
index = ceil(len(df)*0.1)
input_files = df[:index]
293/37:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
#input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
293/38:
list_columns=list(input_files)
#list_columns
293/39: input_files.shape
293/40:
##I only work with .1 percent of the data for the time being
##For my local computer
df = pd.read_hdf(path+'1.h5')

##for google cloud
#df = pd.read_hdf('1.h5')
from math import ceil
index = ceil(len(df)*0.05)
df = df[:17892]
293/41: df.shape
293/42:


#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
#input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
293/43:
list_columns=list(input_files)
#list_columns
293/44:
##I only work with .1 percent of the data for the time being
#from math import ceil
#index = ceil(len(df)*0.1)
#input_files = df[:index]
293/45: input_files.shape
293/46:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
293/47: input_files_train_contracts.shape
293/48:
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
293/49:
sig_lookback_periods = [
            "100ms", "300ms", "500ms", "1s", "2s", "3s", "4s", "5s", "7s", "9s",
            "12s", "18s", "21s"
        ]

inv_sig_lookback_periods = [
            "10s", "13s", "15s", "17s", "23s", "27s", "30s", "45s", "60s", "120s",
            "150s", "180s", "240s", "300s", "450s", "600s", "750s", "900s"
        ]

p = ["40", "30", "20"]

signals = {
            "MOVING_AVG": {"lookback_periods": sig_lookback_periods},
            "INV_MOVING_AVG": {"lookback_periods": inv_sig_lookback_periods},
            "CCI": {"lookback_periods": sig_lookback_periods},
            "INV_CCI": {"lookback_periods": inv_sig_lookback_periods},
            "RSI": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_RSI": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "CMO": {"lookback_periods": sig_lookback_periods},
            "INV_CMO": {"lookback_periods": inv_sig_lookback_periods},
            "STOCH": {"lookback_periods": sig_lookback_periods, "int_paras": p},
            "INV_STOCH": {"lookback_periods": inv_sig_lookback_periods, "int_paras": p},
            "BBI_2": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
            "BBI_3": {"lookback_periods": sig_lookback_periods + inv_sig_lookback_periods},
          }

ti = ["MOVING_AVG", "CCI", "RSI", "CMO", "STOCH", "BBI_2", "BBI_3"]
inv_ti = ["INV_MOVING_AVG", "INV_CCI", "INV_RSI", "INV_CMO", "INV_STOCH", "BBI_2", "BBI_3"]

lookbacks = {**{lb: {"ti": ti, "int_paras": p} for lb in sig_lookback_periods},
                     **{lb: {"ti": inv_ti, "int_paras": p} for lb in inv_sig_lookback_periods}}
293/50:
#we have 11 sets of features is created in features.py
#sets: MOVING_AVG
#CCI,RSI,CMO,STOCH,BBI_2,BBI_3,INV_MOVING_AVG,INV_CCI,INV_RSI,INV_CMO,INV_STOCH
#dic_features is  created in features.py

X = {} #empty dictionary that will contain the sets of features

for i in range(len(ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[ti[i]]:
        for j in signals[ti[i]]["lookback_periods"]:
            for k in signals[ti[i]]["int_paras"]:
                lf.append("sig_"+ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[ti[i]]["lookback_periods"]:
            lf.append("sig_"+ti[i]+"_"+j)
    X[ti[i]] = lf
for i in range(len(inv_ti)):
    lf = [] # list of feature names
    if "int_paras" in signals[inv_ti[i]]:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            for k in signals[inv_ti[i]]["int_paras"]:
                lf.append("sig_"+inv_ti[i]+"_"+j+"_"+k)
    else:
        for j in signals[inv_ti[i]]["lookback_periods"]:
            lf.append("sig_"+inv_ti[i]+"_"+j)
    X[inv_ti[i]] = lf

features_dic = X #dictionary of feature sets to be used for each model
293/51:
for feature, feature_set in features_dic.items():
    ##We need to run all the models in this for loop for all set of features. for the time being 
    ##we do not use it. but later we put every thing here.
    print(feature)

    ##the rest of the codes below will be here!
293/52: features_dic['INV_MOVING_AVG']
293/53:
#We do not need this cell when everything goes to the for loop above
feature = 'MOVING_AVG'
feature_set = features_dic[feature]
X_train = input_files_train_contracts[feature_set]
X_test = input_files_test_contracts[feature_set]
293/54:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,a
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
293/55:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
293/56:
rf_params={'bootstrap': False, 
           'class_weight': None,
            'criterion': 'entropy', 
        'max_depth': 8,
        'max_features':11,
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':431,
        'n_jobs':None, 
            'random_state':50
       }
293/57:
def acc_model(classifier):
    return cross_val_score(classifier, X_train, y_train,cv=my_cv).mean()
293/58:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
293/59:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
293/60:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
293/61:
rf_params={'bootstrap': True, 
           'class_weight': None,
            'criterion': 'gini', 
        'max_depth': None,
        'max_features':'auto',
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':'warn',
        'n_jobs':-1, 
            'random_state':50
           'verbose': 0,
        'warm_start': False
       }
293/62:
rf_params={'bootstrap': True, 
           'class_weight': None,
            'criterion': 'gini', 
        'max_depth': None,
        'max_features':'auto',
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':'warn',
        'n_jobs':-1, 
            'random_state':50,
           'verbose': 0,
        'warm_start': False
       }
293/63:
def acc_model(classifier):
    return cross_val_score(classifier, X_train, y_train,cv=my_cv).mean()
293/64:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
293/65:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
293/66:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
293/67: X_train.shape
293/68:
#save it as classifierwrapper.py and then: from  classifierwrapper import *
class ClassifierWrapperBase():
    def __init__(self):
        self.model = None
     
    def fit(self, X, y):
        self.model.fit(X,y)
        return None

    def predict(self, X):
        return self.model.predict(X)

   # def clone(self):
    #    self.model = clone(self.model)
    #    return self

    def save(self, file_path):
        print("saving model to " + file_path)
        with open(file_path, 'wb') as handle:
            pickle.dump(self.model, handle)

    def load(self, file_path):
        print("loading model from " + file_path)
        with open(file_path, 'rb') as handle:
            self.model = pickle.load(handle)
#------------------------------------------------------------------------------
class RandomForestClassifierWrapper(ClassifierWrapperBase):
    def __init__(self,
                 n_estimators='warn',
                 criterion="gini",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 bootstrap=True,
                 oob_score=False,
                 n_jobs=None,
                 random_state=None,
                 verbose=0,
                 warm_start=False,
                 class_weight=None
                 ):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
            warm_start=warm_start,
            class_weight=class_weight
        )


#---------------------------------------------------------------------------------
293/69:
rf_params={'bootstrap': True, 
           'class_weight': None,
            'criterion': 'gini', 
        'max_depth': None,
        'max_features':'auto',
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':'warn',
        'n_jobs':-1, 
            'random_state':50,
           'verbose': 0,
        'warm_start': False
       }
293/70:
def acc_model(classifier):
    return cross_val_score(classifier, X_train, y_train,cv=my_cv).mean()
293/71:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
293/72:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")

print("Done!")
293/73:
from sklearn import metrics
#how to load saved models

for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.load(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))
print("Done!")
293/74:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))

print("Done!")
293/75:
rf_params={'bootstrap': True, 
           'class_weight': None,
            'criterion': 'gini', 
        'max_depth': None,
        'max_features':'auto',
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':3, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':'warn',
        'n_jobs':-1, 
            'random_state':50,
           'verbose': 0,
        'warm_start': False
       }
293/76:
def acc_model(classifier):
    return cross_val_score(classifier, X_train, y_train,cv=my_cv).mean()
293/77:

model_set = {
    "rf": {
        "clf": RandomForestClassifierWrapper(**rf_params),
        "run": True #run the model or fit the saved model
    }
}#end of model set
293/78:
for k, v in model_set.items():
    if v["run"] is False:
        continue
    model_name = k
    saved_folder = path+"/saved_models/" + model_name
    tc = v["clf"]
    tc.fit(X_train,y_train)
    #scores = tc.cross_validate(X_, y, fold_num, saved_folder=saved_folder)
    tc.save(saved_folder+".model")
    y_pred = tc.predict(X_test)
    print("Accuracy", metrics.accuracy_score(y_test, y_pred))

print("Done!")
293/79:
rf_params={'bootstrap': True, 
           'class_weight': None,
            'criterion': 'gini', 
        'max_depth': None,
        'max_features':'auto',
            'max_leaf_nodes':None, 
        'min_impurity_decrease':0.0,
            'min_impurity_split':None,
        'min_samples_leaf':1,
            'min_samples_split':2, 
        'min_weight_fraction_leaf':0.0,
            'n_estimators':'warn',
        'n_jobs':-1, 
            'random_state':50,
           'verbose': 0,
        'warm_start': False
       }
294/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
294/2:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
294/3:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
#path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
#df = pd.read_hdf(path_to_data+'1.h5')
#removing zeros data is imbalanced
##for google cloud
df = pd.read_hdf('1.h5')
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
294/4:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50)
best_bayes_model.fit(X_train, y_train)
294/5:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
294/6:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
#path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
#df = pd.read_hdf(path_to_data+'1.h5')
#removing zeros data is imbalanced
##for google cloud
df = pd.read_hdf('1.h5')
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
294/7:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.001)
input_files = df[:index]
294/8:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
my_cv = TimeSeriesSplit(n_splits=10)
294/9:
clf=RandomForestClassifier()
clf.get_params()
294/10:
# File to save first results
import csv
##for kaggle and my local computer
#out_file = path_to_data+'RF_trials.csv'
#for google cloud
out_file ='RF_trials.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write the headers to the file
writer.writerow(['loss', 'params', 'iteration'])
of_connection.close()
294/11:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()
param_space = {'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"]),
    'bootstrap': hp.choice('bootstrap', [True, False])
              }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=10, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
294/12:

#beccause the following error, I read the results from the csv file
#https://github.com/hyperopt/hyperopt/issues/431
results = pd.read_csv('RF_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = True, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
294/13: best
294/14:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50)
best_bayes_model.fit(X_train, y_train)
294/15:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
294/16: X_train.shape
294/17:

#beccause the following error, I read the results from the csv file
#https://github.com/hyperopt/hyperopt/issues/431
results = pd.read_csv('RF_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = True, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
294/18: best
294/19:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50)
best_bayes_model.fit(X_train, y_train)
294/20:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
294/21:
best_bayes_model =  RandomForestClassifier(**best, random_state = 50)
best_bayes_model.fit(X_train, y_train)
294/22: best_bayes_params
294/23:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50)
best_bayes_model.fit(X_train, y_train)
294/24: best
294/25:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()
param_space = {'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"]),
    'bootstrap': hp.choice('bootstrap', [True, False])
              }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=100, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
294/26:

#beccause the following error, I read the results from the csv file
#https://github.com/hyperopt/hyperopt/issues/431
results = pd.read_csv('RF_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = True, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
294/27: best
294/28: best_bayes_params
294/29:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50)
best_bayes_model.fit(X_train, y_train)
294/30:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
294/31:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
#from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
294/32:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
#path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
#df = pd.read_hdf(path_to_data+'1.h5')
#removing zeros data is imbalanced
##for google cloud
df = pd.read_hdf('1.h5')
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
294/33:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.001)
input_files = df[:index]
df = df[:17892]
294/34:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
my_cv = TimeSeriesSplit(n_splits=10)
294/35: X_train.shape
294/36:
clf=RandomForestClassifier()
clf.get_params()
294/37:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
#path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'dataset/2.h5')
#input_files=df.copy()
##For my local computer
#df = pd.read_hdf(path_to_data+'1.h5')
#removing zeros data is imbalanced
##for google cloud
df = pd.read_hdf('1.h5')
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#---------------------------------------------------------------------------------
input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
294/38:
##I only work with .1 percent of the data for the time being
#index = ceil(len(df)*0.001)
#input_files = df[:index]
input_files = df[:17892]
294/39:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
my_cv = TimeSeriesSplit(n_splits=10)
294/40: X_train.shape
294/41:
clf=RandomForestClassifier()
clf.get_params()
294/42:
# File to save first results
import csv
##for kaggle and my local computer
#out_file = path_to_data+'RF_trials.csv'
#for google cloud
out_file ='RF_trials.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write the headers to the file
writer.writerow(['loss', 'params', 'iteration'])
of_connection.close()
294/43:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()
param_space = {'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,500)),
    'criterion': hp.choice('criterion', ["gini", "entropy"]),
    'bootstrap': hp.choice('bootstrap', [True, False])
              }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=100, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
294/44:

#beccause the following error, I read the results from the csv file
#https://github.com/hyperopt/hyperopt/issues/431
results = pd.read_csv('RF_trials.csv')
# Sort with best scores on top and reset index for slicing
results.sort_values('loss', ascending = True, inplace = True)
results.reset_index(inplace = True, drop = True)
import ast

# Convert from a string to a dictionary
ast.literal_eval(results.loc[0, 'params'])
# Extract the ideal  hyperparameters
best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()

# Re-create the best model and train on the training data
#best_bayes_model =  RandomForestClassifier(#n_estimators=best_bayes_estimators,
#                       n_jobs = -1, random_state = 50, **best_bayes_params)
294/45: best
294/46: best_bayes_params
294/47:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50)
best_bayes_model.fit(X_train, y_train)
294/48:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
294/49:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()
param_space = {'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,1000)),
    'criterion': hp.choice('criterion', ["gini", "entropy"]),
    'bootstrap': hp.choice('bootstrap', [True, False])
              }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
    print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=1000, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
294/50:
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning begins-----------------
#---------------------------------------------------------------------------------
def acc_model(params):
    clf = RandomForestClassifier(**params, random_state = 50)
    return cross_val_score(clf, X_train, y_train,cv=my_cv).mean()
param_space = {'max_depth': hp.choice('max_depth', range(1,20)),
    'max_features': hp.choice('max_features', range(1,n_features)),
    'n_estimators': hp.choice('n_estimators', range(100,1000)),
    'criterion': hp.choice('criterion', ["gini", "entropy"]),
    'bootstrap': hp.choice('bootstrap', [True, False])
              }
#---------------------------------------------------------------------------------
best = 0
ITERATION = 0
def f(params):
    # Keep track of evals
    global ITERATION
    
    ITERATION += 1
    global best
    acc = acc_model(params)
    loss = -acc
    # Write to the csv file ('a' means append)
    of_connection = open(out_file, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, params, ITERATION])
    if acc > best:
        best = acc
        print ('new acc:', best, params)
    #since fmin minizize, minimizing -acc is the same as maximizing acc
    return {'loss': -acc, 'status': STATUS_OK}

trials = Trials()
#fmin returns a dictionary of the form param_space
best = fmin(f, param_space, algo=tpe.suggest, max_evals=1000, trials=trials)
print ('best:')
print (best)
#---------------------------------------------------------------------------------
#---------------------------Random Forest Bayesian Tunning ends-----------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
#---------------------------------------------------------------------------------
294/51: best={'bootstrap': True, 'criterion': 'entropy', 'max_depth': 18, 'max_features': 1, 'n_estimators': 220}
294/52: best_bayes_params=best
294/53:
best_bayes_model =  RandomForestClassifier(**best_bayes_params, random_state = 50)
best_bayes_model.fit(X_train, y_train)
294/54:
y_pred = best_bayes_model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
297/1:
from hyperopt import fmin
import datetime

import numpy as np
import pandas as pd
# import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier,
                              GradientBoostingClassifier,
                              ExtraTreesClassifier
)
from xgboost import XGBClassifier
#from lightgbm import LGBMClassifier

from sklearn.externals import joblib
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import hp
from hyperopt import Trials
from hyperopt import tpe
from sklearn.model_selection import TimeSeriesSplit
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import glob
from math import ceil
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
297/2:
#---------------------------------------------------------------------------------
##for Kaggle
#import os
#print(os.listdir("../input"))
#path_to_data='../input/'

##for my computer
path_to_data='/Users/shahla/Dropbox/MLCourse/ML1030/Project/'
#---------------------------------------------------------------------------------
##For Kaggle
#df = pd.read_hdf(path_to_data+'merged/merged-data.h5')

##For my local computer
df = pd.read_hdf(path_to_data+'1.h5')
#-----------------------------------------------
#removing zeros data is imbalanced
df = df.drop(df[df.sig_label_sec_1 ==0].index)
#--------------------------------------------

input_files=df.copy()
#---------------------------------------------------------------------------------
list_columns=list(input_files)
#---------------------------------------------------------------------------------
#extracting features that needs to be categorical
sig = list(input_files.select_dtypes(include=['int8']).columns)
# -1 is replaced with 0
# 0 is  replaced with 1
# 1 is replaced with 2
input_files[sig] = input_files[sig].apply(lambda x: x+1)
#Cast a pandas object to a specified dtype 
categories=[0, 1, 2]
input_files[sig] = input_files[sig].astype('category', categories = categories)
#---------------------------------------------------------------------------------
297/3:
##I only work with .1 percent of the data for the time being
index = ceil(len(df)*0.0001)
input_files = df[:index]
297/4:
print("Creating train/test split of data...")
training_index = ceil(len(input_files)*0.7)
input_files_train_contracts = input_files[:training_index]
input_files_test_contracts = input_files[training_index:]
sig_MOVING_AVG=['sig_MOVING_AVG_100ms',
 'sig_MOVING_AVG_300ms',
 'sig_MOVING_AVG_500ms',
 'sig_MOVING_AVG_1s',
 'sig_MOVING_AVG_2s',
 'sig_MOVING_AVG_3s',
 'sig_MOVING_AVG_4s',
 'sig_MOVING_AVG_5s',
 'sig_MOVING_AVG_7s',
 'sig_MOVING_AVG_9s',
 'sig_MOVING_AVG_12s',
 'sig_MOVING_AVG_18s',
 'sig_MOVING_AVG_21s']
X_train = input_files_train_contracts[sig_MOVING_AVG]
X_test=input_files_test_contracts[sig_MOVING_AVG]
y_train=input_files_train_contracts['sig_label_sec_1']
y_test=input_files_test_contracts['sig_label_sec_1']
#---------------------------------------------------------------------------------
n_features=len(sig_MOVING_AVG)
#---------------------------------------------------------------------------------
my_cv = TimeSeriesSplit(n_splits=10)
301/1:
from tweepy import Stream
import pandas as pd
301/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
301/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
301/4:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
301/5:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
301/6:
query = 'politics'
data_dir = '/Users/shahla/Documents/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
301/7:

df = pd.read_json("/Users/shahla/Documents/SharpestMinds/stream/data/stream_politics.json",lines=True)
301/8: df.shape
301/9:
query = 'politics'
data_dir = '/Users/shahla/Documents/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
301/10:

df = pd.read_json("/Users/shahla/Documents/SharpestMinds/stream/data/stream_politics.json",lines=True)
301/11: df.shape
301/12:
list_columns=list(df.columns)
list_columns
301/13: df.created_at[1:10]
301/14:
#keep English languages
data=df[df.lang=='en']
301/15: data.shape
301/16: data.truncated[21]
301/17:

data.text[21]
301/18: data.retweeted_status[32270]
301/19: data.retweeted_status[21]
301/20: data.retweeted_status[31]['extended_tweet']['full_text']
301/21: data.retweeted_status[31]['extended_tweet']
301/22: data.retweeted_status[31]
301/23: tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
301/24:
for i in data.index:   
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweets.tweet[i] = data.text[i]
                    tweets.time[i] = data.created_at[i]
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                     tweets.tweet[i]=data.extended_tweet[i]["full_text"]
                     tweets.time[i] = data.created_at[i]
                    
                else:
                     tweets.tweet[i]=data.text[i] 
                     tweets.time[i] = data.created_at[i]
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    tweets.time[i] = data.created_at[i]
            else:
                 tweets.tweet[i] = data.retweeted_status[i]['text']
                 tweets.time[i] = data.created_at[i]
                    
       #print( tweets.tweet[i])
       #print('----------------\n')
301/25: tweets = tweets.sort_values('time', ascending=False)
301/26: tweets=tweets.drop_duplicates()
301/27: sum(tweets.duplicated())
301/28: print('The time range tweets written is from', min(tweets.time),'to', max(tweets.time))
301/29: tweets.to_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5', key='df', mode='w')
301/30: tweets.to_hdf('/Users/shahla/Documents/SharpestMinds/tweets.h5', key='df', mode='w')
301/31: tweets = pd.read_hdf('/Users/shahla/Dropbox/MLCourse/SharpestMinds/tweets.h5')
301/32: tweets = pd.read_hdf('/Users/shahla/Documents/SharpestMinds/tweets.h5')
301/33: tweets.head()
301/34: tweets.isnull().sum().sum()
301/35:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
301/36:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
301/37:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
301/38:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl=' ',string=text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
301/39:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
301/40:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl=' ',string=text)
    re.sub(r"#[a-zA-Z\-]+", "", text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
301/41:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
301/42:
text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
remove_hashtags(text)
301/43:
import re
def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
    re.sub(r"#[a-zA-Z\-]+", "", text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
301/44:
text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
remove_hashtags(text)
301/45:
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text

sample = denoise_text(sample)
print(sample)
301/46:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en #install it in the terminal

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
301/47:
wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I)
    doc = doc.lower()
    doc = doc.strip()
    # tokenize document
    tokens = wpt.tokenize(doc)
    # filter stopwords out of document
    filtered_tokens = [token for token in tokens if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(normalize_document)
301/48:
corpus = ['The sky is blue and beautiful.',
          'Love this blue and beautiful sky!',
          'The quick brown fox jumps over the lazy dog.',
          'The brown fox is quick and the blue dog is lazy!',
          'The sky is very blue and the sky is very beautiful today',
          'The dog is lazy but the brown fox is quick!'    
]
norm_corpus = normalize_corpus(corpus)
norm_corpus
301/49: df.text[2]
301/50: #normalize_corpus(tweets.tweet[145])
301/51:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[11:12])
301/52: x
301/53: print(tweets.tweet[11:12])
301/54:
pd.set_option('display.max_colwidth', -1)
df.text[0][1:100]
301/55: df.text[0][100:300]
301/56: x[9]
301/57:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ # ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
301/58:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text,contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
301/59:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
301/60:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
303/1:
from tweepy import Stream
import pandas as pd
303/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
303/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
303/4:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
303/5:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
303/6:

df = pd.read_json("/Users/shahla/Documents/SharpestMinds/stream/data/stream_politics.json",lines=True)
303/7: tweets = pd.read_hdf('/Users/shahla/Documents/SharpestMinds/tweets.h5')
303/8: tweets.head()
303/9: tweets.isnull().sum().sum()
303/10:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
303/11:
import re

def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
    re.sub(r"#[a-zA-Z\-]+", "", text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
303/12:
text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
remove_hashtags(text)
303/13:
text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
remove_hashtags(text)
303/14:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
303/15:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
303/16:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
303/17:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor(tweets.tweet[12])

p.fully_preprocess()
print(p.text)
303/18:
text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
remove_hashtags(text)
303/19:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
303/20:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor(tweets.tweet[12])

p.fully_preprocess()
print(p.text)
303/21:
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text

sample = denoise_text(tweets.tweet[12])
print(sample)
303/22:
import spacy
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py
from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer
import unicodedata

#python3 -m spacy download en #install it in the terminal

nlp = spacy.load('en', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
303/23:
wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I)
    doc = doc.lower()
    doc = doc.strip()
    # tokenize document
    tokens = wpt.tokenize(doc)
    # filter stopwords out of document
    filtered_tokens = [token for token in tokens if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(normalize_document)
303/24:
corpus = ['The sky is blue and beautiful.',
          'Love this blue and beautiful sky!',
          'The quick brown fox jumps over the lazy dog.',
          'The brown fox is quick and the blue dog is lazy!',
          'The sky is very blue and the sky is very beautiful today',
          'The dog is lazy but the brown fox is quick!'    
]
norm_corpus = normalize_corpus(corpus)
norm_corpus
303/25:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[11:12])
303/26: x
303/27: print(tweets.tweet[11:12])
303/28: df.text[0][100:300]
303/29: x[9]
303/30: x[1]
303/31: x[0]
303/32:
t =twitter.Api(
    consumer_key = creds['CONSUMER_KEY'],
    consumer_secret = creds['CONSUMER_SECRET'],
    access_token_key = creds['ACCESS_TOKEN'], 
    access_token_secret = creds['ACCESS_SECRET'],
    tweet_mode='extended' # this ensures that we get the full text of the users' original tweets
)
303/33:
import
t =twitter.Api(
    consumer_key = creds['CONSUMER_KEY'],
    consumer_secret = creds['CONSUMER_SECRET'],
    access_token_key = creds['ACCESS_TOKEN'], 
    access_token_secret = creds['ACCESS_SECRET'],
    tweet_mode='extended' # this ensures that we get the full text of the users' original tweets
)
303/34:
import twitter
t =twitter.Api(
    consumer_key = creds['CONSUMER_KEY'],
    consumer_secret = creds['CONSUMER_SECRET'],
    access_token_key = creds['ACCESS_TOKEN'], 
    access_token_secret = creds['ACCESS_SECRET'],
    tweet_mode='extended' # this ensures that we get the full text of the users' original tweets
)
306/1:
from tweepy import Stream
import pandas as pd
306/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
306/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
306/4:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
306/5:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
306/6:

df = pd.read_json("/Users/shahla/Documents/SharpestMinds/stream/data/stream_politics.json",lines=True)
306/7: df.shape
306/8:
list_columns=list(df.columns)
list_columns
306/9: tweets = pd.read_hdf('/Users/shahla/Documents/SharpestMinds/tweets.h5')
306/10:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
306/11:
import re

def remove_url(text):
    return re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
def remove_mentions(text): 
     return re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
def remove_hashtags(text):
    return re.sub(pattern=re.compile(r'#*'),repl='',string=text)
    re.sub(r"#[a-zA-Z\-]+", "", text)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_twitter_reserved_words(text):
    return re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=text)
def remove_single_letter_words(text):
    return re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
def remove_blank_spaces(text):
    return re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
def remove_stopwords(text,extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(' '+w)
        text = ' '.join(new_sentence)
        return text
306/12:
text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
remove_hashtags(text)
306/13:
text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
text=remove_hashtags(text)
remove_url(text)
306/14: tweets.tweet[13]
306/15: tweets.tweet[14]
306/16: tweets.tweet[15]
306/17: tweets.tweet[16]
306/18: tweets.tweet[19]
306/19:
text=tweets.tweet[19]
text
306/20:
#text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
text=remove_hashtags(text)
306/21:
#text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
remove_hashtags(text)
306/22:
text=tweets.tweet[19]
text
306/23:
#text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
remove_hashtags(text)
306/24: remove_url(remove_hashtags(text))
306/25: tweets.tweet[20]
306/26: tweets.tweet[21]
306/27: tweets.tweet[22]
306/28: tweets.tweet
306/29: tweets.tweet[20:40]
306/30: tweets.tweet[30:40]
306/31: tweets.tweet[40:60]
306/32: tweets.tweet[40]
306/33: tweets.tweet[41]
306/34:
text=tweets.tweet[41]
text
306/35:
#text='RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.'
remove_hashtags(text)
306/36: remove_url(remove_hashtags(text))
306/37: remove_stopwords(remove_url(remove_hashtags(text)))
306/38: remove_mentions(remove_stopwords(remove_url(remove_hashtags(text))))
306/39:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=sel.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/40:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
306/41:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/42:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
306/43: TwitterPreprocessor(text).fully_preprocess()
306/44: print(TwitterPreprocessor(text).fully_preprocess())
306/45: print(TwitterPreprocessor(text).fully_preprocess().text)
306/46:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split('')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ''.join(text_list)
        return self
306/47:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split('')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ''.join(text_list)
        return self
306/48: print(TwitterPreprocessor(text).fully_preprocess().text)
306/49:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ''.join(text_list)
        return self
306/50: print(TwitterPreprocessor(text).fully_preprocess().text)
306/51:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/52: print(TwitterPreprocessor(text).fully_preprocess().text)
306/53:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/54: print(TwitterPreprocessor(text).fully_preprocess().text)
306/55:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
#             .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/56: print(TwitterPreprocessor(text).fully_preprocess().text)
306/57:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
306/58:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
#             .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/59:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
#           .remove_blank_spaces() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/60:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self

    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'(RT|rt|FAV|fav|VIA|via)'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/61: print(TwitterPreprocessor(text).fully_preprocess().text)
306/62: print(TwitterPreprocessor('20 April #ddd www.sss.com').fully_preprocess().text)
306/63: print(TwitterPreprocessor(text).fully_preprocess().text)
306/64:

from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[11:12])
306/65:
from text_normalizer import normalize_corpus
x=normalize_corpus(TwitterPreprocessor(text).fully_preprocess().text)
306/66:
from text_normalizer import normalize_corpus
x=normalize_corpus(TwitterPreprocessor(text).fully_preprocess().text)
x
306/67:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/68:
x=normalize_corpus([text])
x
306/69: tweets.tweet[12]
306/70:
text=tweets.tweet[12]
print(tetx)
306/71:
text=tweets.tweet[12]
print(text)
306/72:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/73:
text=tweets.tweet[13]
print(text)
306/74:
text=tweets.tweet[16]
print(text)
306/75:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/76: print(TwitterPreprocessor(text).fully_preprocess().text)
306/77:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/78:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self

    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/79: print(TwitterPreprocessor(text).fully_preprocess().text)
306/80:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/81:
text=tweets.tweet[17]
print(text)
306/82: print(TwitterPreprocessor(text).fully_preprocess().text)
306/83:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/84:
text=tweets.tweet[21]
print(text)
306/85: print(TwitterPreprocessor(text).fully_preprocess().text)
306/86:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/87:
text=tweets.tweet[31]
print(text)
306/88: print(TwitterPreprocessor(text).fully_preprocess().text)
306/89:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/90:
text=tweets.tweet[51]
print(text)
306/91: print(TwitterPreprocessor(text).fully_preprocess().text)
306/92:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/93:
text=tweets.tweet[61]
print(text)
306/94: print(TwitterPreprocessor(text).fully_preprocess().text)
306/95:
text=tweets.tweet[71]
print(text)
306/96: print(TwitterPreprocessor(text).fully_preprocess().text)
306/97:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/98:
text=tweets.tweet[81]
print(text)
306/99: print(TwitterPreprocessor(text).fully_preprocess().text)
306/100:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/101: text.replace("-","")
306/102:
text=tweets.tweet[81]
print(text)
306/103: text.replace("-"," ")
306/104: text="I am text_lala text_boo"
306/105: text.replace("-"," ")
306/106: text="I am text-lala text-boo"
306/107: text.replace("-"," ")
306/108:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers()

    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/109:
text=tweets.tweet[81]
print(text)
306/110: print(TwitterPreprocessor(text).fully_preprocess().text)
306/111:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers()
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/112:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .remove_hyphens()
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/113: print(TwitterPreprocessor(text).fully_preprocess().text)
306/114:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/115:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
306/116:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/117: print(TwitterPreprocessor(text).fully_preprocess().text)
306/118:
text=tweets.tweet[91]
print(text)
306/119: text.replace("-"," ")
306/120:
text=tweets.tweet[91]
print(text)
306/121: print(TwitterPreprocessor(text).fully_preprocess().text)
306/122:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/123:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
306/124:
text=tweets.tweet[101]
print(text)
306/125:
text=tweets.tweet[101]
print(text)
306/126:
text=tweets.tweet[100]
print(text)
306/127: print(TwitterPreprocessor(text).fully_preprocess().text)
306/128:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
306/129:
text=tweets.tweet[101]
print(text)
306/130: tweets[101]
306/131: tweets[101:102]
306/132: tweets.isnull().sum().sum()
306/133: tweets[101:106]
306/134: tweets[100:110]
306/135: tweets[99:110]
307/1: runfile('/Users/shahla/Documents/SharpestMinds/untitled0.py', wdir='/Users/shahla/Documents/SharpestMinds')
307/2: runfile('/Users/shahla/Documents/SharpestMinds/untitled0.py', wdir='/Users/shahla/Documents/SharpestMinds')
306/136: tweets.index[0]
306/137: tweets.index[101]
306/138: tweets.index[101]
306/139: tweets.time[101]
306/140: tweets.index[101]
306/141:
from tweepy import Stream
import pandas as pd
306/142: tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
306/143:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
306/144:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
306/145:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
306/146:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
306/147:

df = pd.read_json("/Users/shahla/Documents/SharpestMinds/stream/data/stream_politics.json",lines=True)
306/148: df.shape
306/149:
list_columns=list(df.columns)
list_columns
306/150: df.created_at[1:10]
306/151:
#keep English languages
data=df[df.lang=='en']
306/152: data.shape
308/1:
from tweepy import Stream
import pandas as pd
308/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
308/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
308/4:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
308/5:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
309/1:
from tweepy import Stream
import pandas as pd
309/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
309/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
309/4:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
309/5:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
309/6:
query = 'politics'
data_dir = '/Users/shahla/Documents/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query])
308/6:
from tweepy import Stream
import pandas as pd
308/7:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
308/8:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
308/9:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
308/10:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
308/11: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
308/12: tweets.head()
308/13: tweets.index[101]
308/14: tweets.isnull().sum().sum()
308/15: tweets.shape
308/16: tweets.time[101]
308/17: tweets.time[102]
308/18: tweets.time[100]
308/19: tweets.time[101]
308/20: tweets.iloc[100]
308/21: tweets.iloc[100].type
308/22: tweets.iloc[100].time
308/23:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
308/24:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
308/25: tweets[99:110]
308/26: tweets.iloc[101].text
308/27: tweets.iloc[101]
308/28: tweets.iloc[101].tweet
308/29:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
308/30:
from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor(tweets.tweet[12])

p.fully_preprocess()
print(p.text)
308/31:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
308/32: tweets[99:110]
308/33: tweets.iloc[101].tweet
308/34:
text=tweets.tweet[101]
print(text)
308/35: print(TwitterPreprocessor(text).fully_preprocess().text)
308/36:
from text_normalizer import normalize_corpus
x=normalize_corpus([TwitterPreprocessor(text).fully_preprocess().text])
x
308/37:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
308/38:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile(r'RT @'),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
308/39: tweets[99:110]
308/40:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
308/41:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
308/42: tweets[99:110]
308/43: tweets.iloc[101].tweet
308/44:
text=tweets.tweet[101]
print(text)
308/45:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
308/46:
# from twitter_preprocessor import TwitterPreprocessor

# p = TwitterPreprocessor(tweets.tweet[12])

# p.fully_preprocess()
# print(p.text)
308/47: tweets.tweet[12]
308/48:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        replace('RT',self.text)
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
308/49:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
308/50:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        self.text.replace('RT','')
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
308/51:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
308/52:
s=re.sub('rt','','RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')
print(s)
308/53:
s=re.sub('RT','','RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')
print(s)
308/54:
s=re.sub('RT','','RT @ptwist .. @ RT #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')
print(s)
308/55:
s=re.sub('RT','','RT @ptwist .. @ RT #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.',flags=re.IGNORECASE)
print(s)
308/56:
s=re.sub('RT','','RT @ptwist .. @ rt #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.',flags=re.IGNORECASE)
print(s)
308/57:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
308/58:
#from twitter_preprocessor import TwitterPreprocessor

p = TwitterPreprocessor('RT @ptwist .. @ #ff,This text contains mentions, urls, some Twitter words and some stopwords to be preprocessed via https://example.com.')

p.fully_preprocess()
print(p.text)
310/1: import re
310/2: text='RT is rt blah blah blue 12'
310/3: re.sub('RT','',text)
310/4: re.sub('RT','',text,flags=re.IGNORECASE)
310/5: re.sub('^b','B',text)
310/6: re.sub('^b','c',text)
310/7: re.sub('^blah','c',text)
310/8: re.sub(^'b','c',text)
310/9: re.sub('^b','c',text)
310/10: re.sub('b','c',text)
310/11: text='RT is rt blah blah blue 12 klab'
310/12: text='RT is rt blah blah blue 12 ambrella'
310/13: re.sub('RT','',text)
310/14: re.sub('RT','',text,flags=re.IGNORECASE)
310/15: re.sub('^b','c',text)
310/16: re.sub('^blah','c',text)
310/17: re.sub('blah^','c',text)
310/18: re.sub('^blah','c',text)
308/59: tweets.tweet[12]
308/60:
#I use text_normalizer class created in my  folder
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.iloc[11:12].tweet)
308/61: x
308/62:
#I use text_normalizer class created in my  folder
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.iloc[11:14].tweet)
308/63: x
308/64:
#I use text_normalizer class created in my  folder
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.iloc[100:120].tweet)
308/65: x
308/66: tweets.text[0][1:100]
308/67: tweets.tweet[0][1:100]
308/68: x[0]
308/69: import gensim #downloaded pretrained model
308/70: x[1]
308/71: x[10]
308/72: tweets=[x[1].split(),x[10].split()]
308/73:
tweets=[x[1].split(),x[10].split()]
tweets
308/74: tweets=[x[1].split(),x[10].split()]
308/75:

filtered_tweets = [[word for word in tweet if word in model.vocab] for tweet in tweets];
308/76: model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
308/77: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
308/78: tweets.shape
308/79: tweets.head()
308/80: tweets.iloc[100].time
308/81: tweets.time[101]
308/82: tweets.isnull().sum().sum()
308/83:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
308/84:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
308/85: texts=tweets.tweet
308/86: texts
308/87:
#from twitter_preprocessor import TwitterPreprocessor

tweets['processed']= TwitterPreprocessor(tweets.tweet).fully_preprocess().text

#p.fully_preprocess()
#print(p.text)
308/88:
#from twitter_preprocessor import TwitterPreprocessor

tweets.processed=tweets.tweet.apply(lambda tweet: TwitterPreprocessor(tweet).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
308/89: tweets.columns
308/90:
#creating a new column
tweets.assign(C="")
308/91: tweets.drop('C')
308/92: tweets.drop(columns=['C'])
308/93: tweets.columns
308/94:
#from twitter_preprocessor import TwitterPreprocessor

tweets.assign(processed, tweets.tweet.apply(lambda tweet: TwitterPreprocessor(tweet).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
308/95:
#from twitter_preprocessor import TwitterPreprocessor

tweets.assign(processed, lambda tweets: TwitterPreprocessor(tweets.tweet).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
308/96:
#from twitter_preprocessor import TwitterPreprocessor

tweets.assign('processed', lambda tweets: TwitterPreprocessor(tweets.tweet).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
308/97:
df = pd.DataFrame({'temp_c': [17.0, 25.0]},
...                   index=['Portland', 'Berkeley'])
308/98:
df = pd.DataFrame({'temp_c': [17.0, 25.0]},
...                   index=['Portland', 'Berkeley'])
df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)
308/99:
#from twitter_preprocessor import TwitterPreprocessor

tweets.assign(processed=lambda tweets: TwitterPreprocessor(tweets.tweet).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
308/100:
df = pd.DataFrame({'temp_c': ['sh www.home.cm, rt #ff', 'djdd # rt via bus']},
...                   index=['Portland', 'Berkeley'])
308/101:
df = pd.DataFrame({'temp_c': ['sh www.home.cm, rt #ff', 'djdd # rt via bus']},
...                   index=['Portland', 'Berkeley'])
df
308/102: df.assign(temp_f=lambda x: TwitterPreprocessor(x.temp_c).fully_preprocess().text )
308/103: df.loc[:,'new column'] = df.temp_c.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
308/104: df
308/105: df.loc[:,'new column'] = df.temp_c.apply(lambda x:normalize_corpus(TwitterPreprocessor(x).fully_preprocess().text)
308/106: df.loc[:,'new column'] = df.temp_c.apply(lambda x: normalize_corpus(TwitterPreprocessor(x).fully_preprocess().text)
308/107: df.loc[:,'new_column'] = df.temp_c.apply(lambda x: normalize_corpus(TwitterPreprocessor(x).fully_preprocess().text)
308/108: df.loc[:,'new_column'] = df.temp_c.apply(lambda x: normalize_corpus(TwitterPreprocessor(x).fully_preprocess().text)
308/109: df.loc[:,'new_column'] = df.temp_c.apply(lambda x: normalize_corpus(TwitterPreprocessor(x).fully_preprocess().text))
308/110: df
308/111: df
308/112: normalize_corpus(df['new columns'])
308/113:
#I use text_normalizer class created in my  folder
from text_normalizer import normalize_corpus

x=normalize_corpus(tweets.iloc[100:120].tweet)
308/114:
#I use text_normalizer class created in my  folder
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.iloc[100:120].tweet)
308/115: x
308/116: tweets.iloc[100:120].tweet
308/117:
#I use text_normalizer class created in my  folder
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[1:10])
308/118: x
308/119:
#I use text_normalizer class created in my  folder
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet)
308/120:
#I use text_normalizer class created in my  folder
from text_normalizer import normalize_corpus
x=normalize_corpus(tweets.tweet[1:10])
308/121: df.loc[:,'new_column'] = df.temp_c.apply(lambda x: (TwitterPreprocessor(x).fully_preprocess().text))
308/122: df.loc[:,'new_column'] = df.temp_c.apply(lambda x: (TwitterPreprocessor(x).fully_preprocess().text))
308/123:
df.loc[:,'new_column'] = df.temp_c.apply(lambda x: (TwitterPreprocessor(x).fully_preprocess().text))
df
308/124:
df.loc[:,'new_column'] = df.temp_c.apply(lambda x: (TwitterPreprocessor(x).fully_preprocess().text))
df.loc[:,'new_column2']=normalize_corpus(df.new_column)
308/125: df
311/1:
from tweepy import Stream
import pandas as pd
311/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
311/3:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
311/4:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
311/5:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
311/6:

df = pd.read_json("/Users/shahla/Dropbox/SharpestMinds/stream/data/stream_politics.json",lines=True)
311/7:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
311/8:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
311/9: tweets[99:110]
311/10: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
311/11: tweets.shape
311/12: tweets.head()
311/13: tweets.iloc[100].time
311/14: tweets.time[101]
311/15: tweets.isnull().sum().sum()
311/16:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
311/17:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
311/18: tweets[99:110]
311/19: tweets.iloc[101].tweet
311/20:
text=tweets.tweet[101]
print(text)
311/21: tweets.columns
311/22:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'procesed'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
311/23: tweets.loc[:,'normalized']=normalize_corpus(tweets.procesed)
311/24: tweets.loc[:,'normalized']=normalize_corpus(tweets.procesed)
311/25:
from text_normalizer import normalize_corpus
tweets.loc[:,'normalized']=normalize_corpus(tweets.procesed)
311/26: tweets.to_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5', key='df', mode='w')
311/27: tweets1 = read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
311/28: tweets1 = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
311/29: tweets1.head()
311/30: tweets1.column
311/31: tweets1.columns
311/32: tweets
311/33: tweets1 = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
311/34: tweets1.columns
311/35: tweets1.tweet[12]
311/36: tweets1.normalized[12]
311/37:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('via','',self.text,flags=re.IGNORECASE)
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
311/38: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
311/39:

import string

import nltk
from nltk.corpus import stopwords
from nltk import re
#import regex, utils
#from profanity_filter import ProfanityFilter

#----------------------------------------------------------------------------

#----------------------------------------------------------------------------
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
# =============================================================================
#         self.pf = self._init_profanity_filter()
# 
#     @staticmethod
#     def _init_profanity_filter():
#         pf = ProfanityFilter()
#         pf.censor_char = ' '
#         pf.censor_whole_words = True
#         return pf
# =============================================================================

    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
#('RT @[\w_]+: ', '', tweet)
#re.sub(pattern=re.compile(r'RT @[\w_]+: '),repl=' ',string=self.text)
#z = lambda x: re.compile('\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('via','',self.text,flags=re.IGNORECASE)
 #       self.text = re.sub(pattern=re.compile('RT @[\w_]+: '),repl=' ',string=self.text)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self

#     def remove_blank_spaces(self):
#         self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
#         return self

    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))

        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self

            
# =============================================================================
#     def remove_profane_words(self):
#         self.text = self.pf.censor(self.text)
#         self.remove_blank_spaces()
#         return self
# =============================================================================

    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
311/40:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'processed'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
311/41:
from text_normalizer import normalize_corpus
tweets.loc[:,'normalized']=normalize_corpus(tweets.processed)
311/42: tweets.head()
311/43: tweets.head()
311/44: tweets.to_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5', key='df', mode='w')
311/45: tweets1 = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
311/46: tweets1.columns
311/47: tweets1.tweet[12]
311/48: tweets1.normalized[12]
311/49: import gensim #downloaded pretrained model
311/50: model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
311/51:
tweets = tweets1.normalized
for tweet in tweets:
    print(tweet)
311/52: tweets = tweets1.normalized
311/53: size(tweets)
311/54: tweets.shape
311/55: tweets = tweets[0:100]
311/56:

filtered_tweets = [[word for word in tweet if word in model.vocab] for tweet in tweets];
311/57: filtered_tweets
311/58:
tweets = tweets[0:100]
tweets
311/59:
for tweet in tweets:
    print(tweet)
311/60:
for tweet in tweets:
    print(tweet)
    print([word for word in tweet if word in model.vocab])
311/61: model.vocab[0:2]
311/62: model.vocab
311/63:
for tweet in tweets:
    print(tweet)
    for word in tweet:
        print(word)
311/64:
tweets = tweets[0:100]
tweets[0]
311/65: tweets = tweets1.normalized
311/66:
tweets = tweets[0:100]
tw
311/67: tweets = tweets1.normalized
311/68: norm = tweets1.normalized
311/69: tweets = norm[0:100]
311/70: tweets.iloc[0]
311/71: tweets.iloc[0][0]
311/72: tweets.iloc[0]
311/73: norm = tweets1.normalized
311/74: norm1 = norm[0:100]
311/75: norm2 = [[tweet.split()] for tweet in norm1]
311/76: norm2
311/77: norm2 = [tweet.split() for tweet in norm1]
311/78: norm2
311/79: norm1[0]
311/80: norm1
311/81: tweets = norm2
311/82:

filtered_tweets = [[word for word in tweet if word in model.vocab] for tweet in tweets];
311/83: filtered_tweets
311/84:
for tweet in filtered_tweets:
    print(tweet)
311/85:
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = normalize(sum(word_vecs).reshape(1, -1))
311/86:
from sklearn import preprocessing.normalize
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = normalize(sum(word_vecs).reshape(1, -1))
311/87:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
311/88:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    print(model_vecs)
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
311/89:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    print(word_vecs)
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
311/90: tweets1[0:100]
311/91: tweets1 = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
311/92:
#for the time being I only use 100 rows
tweets1=tweets1[0:99]
311/93: tweets1.tweet[12]
311/94: tweets1.iloc[:,12]
311/95: tweets1.iloc[12]
311/96: tweets1.normalized[12]
311/97: tweets1.iloc[12].normalized
311/98: print(tweets.tweet[11:12])
311/99: print(tweets.iloc[12].tweet)
311/100: print(tweets1.iloc[12].tweet)
311/101: norm = tweets1.normalized
311/102: tweets = [tweet.split() for tweet in norm]
311/103:

filtered_tweets = [[word for word in tweet if word in model.vocab] for tweet in tweets];
311/104: tweets1.shape
311/105: length(filtered_tweets)
311/106: filtered_tweets.shape
311/107: len(filtered_tweets)
311/108: titles=pd.sereis()
311/109: titles=pd.series()
311/110: titles=pd.Series()
311/111: titles=[]
311/112:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    titles.append(title_vec)
311/113:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    titles.append(title_vec)
        
titles
311/114: titles=[]
311/115: titles=[]
311/116: model.vocab(titles)
311/117: model.most_similar('queen', 'king')
313/1:
from tweepy import Stream
import pandas as pd
313/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
313/3:
from tweepy import Stream
import pandas as pd
313/4:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
313/5:
import tweepy
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

api = tweepy.API(auth)
313/6:

from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
import json


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query):
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        try:
            with open(self.outfile, 'a') as f:
                f.write(data)
                #print(data)
                return True
        except BaseException as e:
            #print("Error on_data: %s" % str(e))
            time.sleep(5)
        return True

    def on_error(self, status):
        print(status)
        return True
313/7: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
313/8: import gensim #downloaded pretrained model
313/9: model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
313/10: data = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
313/11: data.columns
313/12: tweets=[tweet.split() for tweet in data.normalized]
313/13:

filtered_tweets = [[word for word in tweet if word in model.vocab] for tweet in tweets];
313/14: filtered_tweets
313/15:
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    title_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = normalize(sum(word_vecs).reshape(1, -1))
313/16:
from tweepy import Stream
import pandas as pd
import numpy as np
313/17:
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    title_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = normalize(sum(word_vecs).reshape(1, -1))
313/18:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    title_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
313/19: len(filtered_tweets)
313/20: data = data.iloc[0:100]
313/21: tweets=[tweet.split() for tweet in data.normalized]
313/22: tweets=[tweet.split() for tweet in data.normalized]
313/23:

filtered_tweets = [[word for word in tweet if word in model.vocab] for tweet in tweets];
313/24: len(filtered_tweets)
313/25: titles=[]
313/26:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    title_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    titles.append(model.most_similar(positive=[title_vec], topn=1))
314/1:
from tweepy import Stream
import pandas as pd
import numpy as np
314/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
314/3: data = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
314/4: data = data.iloc[0:100]
314/5: import gensim #downloaded pretrained model
314/6: model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
314/7: tweets=[tweet.split() for tweet in data.normalized]
314/8:

filtered_tweets = [[word for word in tweet if word in model.vocab] for tweet in tweets];
314/9: len(filtered_tweets)
314/10: titles=[]
314/11:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    title_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    titles.append(model.most_similar(positive=[title_vec], topn=1))
314/12: word_vecs
314/13: word_vecs[0]
314/14: model.similar_by_vector(word_vecs[0],topn=10, restrict_vocab=None)
314/15: model.similar_by_vector(word_vecs[1],topn=10, restrict_vocab=None)
314/16: model.similar_by_vector(word_vecs[2],topn=10, restrict_vocab=None)
314/17:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    title_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    titles=titles.append(model.similar_by_vector(word_vecs[2],topn=1, restrict_vocab=None))
314/18:
titles=[]
titles.appned([ss])
314/19:
titles=[]
titles.appened([ss])
314/20:
titles=[]
titles.append([ss])
314/21:
titles=[]
titles.append(['hh'])
314/22:
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    title_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    titles=titles.append([model.similar_by_vector(word_vecs[2],topn=1, restrict_vocab=None)])
315/1:
from tweepy import Stream
import pandas as pd
import numpy as np
315/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
315/3:

def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
315/4: data = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
315/5: data = data.iloc[0:100]
315/6: import gensim #downloaded pretrained model
315/7: model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
316/1:
from tweepy import Stream
import pandas as pd
import numpy as np
316/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
316/3: data = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
316/4: data = data.iloc[0:100]
316/5: import gensim #downloaded pretrained model
316/6: model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
316/7: tweets=[tweet.split() for tweet in data.normalized]
316/8:

filtered_tweets = [[word for word in tweet if word in model.vocab] for tweet in tweets];
316/9:
result = most.most_similar(positive=['woman', 'king'], negative=['man'])
print("{}: {:.4f}".format(*result[0]))
316/10:
result = model.most_similar(positive=['woman', 'king'], negative=['man'])
print("{}: {:.4f}".format(*result[0]))
316/11: ?model.wv.word_vec
316/12: ?model.wv
316/13: ?model.wv.word_vec
316/14:
vectorized_titles = pd.DataFrame(columns=["Vectors"])
from sklearn import preprocessing
for tweet in filtered_tweets:
    word_vecs = [model[word] for word in tweet]
    title_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        title_vec = [np.zeros(300)]
    else: 
        title_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_titles = vectorized_titles.append({'Vectors': title_vec}, ignore_index=True)
316/15: data.head()
316/16: data.normalized[1]
316/17: data.normalized
318/1:
from tweepy import Stream
import pandas as pd
import numpy as np
318/2:
# Import the Twython class
from twython import Twython  
import json

# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
318/3:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
318/4:
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
api = tweepy.API(auth)
318/5: ?Stream
318/6: df = pd.read_json("/Users/shahla/Dropbox/SharpestMinds/stream/data/stream_politics.json",lines=True)
318/7: df.shape
318/8:
list_columns=list(df.columns)
list_columns
318/9: df.created_at[1:10]
318/10:
#keep English languages
data=df[df.lang=='en']
318/11: data.shape
318/12: tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
318/13: tweets = tweets.sort_values('time', ascending=False)
318/14: tweets=tweets.drop_duplicates()
318/15: sum(tweets.duplicated())
318/16: print('The time range tweets written is from', min(tweets.time),'to', max(tweets.time))
318/17: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
318/18: tweets.shape
318/19: tweets.head()
318/20: tweets.iloc[100].time
318/21:
class TwitterPreprocessor:

    def __init__(self, text: str):
        self.text = text
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self


    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/22:
from text_normalizer import normalize_corpus
tweets.loc[:,'normalized']=normalize_corpus(tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text))
318/23:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
318/24:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import string
318/25:
class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
        
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/26:
from text_normalizer import normalize_corpus
tweets.loc[:,'normalized']=normalize_corpus(tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text))
318/27:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata
class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/28:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata
class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() 
    
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self

    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/29:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata
class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars()
    
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/30:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'processed'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
318/31:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata
class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions()
    
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/32:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'processed'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
318/33:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata
class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters()
    
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/34:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'processed'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
318/35:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata
class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters()
    
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/36:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'processed'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
318/37:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata
class TwitterPreprocessor:
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters()
    
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/38:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'processed'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
318/39: nlp = spacy.load('en', parse = False, tag=False, entity=False)
318/40:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters()
    
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/41:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'processed'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
318/42:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords()
    
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/43:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'processed'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
318/44:
text='Shahla is good'
text.lower()
318/45:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .Lower()
    
    def Lower(self):
        self.text = text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/46:
#from twitter_preprocessor import TwitterPreprocessor

tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)

#p.fully_preprocess()
#print(p.text)
318/47: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
318/48: tweets=tweets[0:100]
318/49: tweets.shape
318/50: tweets.head()
318/51:
class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
        
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() 
            
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/52:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .Lower()
    
    def Lower(self):
        self.text = text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
318/53: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
318/54: tweets.head()
320/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
320/2: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
320/3: tweets=tweets[0:100]
320/4: tweets.shape
320/5: tweets.head()
320/6:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import string
320/7:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .Lower()
    
    def Lower(self):
        self.text = text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
320/8: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
320/9:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
320/10: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
320/11: tweets.head()
320/12: tweets[0]
320/13: tweets.iloc[0]
320/14: tweets.iloc[0].tweet
320/15: tweets.iloc[0].normalized
320/16:
t='she start rt'
(re.sub(r'\ba\b','',t)).replace(" "," ")
320/17:
t='she start  a rt'
(re.sub(r'\ba\b','',t)).replace(" "," ")
320/18:
t='she start  a rt'
(re.sub(r'\brt\b','',t)).replace(" "," ")
320/19:
t='she start a rt'
(re.sub(r'\brt\b','',t)).replace(" "," ")
320/20:
t='she start a rt'
(re.sub(r'\brt\b','',t))
320/21:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub('RT','',self.text,flags=re.IGNORECASE)
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
        def remove_blank_spaces(self):
            self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
            return self
320/22:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
        def remove_blank_spaces(self):
            self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
            return self
320/23: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
320/24:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', '', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/25: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
320/26: tweets.iloc[0].tweet
320/27: tweets.iloc[0].normalized
320/28: tweets.iloc[1].normalized
320/29: tweets.iloc[1].tweet
320/30:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/31: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
320/32: tweets=tweets[0:30]
320/33: tweets.shape
320/34:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/35: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
320/36: tweets.iloc[1].tweet
320/37: tweets.iloc[1].normalized
320/38: tweets.iloc[2].tweet
320/39: tweets.iloc[2].normalized
320/40: tweets.iloc[3].tweet
320/41: tweets.iloc[3].normalized
320/42: tweets.iloc[4].tweet
320/43: tweets.iloc[4].normalized
320/44: tweets.iloc[5].tweet
320/45: tweets.iloc[5].normalized
320/46: tweets.iloc[6].tweet
320/47: tweets.iloc[6].normalized
320/48: tweets.iloc[7].tweet
320/49: tweets.iloc[7].normalized
320/50: tweets.iloc[9].tweet
320/51: tweets.iloc[9].normalized
319/1:

from nltk.corpus import stopwords

from nltk import word_tokenize

stop_words = set(stopwords.words('english'))

text = word_tokenize("The quick brown fox jumps over the lazy dog")

#print(nltk.pos_tag(text))

new_sentence =[]

for w in text:

if w not in stop_words: new_sentence.append(w)

print(text)
319/2:

from nltk.corpus import stopwords

from nltk import word_tokenize

stop_words = set(stopwords.words('english'))

text = word_tokenize("The quick brown fox jumps over the lazy dog")

#print(nltk.pos_tag(text))

new_sentence =[]

for w in text:

if w not in stop_words:
    new_sentence.append(w)

print(text)
319/3:

from nltk.corpus import stopwords

from nltk import word_tokenize

stop_words = set(stopwords.words('english'))

text = word_tokenize("The quick brown fox jumps over the lazy dog")

#print(nltk.pos_tag(text))

new_sentence =[]

for w in text:

if w not in stop_words:
    new_sentence.append(w)

print(text)
319/4:

from nltk.corpus import stopwords

from nltk import word_tokenize

stop_words = set(stopwords.words('english'))

text = word_tokenize("The quick brown fox jumps over the lazy dog")

#print(nltk.pos_tag(text))

new_sentence =[]

for w in text:

    if w not in stop_words:
       new_sentence.append(w)

print(text)
319/5:

from nltk.corpus import stopwords

from nltk import word_tokenize

stop_words = set(stopwords.words('english'))

text = word_tokenize("The quick ... brown fox jumps over the lazy dog")

#print(nltk.pos_tag(text))

new_sentence =[]

for w in text:

    if w not in stop_words:
       new_sentence.append(w)

print(text)
320/52:
t='she start a rt ...'
TwitterPreprocessor(x).fully_preprocess().t
320/53:
t='she start a rt ...'
TwitterPreprocessor(t).fully_preprocess().text
320/54:
t='she start a rt begin...shahla'
TwitterPreprocessor(t).fully_preprocess().text
320/55:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
#     def remove_stopwords(self):
#         tokens = tokenizer.tokenize(self.text)
#         tokens = [token.strip() for token in tokens]
#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
#         self.text = ' '.join(filtered_tokens)    
#         return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/56:
t='she start a rt begin...shahla'
TwitterPreprocessor(t).fully_preprocess().text
320/57:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
#     def remove_special_characters(self):
#         self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
#         return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/58:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
#     def remove_special_characters(self):
#         self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
#         return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/59:
t='she start a rt begin...shahla'
TwitterPreprocessor(t).fully_preprocess().text
320/60:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

    def remove_punctuation(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/61:
t='she start a rt begin...shahla'
TwitterPreprocessor(t).fully_preprocess().text
320/62:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/63:
t='she start a rt begin...shahla'
TwitterPreprocessor(t).fully_preprocess().text
320/64:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            #.remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/65:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
#             .remove_punctuation() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/66:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
320/67:
t='she start a rt begin...shahla'
TwitterPreprocessor(t).fully_preprocess().text
320/68: tweets.tweet[12]
320/69: tweets.iloc[9].tweet
320/70: tweets.iloc[9].normalized
320/71: tweets.iloc[10].tweet
320/72: tweets.iloc[10].normalized
320/73: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
320/74: tweets.iloc[9].tweet
320/75: tweets.iloc[9].normalized
320/76: tweets.iloc[1].tweet
320/77: tweets.iloc[1].normalized
320/78: tweets.iloc[10].normalized
320/79: tweets.iloc[10].tweet
320/80: tweets.iloc[11].tweet
320/81: tweets.iloc[11].normalized
320/82: tweets.iloc[12].tweet
320/83: tweets.iloc[12].normalized
320/84: tweets.iloc[13].tweet
320/85: tweets.iloc[13].normalized
320/86: tweets.iloc[14].tweet
320/87: tweets.iloc[14].normalized
320/88: tweets.iloc[15].tweet
320/89: tweets.iloc[15].normalized
320/90: tweets.iloc[20].tweet
320/91: tweets.iloc[20].normalized
320/92: tweets.tweet[12]
320/93: tweets.iloc[12].tweet
320/94: tweets.iloc[12].normalized
320/95: tweets.columns
320/96: tweets.time
320/97: min(tweets.time)
320/98: max(tweet.time)
320/99: max(tweets.time)
320/100: import gensim #downloaded pretrained model
320/101: model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
320/102: import gensim #downloaded pretrained model
320/103: model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
320/104: model[0]
320/105: model
320/106: model['a']
320/107: model['key']
320/108: model['key door']
320/109: tweets=[tweet.split() for tweet in tweets.normalized]
320/110: tweets
320/111: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
320/112: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
320/113: tweets=tweets[0:30]
320/114: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
320/115: tweets.normalized = [tweet.split() for tweet in tweets.normalized]
319/6:
import pandas as pd
import numpy as np
319/7: d = {'tweets': ['hello my name is Jim', 'I like apples'], 'normalized': ['hello name Jim', 'like apples']}
319/8: tweets = pd.DataFrame(data=d)
319/9: tweets
319/10:
model = {
        'hello': np.array([1,2,3]), 
         'name': np.array([1,2,3]), 
         'Jim': np.array([1,2,3]), 
         'apples': np.array([1,2,3]), 
         'like': np.array([1,2,3]), 
        }
319/11: model
319/12: tweets.normalized = [tweet.split() for tweet in tweets.normalized]
319/13: tweets
319/14:
vectorized_tweets = pd.DataFrame(columns=["Raw tweets", "Vectors"])

for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in tweet.normalized]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_tweets = vectorized_tweets.append({'Vectors': tweet_vec, 'Raw tweets': tweet.tweets}, ignore_index=True)
320/116:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
319/15:
from sklearn import preprocessing
vectorized_tweets = pd.DataFrame(columns=["Raw tweets", "Vectors"])

for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in tweet.normalized]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_tweets = vectorized_tweets.append({'Vectors': tweet_vec, 'Raw tweets': tweet.tweets}, ignore_index=True)
319/16:
from sklearn import preprocessing
vectorized_tweets = pd.DataFrame(columns=["Raw tweets", "Vectors"])

for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in tweet.normalized]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_tweets = vectorized_tweets.append({'Vectors': tweet_vec, 'Raw tweets': tweet.tweets}, ignore_index=True)
319/17: vectorized_tweets
320/117: tweets.loc[:,'Vectors'] = 0
320/118: tweets.iloc[1:3]
320/119: tweets.loc[:,'Vectors'] = [np.zeros(300)]
320/120: tweets.loc[:,'Vec'] = [np.zeros(300)]
320/121: tweets.index
320/122: tweets[index]
320/123: tweets[8008]
320/124: tweets.index
320/125: tweets.iloc[[80018]]
320/126: Vectors=pd.Series()
320/127:


for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in tweet.normalized]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/128: model.vocab
320/129: fuck in model.vocab
320/130: 'fuck' in model.vocab
320/131:


for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in tweet.normalized & model.vocab]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/132:


for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized & model.vocab)]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/133:


for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/134:  tweets.shape
320/135:  tweets=tweets[0:3]
320/136:


for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/137:  tweets=tweets[0:1]
320/138:
for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/139:  tweets=tweets[0:2]
320/140:
for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/141: tweets
320/142: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets.h5')
320/143: tweets=tweets[0:30]
320/144: sum(tweets.duplicated())
320/145: tweets=tweets[0:30]
320/146: tweets.shape
320/147: tweets.head()
320/148: tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
320/149:  tweets=tweets[0:3]
320/150:
for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/151: tweets
320/152: tweets.normalized = [tweet.split() for tweet in tweets.normalized]
320/153:
for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/154: tweets
320/155: tweets.loc[:,'Vectors']=0
320/156: tweets
320/157:  tweets=tweets[0:2]
320/158:
for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    tweet_vec = [np.zeros(300)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweet.Vectors = tweet_vec
320/159: tweets
320/160: tweets.loc[:,'Vectors']=[np.zeros(300)]
320/161:
for index, tweet in tweets.iterrows():
    tweet.Vectors = [np.zeros(300)]
320/162: tweets
320/163: tweets.Vectors
320/164: tweets.loc[:,'Vectors']=1
320/165: tweets
320/166:
for index, tweet in tweets.iterrows():
    tweet.Vectors = [np.zeros(300)]
320/167: tweets.Vectors
320/168:
for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    tweets.at[index,'Vectors'] = tweet_vec
    #tweet.Vectors = tweet_vec
322/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
322/2:
import pandas as pd 
  
# initialize list of lists 
data = [['tom', 10], ['nick', 15], ['juli', 14]] 
  
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Age']) 
  
# print dataframe. 
df
322/3:
import pandas as pd 
  
# initialize list of lists 
data = [['tom', [np.zeros(300)]], ['nick', [np.zeros(300)]], ['juli', [np.zeros(300)]]] 
  
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Age']) 
  
# print dataframe. 
df
322/4:
import pandas as pd 
  
# initialize list of lists 
data = [['tom', np.zeros(300)], ['nick', np.zeros(300)], ['juli', np.zeros(300)]] 
  
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Age']) 
  
# print dataframe. 
df
322/5:
import pandas as pd 
  
# initialize list of lists 
data = [['tom', np.zeros(300)], ['nick', np.zeros(300)], ['juli', np.zeros(300)]] 
  
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Age']) 
  
# print dataframe. 
df.loc[:,'Gender']=np.nan
322/6:
import pandas as pd 
  
# initialize list of lists 
data = [['tom', np.zeros(300)], ['nick', np.zeros(300)], ['juli', np.zeros(300)]] 
  
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Age']) 
  
# print dataframe. 
df.loc[:,'Gender']=np.nan
df
322/7:
import pandas as pd 
  
# initialize list of lists 
data = [['tom', np.zeros(300)], ['nick', np.zeros(300)], ['juli', np.zeros(300)]] 
  
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Age']) 
  
# print dataframe. 
df.loc[:,'Gender']=np.nan
for index, row in df.iterrows():
    row.Gender='f'
322/8:
import pandas as pd 
  
# initialize list of lists 
data = [['tom', np.zeros(300)], ['nick', np.zeros(300)], ['juli', np.zeros(300)]] 
  
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Age']) 
  
# print dataframe. 
df.loc[:,'Gender']=np.nan
for index, row in df.iterrows():
    row.Gender='f'
df
322/9:
import pandas as pd 
  
# initialize list of lists 
data = [['tom', np.zeros(300)], ['nick', np.zeros(300)], ['juli', np.zeros(300)]] 
  
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Age']) 
  
# print dataframe. 
df.loc[:,'Gender']=np.nan
for index, row in df.iterrows():
    print(row.Gender)
df
322/10: tweets = pd.read_hdf('/Users/shahla/Dropbox/SharpestMinds/tweets-processed.h5')
322/11: import gensim
322/12:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
322/13: tweets=tweets[0:2]
322/14: tweets.normalized = [tweet.split() for tweet in tweets.normalized]
322/15: tweets.loc[:,'Vectors'] = [np.zeros(300)]
322/16:
tweets=tweets[0:2]
tweets
322/17:
from sklearn import preprocessing
vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_tweets = vectorized_tweets.append \
    ({'Raw_tweets': tweet.tweets, 'Normalized_tweet': tweet.normalized,'Vector': tweet_vec}, ignore_index=True)
322/18:
from sklearn import preprocessing
vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_tweets = vectorized_tweets.append \
    ({'Raw_tweets': tweet.tweet, 'Normalized_tweet': tweet.normalized,'Vector': tweet_vec}, ignore_index=True)
322/19: vectorized_tweets
322/20:
from sklearn import preprocessing
vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_tweets = vectorized_tweets.append \
    ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.normalized,'Vector': tweet_vec}, ignore_index=True)
322/21: vectorized_tweets
322/22:
for index, tweet in tweets.iterrows():
    word_vecs = [ word in (tweet.normalized and !model.vocab)]
322/23:
for index, tweet in tweets.iterrows():
    word_vecs = [ word in (tweet.normalized and not model.vocab)]
322/24:
for index, tweet in tweets.iterrows():
    word_vecs = [word for word in (tweet.normalized and not model.vocab)]
322/25:
for index, tweet in tweets.iterrows():
    word_vecs = [word if word in (tweet.normalized and not model.vocab)]
322/26:
for index, tweet in tweets.iterrows():
     [model[word] for word  not in (tweet.normalized and model.vocab)]
322/27:
for index, tweet in tweets.iterrows():
    words= [word for word   in (tweet.normalized and model.vocab)]
322/28:
for index, tweet in tweets.iterrows():
   # words= [word for word   in (tweet.normalized and model.vocab)]
    w = [word in tweet.normalized if word not in model.vocab]
322/29:
for index, tweet in tweets.iterrows():
    w = [word in tweet.normalized if word not in model.vocab]
322/30:
for index, tweet in tweets.iterrows():
    w = [word for word in tweet.normalized if word not in model.vocab]
322/31: w
322/32:
not_words = pd.DataFrame(columns=['tweet','out_of_model_words'])
for index, tweet in tweets.iterrows():
    w = [word for word in tweet.normalized if word not in model.vocab]
    not_words = not_words.append({'tweet':tweet.tweet,'out_of_model_words':w}, ignore_index=True)
322/33: not_words
322/34: not_words[0]
322/35: not_words.iloc[0]
322/36: not_words.loc[:,'out_of_model_words']
322/37: not_words.loc[0,'out_of_model_words']
322/38: not_words.loc[1,'out_of_model_words']
322/39:
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
322/40:
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            try:
                with open(self.outfile, 'a') as f:
                     f.write(data)
                     #print(data)
                return True
            except BaseException as e:
                   #print("Error on_data: %s" % str(e))
                    time.sleep(5)
            return True
        else:
                        self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
322/42:
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            try:
                with open(self.outfile, 'a') as f:
                     f.write(data)
                     #print(data)
                return True
            except BaseException as e:
                   #print("Error on_data: %s" % str(e))
                    time.sleep(5)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
322/43:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
322/44:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/45:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
322/46:
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
322/47:
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
api = tweepy.API(auth)
322/48:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/49:
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            try:
                with open(self.outfile, 'a') as f:
                     f.write(data)
                     #print(data)
                return True
            except BaseException as e:
                   #print("Error on_data: %s" % str(e))
                    time.sleep(5)
            return True
        else:
            f.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
322/50:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/51:
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        #self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
322/52:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
#MyListener() saves the data into a .json file with name stream_query
twitter_stream = Stream(auth, MyListener(data_dir, query),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/53: df = pd.read_json("/Users/shahla/Dropbox/SharpestMinds/stream/data/stream_politics.json",lines=True)
322/54: df.shape
322/55: df.tail(40)
322/56:
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        #self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')
        super(MyStreamListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
322/57:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
time_limit=20
#MyListener() saves the data into a .json file with name stream_query

twitter_stream = Stream(auth, MyListener(data_dir, query, time_limit)),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/58:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
time_limit=20
#MyListener() saves the data into a .json file with name stream_query

twitter_stream = Stream(auth, MyListener(data_dir, query, time_limit),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/59:
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        #self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')
        super(MyStreamListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
322/60:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
time_limit=20
#MyListener() saves the data into a .json file with name stream_query

twitter_stream = Stream(auth, MyListener(data_dir, query, time_limit),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/61:
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        #self.outfile = "%s/stream_%s.json" % (data_dir, query_fname)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
322/62:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
time_limit=20
#MyListener() saves the data into a .json file with name stream_query

twitter_stream = Stream(auth, MyListener(data_dir, query, time_limit),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/63: df = pd.read_json("/Users/shahla/Dropbox/SharpestMinds/stream/data/stream_politics.json",lines=True)
322/64: df.shape
322/65:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
time_limit=20
file_name = "stream_"+query+'.json'
import os
if os.path.exists(file_name):
    os.remove(file_name)
#MyListener() saves the data into a .json file with name stream_query

twitter_stream = Stream(auth, MyListener(data_dir, query, time_limit),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/66: df = pd.read_json("/Users/shahla/Dropbox/SharpestMinds/stream/data/stream_politics.json",lines=True)
322/67: df.shape
322/68:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data'
time_limit=20
file_name = "stream_"+query
import os
if os.path.exists(file_name+'.json'):
    os.remove(file_name+'.json')
#MyListener() saves the data into a .json file with name stream_query

twitter_stream = Stream(auth, MyListener(data_dir, query, time_limit),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/69: df = pd.read_json("/Users/shahla/Dropbox/SharpestMinds/stream/data/"+file_name+".json",lines=True)
322/70: df.shape
322/71: df
322/72:
import os
file_name = "stream_"+query
if os.path.exists(file_name+'.json'):
    os.remove(file_name+'.json')
322/73: file_name
322/74: file_name+'.json
322/75: file_name+'.json'
322/76:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
time_limit=20
file_name = "stream_"+query
import os
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
322/77:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
322/78:
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
api = tweepy.API(auth)
322/79:
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
322/80:
query = 'politics'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
time_limit=20
file_name = "stream_"+query
import os
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#MyListener() saves the data into a .json file with name stream_query

twitter_stream = Stream(auth, MyListener(data_dir, query, time_limit),tweet_mode='extended')
twitter_stream.filter(track=[query]) # list of querries to track
322/81: df = pd.read_json(data_dir+file_name+".json",lines=True)
322/82: df.shape
322/83:
import os
class get_latest_tweets(data_dir, time_limit, topic):
    def __init__(self, time_limit, topic):
        self.time = time_limit
        self.topic = topic
        self.data_dir

    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(self.data_dir, self.topic, self.time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[self.topic]) # list of querries to track
322/84:
import os
class get_latest_tweets(data_dir, auth, time_limit, topic):
    def __init__(data_dir, time_limit, topic):
        self.time = time_limit
        self.topic = topic
        self.data_dir = data_dir
        self.auth = auth

    file_name = "stream_"+self.topic
    twitter_stream = Stream(self.auth, MyListener(self.data_dir, self.topic, self.time),tweet_mode='extended')
    twitter_stream.filter(track=[self.topic]) # list of querries to track
322/85:
import os
class get_latest_tweets(data_dir, auth, time_limit, topic):
    def __init__(data_dir, auth, time_limit, topic):
        self.time = time_limit
        self.topic = topic
        self.data_dir = data_dir
        self.auth = auth

    file_name = "stream_"+self.topic
    twitter_stream = Stream(self.auth, MyListener(self.data_dir, self.topic, self.time),tweet_mode='extended')
    twitter_stream.filter(track=[self.topic]) # list of querries to track
322/86:
import os
class get_latest_tweets:
    def __init__(self, data_dir, auth, time_limit, topic):
        self.time = time_limit
        self.topic = topic
        self.data_dir = data_dir
        self.auth = auth

    file_name = "stream_"+self.topic
    twitter_stream = Stream(self.auth, MyListener(self.data_dir, self.topic, self.time),tweet_mode='extended')
    twitter_stream.filter(track=[self.topic]) # list of querries to track
322/87:
import os
class get_latest_tweets:
    def __init__(self, data_dir, auth, time_limit, topic):
        self.time = time_limit
        self.topic = topic
        self.data_dir = data_dir
        self.auth = auth

    file_name = "stream_"+self.topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time),tweet_mode='extended')
    twitter_stream.filter(track=[self.topic]) # list of querries to track
322/88:
import os
class get_latest_tweets:
    def __init__(self, data_dir, auth, time_limit, topic):
        self.time = time_limit
        self.topic = topic
        self.data_dir = data_dir
        self.auth = auth
322/89:
import os
class get_latest_tweets:
    def __init__(self, data_dir, auth, time_limit, topic):
        self.time = time_limit
        self.topic = topic
        self.data_dir = data_dir
        self.auth = auth

    file_name = "stream_"+self.topic
322/90:
import os
class get_latest_tweets:
    def __init__(self, data_dir, auth, time_limit, topic):
        self.time = time_limit
        self.topic = topic
        self.data_dir = data_dir
        self.auth = auth

    file_name = "stream_"+topic
322/91:
import os
class get_latest_tweets:
    def __init__(self, data_dir, auth, time_limit, topic):
        self.time = time_limit
        self.topic = topic
        self.data_dir = data_dir
        self.auth = auth
    topic= self.topic
    file_name = "stream_"+topic
322/92:
import os
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
322/93:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
322/94:
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#api = tweepy.API(auth)
322/95:
import os
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
322/96:
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
322/97:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
322/98:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
get_latest_tweets(data_dir, auth, time_limit, topic)
322/99:
import os
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    df = pd.read_json(data_dir+file_name+".json",lines=True)
    df = df[df.lang=='en']
322/100:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
get_latest_tweets(data_dir, auth, time_limit, topic)
322/101:
import os
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    df = pd.read_json(data_dir+file_name+".json",lines=True)
    df = df[df.lang=='en']
    return df
322/102:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
df =get_latest_tweets(data_dir, auth, time_limit, topic)
df
322/103:
import os
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    df = pd.read_json(data_dir+file_name+".json",lines=True)
    df = df[df.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=df.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']          
    return df
322/104:
import os
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    df = pd.read_json(data_dir+file_name+".json",lines=True)
    df = df[df.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=df.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
322/105:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
df = get_latest_tweets(data_dir, auth, time_limit, topic)
df
322/106:
import os
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=df.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
322/107:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
df = get_latest_tweets(data_dir, auth, time_limit, topic)
df
322/108:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
tweets
322/109:
import os
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
322/110:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
tweets
322/111:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
322/112:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
322/113:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
322/114:
def vectorize_latest_tweets(tweets):
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.Normalized = [tweet.split() for tweet in tweets.Normalized]
    tweets=tweets[0:2]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
322/115:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
322/116: vectorized_tweets = vectorize_latest_tweets(tweets)
322/117:
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import string
322/118:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import string
322/119: vectorized_tweets = vectorize_latest_tweets(tweets)
322/120: vectorized_tweets
322/121: vectorized_tweets[0].raw_tweet
322/122: vectorized_tweets.iloc[0].raw_tweet
322/123: vectorized_tweets.iloc[0]
322/124: vectorized_tweets.iloc[0,0]
322/125: input = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'
322/126:
string = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'
input=pd.DataFrame(columns=["Raw_input","Normalized_input","Vector"])
input.loc[:, "Raw_input"] = string
322/127:
string = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'
input=pd.DataFrame(columns=["Raw_input","Normalized_input","Vector"])
input.loc[:, "Raw_input"] = string
input
322/128:
st = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'
input=pd.DataFrame(columns=["Raw_input","Normalized_input","Vector"])
input.loc[:, "Raw_input"] = st
input
322/129:
st = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'
input=pd.DataFrame(columns=["Raw_input","Normalized_input","Vector"])
input.loc[0, "Raw_input"] = st
input
322/130:
def vectorize_user_input(input):
    vectorized_input=pd.DataFrame(columns=["Raw_input","Normalized_input","Vector"])
    user_input.loc[0, "Raw_input"] = input
322/131:
def vectorize_user_input(user_input):
    vectorized_input=pd.DataFrame(columns=["Raw_input","Normalized_input","Vector"])
    vectorized_input.loc[0, "Raw_input"] = user_input
    vectorized_input.loc[:,'Normalized'] = vectorized_input.Raw_input.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    vectorized_input.Normalized = [inputs.split() for inputs in vectorized_input.Normalized]
    word_vecs = [model[word] for word in (vectorized_input.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input.loc[0, "Vectors"] = tweet_vec
322/132:
def vectorize_user_input(user_input):
    vectorized_input=pd.DataFrame(columns=["Raw_input","Normalized_input","Vector"])
    vectorized_input.loc[0, "Raw_input"] = user_input
    vectorized_input.loc[:,'Normalized'] = vectorized_input.Raw_input.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    vectorized_input.Normalized = [inputs.split() for inputs in vectorized_input.Normalized]
    word_vecs = [model[word] for word in (vectorized_input.Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input.loc[0, "Vectors"] = tweet_vec
322/133:
user_input = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'

vectorized_input = vectorize_user_input(user_input)
322/134:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (vectorized_input.Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
322/135:
user_input = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'

vectorized_input = vectorize_user_input(user_input)
322/136:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
322/137:
user_input = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'

vectorized_input = vectorize_user_input(user_input)
322/138: vectorized_input
322/139:   ?model.cosine_similarities
322/140: tweets.columns
322/141: vectorized_tweets.columns
322/142:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vectorized_tweets.iloc[:,'cosine_simil']=model.cosine_similarities(vectorized_input, vectorized_tweets.Vector)
    return vectorized_tweets
322/143: vectorized_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
322/144: cos = find_most_similar_tweets(vectorized_input, vectorized_tweets)
322/145: vectorized_input
322/146:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vectorized_tweets.iloc[:,'cosine_simil']=model.cosine_similarities(vectorized_input.Vector, vectorized_tweets.Vector)
    return vectorized_tweets
322/147: cos = find_most_similar_tweets(vectorized_input, vectorized_tweets)
322/148:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vectorized_tweets.iloc[:,'cosine_simil']=model.cosine_similarities(vectorized_input['Vector'], vectorized_tweets.Vector)
    return vectorized_tweets
322/149: cos = find_most_similar_tweets(vectorized_input, vectorized_tweets)
321/1: vectorized_tweets.Vector.dtype
322/150: vectorized_tweets.Vector.dtype
322/151: vectorized_tweets.Vector
322/152: vectorized_tweets.Vector.values
322/153: vectorized_tweets.Vector[0]
322/154: vectorized_input['Vector'].dtype
322/155: vectorized_input['Vector']
322/156:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vectorized_tweets.iloc[:,'cosine_simil']=model.cosine_similarities(vectorized_input['Vector'], vectorized_tweets.Vector.values)
    return vectorized_tweets
322/157: #vectorized_tweets.Vector[0]
322/158: cos = find_most_similar_tweets(vectorized_input, vectorized_tweets)
322/159:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    x=model.cosine_similarities(vectorized_input['Vector'], vectorized_tweets.Vector.values)
    return x
322/160: cos = find_most_similar_tweets(vectorized_input, vectorized_tweets)
322/161:   ?model.cosine_similarities
322/162: vectorized_input['Vector'].shape
322/163: vectorized_tweets.Vector.shape
322/164: vectorized_tweets.Vector.values.shape
322/165: vectorized_input['Vector'].reshape(-1,1).shape
322/166:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
322/167:
user_input = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'

vectorized_input = vectorize_user_input(user_input)
322/168: vectorized_input['Vector'].reshape(-1,).shape
322/169:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)]
    else: 
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
322/170:
user_input = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'

vectorized_input = vectorize_user_input(user_input)
322/171: vectorized_tweets.Vector.values
322/172: np.vstack(vectorized_tweets.Vector.values)
322/173: np.vstack(vectorized_tweets.Vector.values).shape
322/174: np.stack(vectorized_tweets.Vector.values).shape
322/175: np.vstack(vectorized_tweets.Vector.values).shape
322/176: np.vstack(vectorized_tweets.Vector).shape
322/177: np.vstack(vectorized_tweets.Vector
322/178: np.vstack(vectorized_tweets.Vector)
322/179: vectorized_tweets.Vector[0]
322/180: vectorized_tweets.Vector[0].shape
322/181: np.zeros(300)
322/182: np.zeros(300).shape
322/183: model['water']
322/184: np.zeros(300)
322/185: model['water'].shape
322/186: np.zeros(300).shape
322/187: word_vecs
322/188: word_vecs.shape
322/189: len(word_vecs)
322/190: vectorized_tweets.Vector
322/191: vectorized_tweets.Vector[0]
322/192: [np.zeros(300)]
322/193: a = np.array([1, 2, 3])
322/194:
a = np.array([1, 2, 3])
a
322/195: vectorized_tweets.Vector
322/196: type(vectorized_tweets.Vector)
322/197: type(vectorized_tweets.Vector.value)
322/198: type(vectorized_tweets.Vector.values)
322/199: vectorized_tweets.Vector.values.shape
322/200: vectorized_tweets.Vector.values
322/201: vectorized_tweets.Vector.values[0]
322/202: vectorized_tweets.Vector[0]
322/203: np.zeros(300)
322/204: np.zeros(300).shape
322/205: t=pd.DataFrame(columns=['vec'])
322/206:
t=pd.DataFrame(columns=['vec'])
t
322/207:
t=pd.DataFrame(columns=['vec'])
t.append({'vec':np.zeros(300)})
322/208:
t=pd.DataFrame(columns=['vec'])
t.append({'vec':np.zeros(300)},ignore_index=True)
322/209: np.zeros(300)
322/210: np.zeros(300).shape
322/211: tweet_vec
322/212: tweet_vec.shape
322/213: tweet_vec
322/214: tweet_vec.shape
322/215: tweet_vec..reshape(-1,)
322/216: tweet_vec.reshape(-1,)
322/217: tweet_vec.reshape(-1,).shape
322/218: vectorized_tweets.Vector.values.reshape(-1,).shape
322/219: vectorized_tweets.Vector.reshape(-1,).shape
322/220: [np.zeros(300)].shape
322/221: [np.zeros(300)].reshape(-1,1)
322/222: [np.zeros(300)]
322/223: len([np.zeros(300)])
322/224:
import numpy as np
 np.asarray(([np.zeros(300)]))
322/225:
import numpy as np
np.asarray(([np.zeros(300)]))
322/226: [np.zeros(300)]
322/227:
import numpy as np
np.asarray(([np.zeros(300)])).shape
322/228:
t=pd.DataFrame(columns=['vec'])
t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
322/229:
t=pd.DataFrame(columns=['vec'])
t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
322/230:

t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
322/231:

t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
322/232: tweet_vec.reshape(-1,).shape
322/233:

t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
322/234: tweet_vec.reshape(-1,).shape
322/235:

t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t
322/236:

t=t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t
322/237:

t=t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t=t.append({'vec':np.asarray(([np.zeros(300)]))},ignore_index=True)
t
322/238: t.vec.shape
322/239: t.vec.reshape(-1,1)
322/240: type(t.vec)
322/241: (t.vec.values)
322/242: (t.vec.values).shape
322/243: (t.vec.values).reshape(-1,1)
322/244: (t.vec.values).reshape(-1,1).shape
322/245: (t.vec.values).shape
322/246: np.asarray(([np.zeros(300)]))}
322/247: np.asarray(([np.zeros(300)]))
322/248: np.asarray(([np.zeros(300)])).reshape(-1,1)
322/249: np.asarray(([np.zeros(300)])).reshape(1,)
322/250: np.asarray(([np.zeros(300)])).reshape(-1,)
322/251: np.asarray(([np.zeros(300)])).reshape(-1,).shape
322/252: np.asarray(([np.zeros(300)])).reshape(-1,)
322/253:
t=pd.DataFrame(columns=['vec'])
t=t.append({'vec':np.asarray(([np.zeros(300)])).reshape(-1,)},ignore_index=True)
t=t.append({'vec':np.asarray(([np.zeros(300)])).reshape(-1,)},ignore_index=True)
t
322/254: (t.vec.values).shape
322/255: tweet_vec.reshape(-1,).shape
322/256: tweet_vec.reshape(-1,1).shape
322/257: tweet_vec.reshape(-1,2).shape
322/258: (t.vec.values).reshape(-1,1)
322/259: (t.vec.values).reshape(-1,1).shape
322/260: (t.vec.values).reshape(-1,).shape
322/261: (t.vec.values).shape
322/262:   ?model.cosine_similarities
322/263: (t.vec.values).reshape(2,300)
322/264: np.zeros(300).shape
322/265: (t.vec.values).reshape(-1,1)
322/266: (t.vec.values).reshape(-1,1).shape
322/267: (t.vec.values).reshape(-1,1)
322/268: (t.vec.values).reshape(-1,1).reshape(2,300)
322/269: (t.vec.as_matrix())
322/270: (t.vec.as_matrix()).shape
322/271: (t.vec.values).shape
322/272: cos = find_most_similar_tweets(vectorized_input, vectorized_tweets)
322/273: vectorized_tweets.Vector.values.shape
322/274:   ?model.cosine_similarities
322/275: type(vectorized_tweets.Vector[0])
322/276: (vectorized_tweets.Vector[0]).shape
322/277: (vectorized_tweets.Vector[0])
322/278: (vectorized_tweets.Vector[0]).shape
322/279: (vectorized_tweets.Vector[0])
322/280: (vectorized_tweets.Vector[0]).shape
322/281: x=[[1,2,3],[2,3,4]]
322/282: x=[[1,2,3],[2,3,4]]x
322/283:
x=[[1,2,3],[2,3,4]]
x
322/284:
x=[[1,2,3],[2,3,4]]
type(x)
322/285:
x=[[1,2,3],[2,3,4]]
np.asarray(x)
322/286: vectorized_tweets.Vector.shape
322/287: type(vectorized_tweets.Vector)
322/288: vectorized_tweets.Vector.values
322/289: vectorized_tweets.Vector
322/290: pd.asarray(vectorized_tweets.Vector)
322/291: pd.asarray(vectorized_tweets.Vector.values)
322/292: pd.asarray(vectorized_tweets.Vector[0])
322/293: type(vectorized_tweets.Vector[0])
322/294: pd.asarray(vectorized_tweets.Vector[0])
322/295: pd.asarray(vectorized_tweets.Vector[0].values)
322/296: pd.asarray(vectorized_tweets.Vector.values)
322/297: type(vectorized_tweets.Vector.values)
322/298: np.vstack(vectorized_tweets.Vector[0])
322/299: type(np.vstack(vectorized_tweets.Vector[0]))
322/300: np.vstack(np.vstack(vectorized_tweets.Vector[0]))
322/301: (np.vstack(vectorized_tweets.Vector[0]))
322/302: np.vstack(np.vstack(vectorized_tweets.Vector[0]))
322/303: (np.vstack(vectorized_tweets.Vector[0])).shape
322/304: ((vectorized_tweets.Vector[0])).shape
322/305: (np.vstack(vectorized_tweets.Vector[0])).shape
322/306: vectorized_tweets.Vector[0]
322/307: (np.vstack(vectorized_tweets.Vector[0]))
322/308: (np.vstack(vectorized_tweets.Vector[0]))==vectorized_tweets.Vector[0]
322/309: (np.stack(vectorized_tweets.Vector[0]))==vectorized_tweets.Vector[0]
322/310: vectorized_tweets.Vector.values
322/311: vectorized_tweets.Vector
322/312: vectorized_tweets.Vector[0]
322/313: vectorized_tweets.Vector.tolist()
322/314: np.stack(vectorized_tweets.Vector.tolist())
322/315: np.stack(vectorized_tweets.Vector.tolist()).tolist()
322/316: len(np.stack(vectorized_tweets.Vector.tolist()).tolist())
322/317: (vectorized_tweets.Vector.tolist()).tolist()
322/318: (vectorized_tweets.Vector.tolist())
322/319: np.stack(vectorized_tweets.Vector.tolist())
322/320: np.stack(vectorized_tweets.Vector.tolist()).shape
322/321: np.stack(vectorized_tweets.Vector[0].tolist()).shape
322/322: (vectorized_tweets.Vector.values).shape
322/323: (vectorized_tweets.Vector.values).shape
322/324: vectorized_tweets.Vector[0].shape
322/325: vectorized_tweets.Vector.apply(lambda x: x.tolist() )
322/326: vectorized_tweets.V=vectorized_tweets.Vector.apply(lambda x: x.tolist() )
322/327: vectorized_tweets.V
322/328: np.stack(vectorized_tweets.V).shape
322/329: np.vstack(vectorized_tweets.V).shape
322/330: np.vstack(vectorized_tweets.V)
322/331: np.vstack(vectorized_tweets.V).shape
322/332: vectorized_tweets.C=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
322/333: vectorized_tweets.C.shape
322/334: np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
322/335:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    x=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    return x
322/336: cos = find_most_similar_tweets(vectorized_input, vectorized_tweets)
322/337: cos
322/338: ?model.cosine_similarities
322/339: vectorized_tweets.columns
322/340:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = cos
    return vectorized_tweets
322/341: vectorized_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
322/342: vectorized_tweets.columns
322/343: vectorized_tweets
323/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
323/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
323/3:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
323/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
323/5:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
323/6:
def vectorize_latest_tweets(tweets):
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.Normalized = [tweet.split() for tweet in tweets.Normalized]
    tweets=tweets[0:2]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
323/7:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
323/8:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
323/9:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = cos
    return vectorized_tweets
323/10:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)

#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)

vectorized_tweets = vectorize_latest_tweets(tweets)
user_input = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'
vectorized_input = vectorize_user_input(user_input)
vectorized_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
323/11: vectorized_tweets
323/12:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)

#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)

vectorized_tweets = vectorize_latest_tweets(tweets)
user_input = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'
vectorized_input = vectorize_user_input(user_input)
323/13: cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
323/14: cos_similar_tweets
323/15:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = cos
    return cos
323/16: cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
323/17: cos_similar_tweets
323/18: round(cos_similar_tweets,8)
323/19: cos_similar_tweets
323/20: round(cos_similar_tweets,8)
323/21: np.round(cos_similar_tweets,8)
323/22:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    return vectorized_tweets
323/23: cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
323/24: cos_similar_tweets
323/25:
pd.options.display.precision = 0
cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
323/26: cos_similar_tweets
323/27:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    pd.options.display.precision = 0
    return vectorized_tweets
323/28:
pd.options.display.precision = 0
cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
323/29: cos_similar_tweets
323/30:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    pd.options.display.precision = 0
    return cos
323/31:
pd.options.display.precision = 0
cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
323/32: cos_similar_tweets
323/33: vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos_similar_tweets,10)
323/34: vectorized_tweets
324/1: tweets.shape
324/2:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
324/3:
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#api = tweepy.API(auth)
324/4:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
324/5:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
324/6:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
324/7:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
tweets
324/8:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import os
324/9:
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#api = tweepy.API(auth)
324/10:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
324/11:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
324/12:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
324/13:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
tweets
326/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string, unicodedata
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
326/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_"+topic
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic]) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/3:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        query_fname = format_filename(query)
        self.saveFile = open("%s/stream_%s.json" % (data_dir, query_fname), 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
326/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
326/5:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
326/6:
def vectorize_latest_tweets(tweets):
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.Normalized = [tweet.split() for tweet in tweets.Normalized]
    tweets=tweets[0:2]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
326/7:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
326/8:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    return vectorized_tweets
326/9:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)

#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)

vectorized_tweets = vectorize_latest_tweets(tweets)
user_input = 'The 2016 United States presidential election was the 58th quadrennial American presidential..'
vectorized_input = vectorize_user_input(user_input)
326/10:

cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
326/11: cos_similar_tweets
326/12:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    return cos
326/13:

cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
326/14: cos_similar_tweets
326/15: vectorized_tweets
326/16: vectorized_tweets.Raw_tweet
326/17: vectorized_tweets.Raw_tweet[0]
326/18: vectorized_tweets
326/19: vectorized_tweets.Normalized_tweet[0]
326/20:
user_input = 'food is very delicious'
vectorized_input = vectorize_user_input(user_input)
326/21:

cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
326/22: cos_similar_tweets
326/23: tweets
326/24: tweets.Normalized[0]
326/25: tweets.Normalized[1]
326/26: tweets.Normalized[2]
326/27: tweets.Normalized[1]
326/28: tweets.Normalized[2]
326/29: tweets.Normalized[1]
326/30: tweets.Normalized[2]
326/31: tweets
326/32: tweets.tweet[0]
326/33: tweets.tweet[1]
326/34: tweets.tweet[2]
326/35:
def vectorize_latest_tweets(tweets):
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.Normalized = [tweet.split() for tweet in tweets.Normalized]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
326/36:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    return vectorized_tweets
326/37:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    return vectorized_tweets
326/38:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ' Asian food'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/39:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Asian food'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/40: twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
326/41:
twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
twitter_stream
326/42:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food'
time_limit=20
file_name = "stream_"+topic
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/43: 'Asian food'.split()
326/44: 'food'.split()
326/45:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open("%s stream_topic.json" % (data_dir), 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
326/46:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream_topic"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=topic.split()) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/47:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Asian food'
time_limit=20
file_name = "stream_topic"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/48:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food'
time_limit=20
file_name = "stream_topic"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/49:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open("%s stream.json" % (data_dir), 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
326/50:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
326/51:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/52:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=topic.split()) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/53:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/54:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food Asian'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/55: tweets
326/56: tweets.tweet[0]
326/57: tweets.iloc[0,'tweet']
326/58: tweets.loc[0,'tweet']
326/59: tweets.loc[:,'tweet']
326/60: tweets.loc[:,'tweet'][0]
326/61: tweets.loc[1,'tweet']
326/62: tweets.loc[2,'tweet']
326/63: tweets.loc[4,'tweet']
326/64: tweets.iloc[0]
326/65: tweets.iloc[0].tweet
326/66: 'shahla is a good girl'.split()
326/67:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=topic # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/68:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/69:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ['food Asian'] #it can be a list of topics,  comman means 'or'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/70:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ['food'] #it can be a list of topics,  comman means 'or'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/71:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ['italy food'] #it can be a list of topics,  comman means 'or'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/72:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    #data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/73:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ['italy food'] #it can be a list of topics,  comman means 'or'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/74:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=topic, language='en') # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    #data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/75:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=topic, language='en') # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    #data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/76:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ['food'] #it can be a list of topics,  comman means 'or'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/77:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=topic, lan='en') # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    #data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/78:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ['food'] #it can be a list of topics,  comman means 'or'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/79:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/80:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ['food'] #it can be a list of topics,  comman means 'or'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/81:
track_list='food asian chineese'
[k for k in track_list.split(',')]
326/82:
track_list='food asian chineese, dimsum'
[k for k in track_list.split(',')]
326/83: track_list.split()
326/84:
track_list='food asian chineese dimsum'
[k for k in track_list.split(',')]
326/85: track_list.split()
326/86:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track=[topic.split(',')]) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/87:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ['food asia'] #it can be a list of topics,  comman means 'or'
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, topic)
326/88:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/89:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = ['food asia'] #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/90:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food asia' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/91: data = pd.read_json(data_dir+file_name+".json",lines=True)
326/92: data.columns
326/93: data
326/94:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food diet' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/95: data = pd.read_json(data_dir+file_name+".json",lines=True)
326/96: data
326/97:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'trump president' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/98: data = pd.read_json(data_dir+file_name+".json",lines=True)
326/99: data
326/100:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string, unicodedataf
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
326/101:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    if data.empty:
        raise Exception('There is no data with the topic provided')
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
        
    return tweets
326/102:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food asia' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/103:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food asia' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/104:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    if data.empty:
        raise Exception('There is no data with the topic provided')
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()

        return tweets
326/105:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food asia' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/106:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
326/107: data = pd.read_json(data_dir+file_name+".json",lines=True)
326/108: data
326/109:
if data.empty:
    print('d')
326/110:
if data.empty:
    print('d')
else: print('dd')
326/111:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/112: data = pd.read_json(data_dir+file_name+".json",lines=True)
326/113:
if data.empty:
    print('d')
else: print('dd')
326/114: data
326/115:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    if data.empty:
        print('empty')
        raise Exception('There is no data with the topic provided')
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()

        return tweets
326/116:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food asia' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/117:
def f(st):
    if st=='shahla':
        raise Exception('shahal')
    else: 
        print('haha')
326/118:
def f(st):
    if st=='shahla':
        raise Exception('shahal')
    else: 
        print('haha')
        return st+'haha'

f('shahla')
326/119:
def f(st):
    if st=='shahla':
        raise Exception('shahal')
    else: 
        print('haha')
        return st+'haha'

st=f('shahla')
326/120:
def f(st):
    if st=='shahla':
        #raise Exception('shahal')
        print('shahla')
    else: 
        print('haha')
        return st+'haha'

st=f('shahla')
326/121:
def f(st):
    if st=='shahla':
        raise Exception('shahal')
    else: 
        print('haha')
        return st+'haha'

st=f('shahla')
326/122: data
326/123:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food asia' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/124: data = pd.read_json(data_dir+file_name+".json",lines=True)
326/125: data
326/126: data.empty
326/127:
if data.empty:
        print('empty')
        raise Exception('There is no data with the topic provided')
    else:
        print('dd')
326/129:
if data.empty:
        print('empty')
        raise Exception('There is no data with the topic provided')
else:
        print('dd')
326/130:
if data.empty:
    print('empty')
    raise Exception('There is no data with the topic provided')
else:
    print('dd')
326/131:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    if data.empty:
        print('empty')
        raise Exception('There is no data with the topic provided')
    else:
        return data
326/132:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food asia' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets = get_latest_tweets(data_dir, auth, time_limit, track_list)
326/133:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    if data.empty:
        return True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()

        return tweets
326/134: data
326/135: tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
326/136:
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
tweets
326/137:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()

return no_data, tweets
326/138:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()

return no_data and  tweets
326/139:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()

return no_data, tweets
326/140:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()

return  tweets
326/141:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()

return tweets
326/142:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
return tweets
326/143:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
return tweets, no_data
326/144:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    return no_data
326/145:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
    return tweets, no_data
326/146:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food asia' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic:'+topic+'in the last'+ time_limit)
    continue
else:
    print('remainder of the code here')
326/147:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food asia' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=20
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic:'+topic+'in the last'+ time_limit)
else:
    print('remainder of the code here')
326/148:
if no_data:
    print('There is no data with topic:'+topic+'in the last'+ str(time_limit))
else:
    print('remainder of the code here')
326/149:
if no_data:
    print('There is no data with topic:'+topic+'in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/150:

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'food france, french food' #it can be a list of topics,  comman means 'or'
track_list=[k for k in topic.split(',')]
time_limit=10
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
#-----------------------------------------------
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic:'+topic+'in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/151:
if no_data:
    print('There is no data with topic:'+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/152:
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/153:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'trump Iran, trump iranian, trump middle east' #it can be a list of topics,  comman means 'or'
time_limit=10
326/154:
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
326/155:
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/156: tweets
326/157: tweets.loc['tweets']
326/158: tweets.loc[0]
326/159: tweets.loc[0].tweet
326/160: tweets.loc[1].tweet
326/161: tweets.loc[2].tweet
326/162: tweets.loc[3].tweet
326/163: tweets.loc[4].tweet
326/164: tweets.loc[5].tweet
326/165: tweets.shape
326/166:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'middle east' #it can be a list of topics,  comman means 'or'
time_limit=10
326/167:
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
326/168:
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/169: tweets.shape
326/170:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'middle east' #it can be a list of topics,  comman means 'or'
time_limit=20
326/171:
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
326/172: tweets.shape
326/173:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'middle east' #it can be a list of topics,  comman means 'or'
time_limit=20
326/174:
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
326/175:
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/176: tweets.shape
326/177: tweets
326/178: tweets.loc[0].tweet
326/179: tweets.loc[1].tweet
326/180: tweets.loc[2].tweet
326/181: tweets.loc[3].tweet
326/182:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=20
326/183:
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
326/184:
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/185: tweets.loc[3].tweet
326/186: vectorized_tweets = vectorize_latest_tweets(tweets)
326/187: tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
326/188: tweets.tweet
326/189: tweets.tweet.dtype
326/190: tweets.tweet= tweets.tweet.astype(str)
326/191: tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
326/192: tweets
326/193:
def vectorize_latest_tweets(tweets):
    tweets.tweet= tweets.tweet.astype(str)
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.Normalized = [tweet.split() for tweet in tweets.Normalized]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
326/194: vectorized_tweets = vectorize_latest_tweets(tweets)
326/195:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
326/196:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
    return tweets, no_data
326/197:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
326/198:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
326/199:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
326/200:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
326/201:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
326/202:
def vectorize_latest_tweets(tweets):
    tweets.tweet= tweets.tweet.astype(str)
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.Normalized = [tweet.split() for tweet in tweets.Normalized]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
326/203:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
326/204:
def find_most_similar_tweets(vectorized_input, vectorized_tweets):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    return vectorized_tweets
326/205:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
326/206:
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
326/207:
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/208:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
326/209: tweets=tweets[1:3]
326/210: tweets=tweets[1:3]
326/211: vectorized_tweets = vectorize_latest_tweets(tweets)
326/212: vectorized_input = vectorize_user_input(user_input)
326/213: cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets)
326/214: cos_similar_tweets
326/215: tweets
326/216: cos_similar_tweets.Similarity_score.nlargest(3, keep='all')
326/217:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
326/218:
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
326/219:
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
326/220: tweets
326/221: tweets
326/222: tweets.dropna(axis='tweet')
326/223: tweets
326/224: tweets.dropna(axis='tweet')
326/225: tweets.dropna(subset=['tweet'])
326/226:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    return tweets, no_data
326/227: tweets.shape
326/228: t=tweets[1:5]
326/229:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
326/230:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
x
326/231:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
t.x=x
326/232:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
t.x=x

t
326/233:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
t.time

t
326/234:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
t.time
326/235:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
t.time.nlargest(3, keep='all'))
326/236:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
t.time.nlargest(3, keep='all')
326/237:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
x.nlargest(3, keep='all')
326/238:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
t
x.nlargest(3, keep='all')
326/239:
t=tweets[1:5]
x=pd.Series([1,2,3,5,3])
t
326/240:
df = pandas.DataFrame(
[[1,datetime.datetime(2018, 1, 1), 'New York'], 
 [1, datetime.datetime(2018, 1, 20), 'Rio de Janeiro'],
 [1, datetime.datetime(2018, 2, 13), 'London'],
 [2, datetime.datetime(2017, 6, 12), 'Seatle'],
 [2, datetime.datetime(2016, 10, 10), 'New Mexico'],
 [3, datetime.datetime(2017, 9, 19), 'Sao Paulo'],
 [3, datetime.datetime(2015, 12, 11), 'Bangladesh']]],
columns=['PERSON ID', 'MOVING DATE', 'PLACE']
)
326/241:
df = pandas.DataFrame(
[[1,datetime.datetime(2018, 1, 1), 'New York'], 
 [1, datetime.datetime(2018, 1, 20), 'Rio de Janeiro'],
 [1, datetime.datetime(2018, 2, 13), 'London'],
 [2, datetime.datetime(2017, 6, 12), 'Seatle'],
 [2, datetime.datetime(2016, 10, 10), 'New Mexico'],
 [3, datetime.datetime(2017, 9, 19), 'Sao Paulo'],
 [3, datetime.datetime(2015, 12, 11), 'Bangladesh']],
columns=['PERSON ID', 'MOVING DATE', 'PLACE']
)
326/242:
df = pandas.DataFrame(
[[1,datetime.datetime(2018, 1, 1), 'New York'], 
 [1, datetime.datetime(2018, 1, 20), 'Rio de Janeiro'],
 [1, datetime.datetime(2018, 2, 13), 'London'],
 [2, datetime.datetime(2017, 6, 12), 'Seatle'],
 [2, datetime.datetime(2016, 10, 10), 'New Mexico'],
 [3, datetime.datetime(2017, 9, 19), 'Sao Paulo'],
 [3, datetime.datetime(2015, 12, 11), 'Bangladesh'],
columns=['PERSON ID', 'MOVING DATE', 'PLACE']
)
326/243:
df = pandas.DataFrame(
[1,datetime.datetime(2018, 1, 1), 'New York'], 
 [1, datetime.datetime(2018, 1, 20), 'Rio de Janeiro'],
 [1, datetime.datetime(2018, 2, 13), 'London'],
 [2, datetime.datetime(2017, 6, 12), 'Seatle'],
 [2, datetime.datetime(2016, 10, 10), 'New Mexico'],
 [3, datetime.datetime(2017, 9, 19), 'Sao Paulo'],
 [3, datetime.datetime(2015, 12, 11), 'Bangladesh'],
columns=['PERSON ID', 'MOVING DATE', 'PLACE']
)
326/244:
df = pd.DataFrame(
[1,datetime.datetime(2018, 1, 1), 'New York'], 
 [1, datetime.datetime(2018, 1, 20), 'Rio de Janeiro'],
 [1, datetime.datetime(2018, 2, 13), 'London'],
 [2, datetime.datetime(2017, 6, 12), 'Seatle'],
 [2, datetime.datetime(2016, 10, 10), 'New Mexico'],
 [3, datetime.datetime(2017, 9, 19), 'Sao Paulo'],
 [3, datetime.datetime(2015, 12, 11), 'Bangladesh'],
columns=['PERSON ID', 'MOVING DATE', 'PLACE']
)
326/245:
df = pd.DataFrame(
[1,3, 'New York'], 
 [1, 7, 'Rio de Janeiro'],
 [1, 9, 'London'],
 [2, 3, 'Seatle'],
 [2, 6, 'New Mexico'],
 [3, 2, 'Sao Paulo'],
 [3, 3, 'Bangladesh'],
columns=['PERSON ID', 'MOVING DATE', 'PLACE']
)
326/246:
def find_most_similar_tweets(vectorized_input, vectorized_tweets,topn):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='Similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
327/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
327/2:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
327/3:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    return tweets, no_data
327/4:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
327/5:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
327/6:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
327/7:
def vectorize_latest_tweets(tweets):
    tweets.tweet= tweets.tweet.astype(str)
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.Normalized = [tweet.split() for tweet in tweets.Normalized]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
327/8:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
327/9:
def find_most_similar_tweets(vectorized_input, vectorized_tweets,topn):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='Similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
327/10:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
327/11:
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)

# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
327/12:
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
327/13: vectorized_tweets = vectorize_latest_tweets(tweets)
327/14: tweets=tweet[1:3]
327/15: tweets=tweets[1:3]
327/16: vectorized_tweets = vectorize_latest_tweets(tweets)
327/17:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
327/18: tweets=tweets[1:3]
327/19: vectorized_tweets = vectorize_latest_tweets(tweets)
327/20:
def vectorize_latest_tweets(tweets):
    #tweets.tweet= tweets.tweet.astype(str)
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.Normalized = [tweet.split() for tweet in tweets.Normalized]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
327/21: vectorized_tweets = vectorize_latest_tweets(tweets)
327/22: vectorized_input = vectorize_user_input(user_input)
327/23: tweets.loc['Normalized'] = [tweet.split() for tweet in tweets.Normalized]
327/24:
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
327/25: tweets.loc['Normalized'] = [tweet.split() for tweet in tweets.Normalized]
327/26: tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
327/27:
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
327/28: tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
327/29:
def vectorize_latest_tweets(tweets):
    tweets.tweet= tweets.tweet.astype(str)
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
327/30:
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    print('There is no data with topic: '+topic+' in the last '+ str(time_limit))
else:
    print('remainder of the code here')
327/31:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
327/32: tweets=tweets[1:3]
327/33: vectorized_tweets = vectorize_latest_tweets(tweets)
327/34: vectorized_input = vectorize_user_input(user_input)
327/35: cos_similar_tweets = find_most_similar_tweets(vectorized_input, vectorized_tweets,10)
327/36: cos_similar_tweets
327/37:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
327/38:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    return tweets, no_data
327/39:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
327/40:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
327/41:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
327/42:
def vectorize_latest_tweets(tweets):
    tweets.tweet= tweets.tweet.astype(str)
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
327/43:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
327/44:
def find_most_similar_tweets(vectorized_input, vectorized_tweets,topn):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='Similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
327/45:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)

    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
        #downloaded pretrained model
        model = gensim.models.KeyedVectors.load_word2vec_format \ 
        ('GoogleNews-vectors-negative300.bin', binary = True)
        vectorized_tweets = vectorize_latest_tweets(latest_tweets)
        vectorized_user_input = vectorize_user_input(user_input)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
327/46:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)

    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
#       downloaded pretrained model
        model = gensim.models.KeyedVectors.load_word2vec_format \ 
        ('GoogleNews-vectors-negative300.bin', binary = True)
        vectorized_tweets = vectorize_latest_tweets(latest_tweets)
        vectorized_user_input = vectorize_user_input(user_input)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
327/47:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)

    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
#       downloaded pretrained model
        model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
        vectorized_tweets = vectorize_latest_tweets(latest_tweets)
        vectorized_user_input = vectorize_user_input(user_input)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
327/48:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendation = process_user_input(user_input, time_limit, topic):
327/49:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
328/1: import numpty as np
328/2: import numpy as np
328/3:
import numpy as np
z=np.array([1,3,4,.5])
328/4: z
328/5: z.shapE
328/6: z.shape
328/7:
import numpy as np
z=np.array([1 3 4 .5])
328/8:
import numpy as np
z=np.array([1,3,4,.5])
328/9: A=np.reciprocal(np.exp(-z)+1)
328/10: A
328/11: X=np.array([1,2,3;2,4,3])
328/12: X=np.array([1,2,3],[2,4,3])
328/13: X=np.array([[1,2,3],[2,4,3]])
328/14: X
328/15: X.shape
328/16: A.shape
328/17: X=np.array([[1,2,3,4],[2,4,3,7]])
328/18: X.shape
328/19: A.shape
328/20: XA
328/21: X.A
328/22: np.dot(X,A)
328/23: np.dot(A,X)
328/24: np.dot(A.T,X)
328/25: np.dot(A.T,X.T)
328/26: 1/2np.dot(A.T,X.T)
328/27: 1/2*np.dot(A.T,X.T)
328/28: np.dot(A.T,X.T)
328/29: w=np.zeros(2,1)
328/30: w=np.zeros((2,1))
328/31: X
328/32: cal=X.sum(axis=0)
328/33: cal.shape
328/34: cal
328/35: cal.reshape(1,4)
328/36: c=cal.reshape(1,4)
328/37: c
328/38: cal.reshape(4,1)
328/39: cal.shape
328/40:
cal=X.sum(axis=0)
cal
328/41: c.shape
328/42: c
329/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
329/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    return tweets, no_data
329/3:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
329/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
329/5:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
329/6:
def vectorize_latest_tweets(tweets):
    tweets.tweet= tweets.tweet.astype(str)
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
329/7:
def vectorize_user_input(user_input):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
329/8:
def find_most_similar_tweets(vectorized_input, vectorized_tweets,topn):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='Similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
329/9:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)

    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
#       downloaded pretrained model
        model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
        vectorized_tweets = vectorize_latest_tweets(latest_tweets)
        vectorized_user_input = vectorize_user_input(user_input)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
329/10:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
329/11:
#       downloaded pretrained model
#       Print("model loading...")
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
print("model loading successful!")
329/12:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    return vectorized_tweets
329/13:
def vectorize_user_input(user_input, model):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
329/14:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
329/15:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
329/16:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    return tweets, no_data
329/17:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
329/18:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
329/19:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
329/20:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
329/21:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
329/22:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
329/23:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    print('preprocessing finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
329/24:
def vectorize_user_input(user_input, model):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
329/25:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
329/26:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
330/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
330/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
330/3:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
330/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
330/5:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
330/6:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    print('preprocessing finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        print('tweet vectorization starting...')
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        print('vectorization finished')
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
330/7:
def vectorize_user_input(user_input, model):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
330/8:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='Similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
330/9:
#       downloaded pretrained model
print("model loading...")
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
print("model loading successful!")
330/10:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
330/11:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
331/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
331/2:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
331/3:
#       downloaded pretrained model
print("model loading...")
toc=time.time()
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
tic=time.time()
print(toc-tic)
print("model loading successful!")
331/4: print((171.79195070266724)/60)
331/5:
#       downloaded pretrained model
print("model loading...")
toc=time.time()
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
tic=time.time()
print((tic-toc)/60)
print("model loading successful!")
331/6:
#       downloaded pretrained model
print("model loading...")
toc=time.time()
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
tic=time.time()
print(str((tic-toc)/60)+'minutes')
print("model loading successful!")
331/7:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tic=time.time()
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    toc=time.time()
    print((toc-tic)/60)
    print('preprocessing finished!')
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    tic=time.time()
    print((tic-toc)/60)
    print('spliting finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        print('tweet vectorization starting...')
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        print('vectorization finished')
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
331/8:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tic=time.time()
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    toc=time.time()
    print((toc-tic)/60)
    print('preprocessing finished!')
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    tic=time.time()
    print((tic-toc)/60)
    print('spliting finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        print('tweet vectorization starting...')
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        print('vectorization finished')
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
331/9:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
331/10:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
331/11:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
331/12:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
331/13:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
331/14:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
331/15:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
331/16:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    print('preprocessing finished!')
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    print('spliting finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        print('tweet vectorization starting...')
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        print('vectorization finished')
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
331/17:
def vectorize_user_input(user_input, model):
    
    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (Normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}
    return vectorized_input
331/18:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['Similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='Similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
331/19:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tic=time.time()
    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    toc=time.time()
    print((toc-tic)/60)
    print('preprocessing finished!')
    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]
    tic=time.time()
    print((tic-toc)/60)
    print('spliting finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["Raw_tweet","Normalized_tweet","Vector"])

    for index, tweet in tweets.iterrows():
        print('tweet vectorization starting...')
        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]
        print('vectorization finished')
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
331/20:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
331/21:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
331/22: tweets
331/23: latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
331/24:
 data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
331/25:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
331/26:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
331/27: tweets=latest_tweets
331/28: tweets=latest_tweets.dropna
331/29: tweets
331/30: tweets.dropna(subset=['tweet'])
331/31: tweets
331/32: tweets.dropna(subset=['tweet'])
331/33: latest_tweets.dropna
331/34: tweets=latest_tweets.copy()
331/35: tweets.dropna
331/36: tweets=tweets.dropna
331/37:
tweets=tweets.dropna
tweets
331/38: latest_tweets.dropna.copy()
331/39: latest_tweets=latest_tweets.dropna()
331/40: latest_tweets
331/41: latest_tweets.loc[:,'Normalized'=latest_tweets.tweet.apply(lambda x: x.split())
331/42: latest_tweets.loc[:,'Normalized']=latest_tweets.tweet.apply(lambda x: x.split())
331/43:
tic=time.time()
latest_tweets.loc[:,'Normalized']=llatest_tweets.loc[:,'Normalized'].apply(lambda x: x.split())
toc=time.time()
print(toc-tic)
331/44:
tic=time.time()
latest_tweets.loc[:,'Normalized']=latest_tweets.loc[:,'Normalized'].apply(lambda x: x.split())
toc=time.time()
print(toc-tic)
331/45:
tic=time.time()
latest_tweets.loc[:,'Normalized']=latest_tweets.loc[:,'Normalized'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/46: latest_tweets
331/47:
tic=time.time()
latest_tweets.loc[:,'Normalized']=latest_tweets.loc[:,'Normalized'].apply(lambda tweet: word.split() for word in tweet)
toc=time.time()
print(toc-tic)
331/48:
tic=time.time()
latest_tweets.loc[:,'Normalized']=latest_tweets.loc[:,'Normalized'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/49:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
331/50: latest_tweets.columsn
331/51: latest_tweets.columns
331/52:
tic=time.time()
latest_tweets.loc[:,'Normalized']=latest_tweets.loc[:,'tweet'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/53: latest_tweets.tweet.dtype
331/54: latest_tweets.tweet[0]
331/55:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
331/56: tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
331/57:
tic=time.time()
tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+'minutes to normalize')
331/58:
tic=time.time()
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+'minutes to normalize')
331/59:
tic=time.time()
latest_tweets.loc[:,'normalized']=latest_tweets.loc[:,'tweet'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/60:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
latest_tweets.loc[:,'normalized']=latest_tweets.loc[:,'tweet'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/61:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+'minutes to normalize')
331/62:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+' minutes to normalize')
331/63:
tic=time.time()
latest_tweets.loc[:,'normalized']=latest_tweets.loc[:,'normalized'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/64: tweets
331/65:
tic=time.time()
latest_tweets.loc[:,'normalized']=latest_tweets.normalized.apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/66:
tic=time.time()
tweets.loc[:,'normalized']=tweets.loc[:,'normalized'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/67:
tic=time.time()
tweets.loc[:,'normalized'] = [tweet.split() for tweet in tweets.Normalized]
toc=time.time()
print(toc-tic)
331/68:
tic=time.time()
tweets.loc[:,'normalized'] = [tweet.split() for tweet in tweets.normalized]
toc=time.time()
print(toc-tic)
331/69:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+' minutes to normalize')
331/70:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
331/71:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+' minutes to normalize')
331/72:
tic=time.time()
tweets.loc[:,'normalized1']=tweets.loc[:,'normalized'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/73:
tic=time.time()
tweets.loc[:,'normalized2'] = [tweet.split() for tweet in tweets.normalized]
toc=time.time()
print(toc-tic)
331/74:
tic=time.time()
tweets.loc[:,'normalized2'] = [tweet.split() for tweet in tweets.normalized]
toc=time.time()
print(toc-tic)
331/75: tweets.loc[:,'normalized'] = [tweet.split() for tweet in tweets.normalized]
331/76:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
331/77:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+' minutes to normalize')
331/78:
tic=time.time()
tweets.loc[:,'normalized1']=tweets.loc[:,'normalized'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/79:
tic=time.time()
tweets.loc[:,'normalized2'] = [tweet.split() for tweet in tweets.normalized]
toc=time.time()
print(toc-tic)
331/80:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
331/81:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+' minutes to normalize')
331/82:
tic=time.time()
tweets.loc[:,'normalized1']=tweets.loc[:,'normalized'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
331/83:
tic=time.time()
tweets.loc[:,'normalized2'] = [tweet.split() for tweet in tweets.normalized]
toc=time.time()
print(toc-tic)
331/84: vectorized_tweets = pd.DataFrame(columns=["raw_tweet","normalized_tweet","vector"])
331/85: word_vecs=tweets.loc[:,'vectors'] = tweets.normalized.apply(lambda tweet:[model[word] for word in (tweet and model.vocab)])
331/86:
tic=time.time()
word_vecs=tweets.loc[:,'vectors'] = tweets.normalized.apply(lambda tweet:[model[word] for word in (tweet and model.vocab)])
toc=time.time()
print(toc-tic)
331/87:
tic=time.time()
tweets.loc[:,'vectors'] = tweets.normalized.apply(lambda tweet:[model[word] for word in (tweet and model.vocab)])
toc=time.time()
print(toc-tic)
331/88:
tic=time.time()
for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
toc=time.time()
print(toc-tic)
332/1:
tic=time.time()
for index, tweet in tweets.iterrows():
    word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
toc=time.time()
print(toc-tic)
333/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
333/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
333/3:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
333/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
333/5:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
333/6:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    print('preprocessing finished!')
    tweets.loc[:,'normalized'] = [tweet.split() for tweet in tweets.normalized]
    print('spliting finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["raw_tweet","normalized_tweet","vector"])

    for index, tweet in tweets.iterrows():
        print('tweet vectorization starting...')
        word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
        print('vectorization finished')
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'raw_tweet': tweet.tweet, 'normalized_tweet': tweet.Normalized,'vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
333/7:
def vectorize_user_input(user_input, model):
    
    normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': tweet_vec}
    return vectorized_input
333/8:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
333/9:
#       downloaded pretrained model
print("model loading...")
toc=time.time()
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
tic=time.time()
print(str((tic-toc)/60)+'minutes')
print("model loading successful!")
333/10:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tic=time.time()
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to normalize')
    print('preprocessing finished!')
    tweets.loc[:,'normalized'] = [tweet.split() for tweet in tweets.Normalized]
    tic=time.time()
    print(str((tic-toc)/60)+'minutes to split')
    print('spliting finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["raw_tweet","normalized_tweet","vector"])

    for index, tweet in tweets.iterrows():
        print('tweet vectorization starting...')
        word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
        print('vectorization finished')
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'raw_tweet': tweet.tweet, 'normalized_tweet': tweet.normalized,'vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
333/11:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
333/12:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
333/13:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
333/14:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+' minutes to normalize')
333/15:
tic=time.time()
tweets.loc[:,'normalized1']=tweets.loc[:,'normalized'].apply(lambda tweet: [word.split() for word in tweet])
toc=time.time()
print(toc-tic)
333/16:
tic=time.time()
tweets.loc[:,'vectors'] = tweets.normalized1.apply(lambda tweet:[model[word] for word in (tweet and model.vocab)])
toc=time.time()
print(toc-tic)
333/17: model
333/18: type(model)
333/19: model.dtype
333/20: model
333/21: tweets=tweets[0:2]
333/22: tweets=tweets[0:2]
333/23:
tweets=tweets[0:2]
tweets
333/24:
tic=time.time()
tweets.loc[:,'normalized2'] = [tweet.split() for tweet in tweets.normalized]
toc=time.time()
print(toc-tic)
333/25:
tweets=tweets[0:2]
tweets
333/26:
tic=time.time()
tweets.loc[:,'normalized1']=tweets.loc[:,'normalized'].apply(lambda tweet: word.split() for word in tweet)
toc=time.time()
print(toc-tic)
333/27:
tic=time.time()
tweets.loc[:,'normalized1']=tweets.normalized.apply(lambda tweet: word.split() for word in tweet)
toc=time.time()
print(toc-tic)
333/28:
tic=time.time()
tweets.loc[:,'normalized1']=tweets.normalized.apply(lambda tweet:[word.split() for word in tweet])
toc=time.time()
print(toc-tic)
333/29:
tweets=tweets[0:2]
tweets
333/30:
tic=time.time()
tweets.loc[:,'normalized1']=tweets.normalized.apply(lambda tweet:tweet.split())
toc=time.time()
print(toc-tic)
333/31:
tweets=tweets[0:2]
tweets
333/32:
tic=time.time()
tweets.loc[:,'normalized2'] = [tweet.split() for tweet in tweets.normalized]
toc=time.time()
print(toc-tic)
334/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
334/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
334/3:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
334/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
334/5:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
334/6:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    print('preprocessing finished!')
    tweets.loc[:,'normalized'] = [tweet.split() for tweet in tweets.normalized]
    print('spliting finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["raw_tweet","normalized_tweet","vector"])

    for index, tweet in tweets.iterrows():
        print('tweet vectorization starting...')
        word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
        print('vectorization finished')
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'raw_tweet': tweet.tweet, 'normalized_tweet': tweet.Normalized,'vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
334/7:
def vectorize_user_input(user_input, model):
    
    normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()
    word_vecs = [model[word] for word in (normalized and model.vocab)]
    if len(word_vecs) == 0:
        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]
    else:
        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': tweet_vec}
    return vectorized_input
334/8:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
334/9:
#       downloaded pretrained model
print("model loading...")
toc=time.time()
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
tic=time.time()
print(str((tic-toc)/60)+'minutes')
print("model loading successful!")
334/10: ?model.word_vec
334/11: ?model.wv
334/12: type(model.vocab)
334/13: ?model.vocab
334/14: model.vocab['president']
334/15: type(model.vocab['president'])
334/16: ?model.wv
334/17: ?model.word_vec
334/18: type(model.vocab['president'])
334/19: type(model.vocab)
334/20: model.vocab
334/21: model.wv
334/22: model..self
334/23: model.self
334/24: model.wv
334/25:
tic=time.time()
model['president']
toc=time.time()
    print(str((toc-tic)/60)+'seconds to find a word')
334/26:
tic=time.time()
model['president']
toc=time.time()
print(str((toc-tic)/60)+'seconds to find a word')
334/27:
tic=time.time()
model['president']
toc=time.time()
print(str((toc-tic))+'seconds to find a word')
334/28:
tic=time.time()
model['pd']
toc=time.time()
print(str((toc-tic))+'seconds to find a word')
334/29:
tic=time.time()
model['hichi']
toc=time.time()
print(str((toc-tic))+'seconds to find a word')
334/30:
tic=time.time()
if 'hichi' in model.vocab:
    print('yes')
toc=time.time()
print(str((toc-tic))+'seconds to find a word')
334/31:
tic=time.time()
if 'hichi' in model.vocab:
    print('yes')
toc=time.time()
print(str((toc-tic)*1000)+'seconds to find a word')
334/32:
tic=time.time()
if 'hichi' in model.vocab:
    print('yes')
toc=time.time()
print(str((toc-tic)*10000)+'seconds to find a word')
334/33:
tic=time.time()
if 'hichi' in model.vocab:
    print('yes')
toc=time.time()
print(str((toc-tic)*1000)+'mili seconds to find a word')
334/34:
tic=time.time()
if 'hichi' in model.vocab:
    print('yes')
toc=time.time()
print(str((toc-tic)*1000)+' mili seconds to find a word')
334/35:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
334/36:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)
    else:
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
334/37:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
334/38:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+' minutes to normalize')
334/39:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
334/40:
tic=time.time()
tweets.tweet= tweets.tweet.astype(str)
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
toc=time.time()
print(str((toc-tic)/60)+' minutes to normalize')
334/41:
tic=time.time()
tweets.loc[:,'normalized1']=tweets.normalized.apply(lambda tweet:tweet.split())
toc=time.time()
print(toc-tic)
334/42:
tic=time.time()
tweets.loc[:,'normalized2'] = [tweet.split() for tweet in tweets.normalized]
toc=time.time()
print(toc-tic)
334/43:
tweets=tweets[0:2]
tweets
334/44:
tic=time.time()
tweets.loc[:,'vectors'] = tweets.normalized1.apply(lambda tweet:[model[word] for word in (tweet and model.vocab)])
toc=time.time()
print(toc-tic)
334/45:
tic=time.time()
tweets.loc[:,'vectors'] = tweets.normalized1.apply(lambda tweet:[model[word] for word in (tweet and model.vocab)])
toc=time.time()
print((toc-tic)/60)
334/46:
tic=time.time()
tweets.loc[:,'vectors'] = tweets.normalized1.apply(lambda tweet:[model[word] for word in (tweet and model.vocab)])
toc=time.time()
print((toc-tic)/60)
334/47: model['the']
334/48: model['the'].shape
334/49: np.array(model['the'])
334/50: np.array(model['the']).shape
334/51: model['the'].type
334/52: type(model['the'])
334/53: a=np.array[1,2]
334/54: a=np.array[[1,2]
334/55: a=np.array[[1,2]]
334/56: a=np.array[[1,2]]
334/57: a=np.zeros((3))
334/58:
a=np.zeros((3))
a
334/59:
a=np.zeros((3))
a.shape
334/60:
a=np.zeros((3))
a=a+1
334/61:
a=np.zeros((3))
a=a+1
b=b+(2,3,5)
334/62:
a=np.zeros((3))
a=a+1
a
334/63:
a=np.zeros((3))
a=a+1
b=np.array([1,3,4])
334/64:
a=np.zeros((3))
a=a+1
b=np.array([1,3,4])
a=a+b
334/65:
a=np.zeros((3))
a+=1
b=np.array([1,3,4])
a=a+b
334/66:
a=np.zeros((3))
a+=1
334/67:
a=np.zeros((3))
a+=1
a
334/68:
tic=time.time()
vec=np.zeros((300))
tweets.loc[:,'vectors'] = tweets.normalized1.apply(lambda tweet:vec+=[model[word] for word in (tweet and model.vocab)])
toc=time.time()
print((toc-tic)/60)
334/69:
tic=time.time()
vec=np.zeros((300))
tweets.loc[:,'vectors'] = tweets.normalized1.apply(lambda tweet:vec+=model[word] for word in (tweet and model.vocab))
toc=time.time()
print((toc-tic)/60)
334/70:
def topic_vec(tweet):
    vec=np.zeros((300))
    for word in (tweet and model.vocab)
    vec+=model[word] 
    return vec
tic=time.time()
vec=np.zeros((300))
tweets.loc[:,'vectors'] = tweets.normalized1.apply(lambda tweet:topic_vec(tweet))
toc=time.time()
print((toc-tic)/60)
334/71:
def topic_vec(tweet):
    vec=np.zeros((300))
    for word in (tweet and model.vocab):
        vec+=model[word] 
    return vec
tic=time.time()
vec=np.zeros((300))
tweets.loc[:,'vectors'] = tweets.normalized1.apply(lambda tweet:topic_vec(tweet))
toc=time.time()
print((toc-tic)/60)
334/72:
def topic_vec(tweet):
    vec=np.zeros((300))
    for word in (tweet and model.vocab):
        vec+=model[word] 
    return vec
tic=time.time()
vec=np.zeros((300))
tweets.loc[:,'vectors'] = tweets.normalized1.apply(lambda tweet:topic_vec(tweet))
toc=time.time()
print((toc-tic)/60)
334/73:
def vectorize_tweet(normalized_tweet):
    vec=np.zeros((300))
    for word in (normalized_tweet and model.vocab):
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
334/74:
def tweet_preprocess(row_tweet):
    return TwitterPreprocessor(row_tweet).fully_preprocess().text
334/75:
def vectorize_tweet(normalized_tweet):
    vec=np.zeros((300))
    for word in (normalized_tweet and model.vocab):
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
334/76:
def tweet_preprocess(row_tweet):
    return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]
334/77:
def vectorize_tweet(normalized_tweet):
    vec=np.zeros((300))
    for word in (normalized_tweet and model.vocab):
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
334/78:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tic=time.time()
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to normalize')
    tic=time.time()
    print('preprocessing finished!')
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to vectorize')
    print('tweets vectorizd!')
    return tweets
334/79:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
334/80:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
334/81: tweets.shape
334/82: tweets=tweets[0:2]
334/83:
tweets=tweets[0:2]
tweets.shape
334/84:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input)
    word_vec = vectorize_tweet(normalized)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
336/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
336/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
336/3:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
336/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
336/5:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
336/6:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)
    print('preprocessing finished!')
    tweets.loc[:,'normalized'] = [tweet.split() for tweet in tweets.normalized]
    print('spliting finished!')
    
    vectorized_tweets = pd.DataFrame(columns=["raw_tweet","normalized_tweet","vector"])

    for index, tweet in tweets.iterrows():
        print('tweet vectorization starting...')
        word_vecs = [model[word] for word in (tweet.normalized and model.vocab)]
        print('vectorization finished')
        if len(word_vecs) == 0:
            tweet_vec = [np.zeros(300)]
        else: 
            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))
        vectorized_tweets = vectorized_tweets.append \
        ({'raw_tweet': tweet.tweet, 'normalized_tweet': tweet.Normalized,'vector': tweet_vec}, ignore_index=True)
    print('tweets normalized!')
    return vectorized_tweets
336/7:
def tweet_preprocess(row_tweet):
    return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]
336/8:
def vectorize_tweet(normalized_tweet):
    vec=np.zeros((300))
    for word in (normalized_tweet and model.vocab):
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
336/9:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tic=time.time()
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to normalize')
    tic=time.time()
    print('preprocessing finished!')
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to vectorize')
    print('tweets vectorizd!')
    return tweets
337/1:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
337/2:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):
    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
#       find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
337/3:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
337/4:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
337/5:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
337/6:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
337/7:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        self.text = self.text.lower()
        return self
    def strip_html_tags(self):
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        return self
    
    def remove_accented_chars(self):
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        return self
    
    def expand_contractions(self):
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens)    
        return self
    
    def remove_hyphens(self):
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        return self

    def remove_hashtags(self):
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        return self

    def remove_single_letter_words(self):
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        return self
    def remove_stopwords(self, extra_stopwords=None):
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        return self
    def remove_numbers(self, preserve_years=False):
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        return self
    def remove_blank_spaces(self):
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        return self
337/8:
def tweet_preprocess(row_tweet):
    return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]
337/9:
def vectorize_tweet(normalized_tweet):
    vec=np.zeros((300))
    for word in (normalized_tweet and model.vocab):
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
337/10:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tic=time.time()
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to normalize')
    tic=time.time()
    print('preprocessing finished!')
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to vectorize')
    print('tweets vectorizd!')
    return tweets
337/11:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input)
    word_vec = vectorize_tweet(normalized)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
337/12:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
337/13:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
337/14:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        latest_tweets = latest_tweets[0:2]
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
337/15:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        latest_tweets = latest_tweets[0:2]
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
337/16:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
337/17:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = np.round(cos,10)
    decimals = pd.Series([8], index=['similarity_score'])
    vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
337/18:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    with open("twitter_credentials.json", "r") as file:  
         credentials = json.load(file)
    # Instantiate an object
    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        latest_tweets = latest_tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
337/19:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
337/20: recommendations
337/21: recommendations.tweet
337/22: recommendations.tweet[0]
337/23: recommendations.tweet
337/24: recommendations.loc['tweet']
337/25: recommendations.loc[:,'tweet']
337/26: recommendations.loc[0,'tweet']
337/27: recommendations.loc[:,'tweet'].shape
337/28: recommendations.loc[:,'tweet'][0:13]
337/29: recommendations
337/30: recommendations.tweet
337/31: recommendations.tweet[0]
337/32: recommendations.tweet
337/33:
with pd.option_context('display.max_colwidth', 100):
    print (recommendations)
337/34:
with pd.option_context('display.max_colwidth', 300):
    print (recommendations)
337/35:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)
    #decimals = pd.Series([8], index=['similarity_score'])
    #vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
337/36:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
337/37:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained and now filtering...")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
337/38:
with pd.option_context('display.max_colwidth', 300):
    print (recommendations)
337/39: model.cosine_similarities(model['president'], model['food'])
337/40: ?model.cosine_similarities
337/41: model['president'].shape
337/42: model['president'].reshape(-1,1)
337/43: model['president'].reshape(-1,1).shape
337/44: model['president'].reshape(1,-1).shape
337/45: model.cosine_similarities(model['president'], model['food'].reshape(1,-1))
337/46: model.cosine_similarities(model['president'], model['politics'].reshape(1,-1))
337/47: model.cosine_similarities(model['president'], model['white house'].reshape(1,-1))
337/48: model.cosine_similarities(model['president'], model['USA'].reshape(1,-1))
337/49: model.cosine_similarities(model['Iran'], model['USA'].reshape(1,-1))
337/50: model.cosine_similarities(model['canada'], model['USA'].reshape(1,-1))
337/51: model.cosine_similarities(model['bbc'], model['USA'].reshape(1,-1))
337/52: model.cosine_similarities(model['CNN'], model['USA'].reshape(1,-1))
337/53: model.cosine_similarities(model['iran'], model['USA'].reshape(1,-1))
337/54: model.cosine_similarities(model['Iran'], model['USA'].reshape(1,-1))
337/55: model.cosine_similarities(model['iran'], model['USA'].reshape(1,-1))
337/56: model.cosine_similarities(model['khomeini'], model['USA'].reshape(1,-1))
337/57: model.cosine_similarities(model['vice president'], model['USA'].reshape(1,-1))
337/58: model.cosine_similarities(model['bomb'], model['USA'].reshape(1,-1))
337/59: model.cosine_similarities(model['energy'], model['USA'].reshape(1,-1))
337/60: model.cosine_similarities(model['IT'], model['USA'].reshape(1,-1))
337/61: model.cosine_similarities(model['terrorism'], model['USA'].reshape(1,-1))
337/62: model.cosine_similarities(model['terrorism'], model['Iran'].reshape(1,-1))
337/63: model.cosine_similarities(model['terrorism'], model['Iran'].reshape(1,-1))
337/64: model.cosine_similarities(model['terrorism'], model['IRAN'].reshape(1,-1))
337/65: model.cosine_similarities(model['ELECTION'], model['USA'].reshape(1,-1))
337/66: model.cosine_similarities(model['election'], model['USA'].reshape(1,-1))
337/67: model.cosine_similarities(model['grass'], model['USA'].reshape(1,-1))
337/68: model.cosine_similarities(model['home'], model['USA'].reshape(1,-1))
337/69: latest_tweets
337/70: recommendations
337/71:
if !recommendations.empty:
    print('d')
337/72:
if not recommendations.empty:
    print('d')
337/73: model.cosinesimilarity(model['food'],model['USA'].reshape(1,-1))
337/74: model.cosine_similarity(model['food'],model['USA'].reshape(1,-1))
337/75: model.cosine_similarities(model['food'],model['USA'].reshape(1,-1))
337/76: model.cosine_similarities(model['food'],model['usa'].reshape(1,-1))
337/77: model.cosine_similarities(model['food'],model['USA'].reshape(1,-1))
337/78: ?model.closer_than
337/79:
latest_tweets = latest_tweets[0:1]
vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
337/80:
latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
latest_tweets = latest_tweets[0:1]
vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
337/81:
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
337/82:
latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
latest_tweets = latest_tweets[0:1]
vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
337/83:
track_list=[k for k in topic.split(',')]
latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
latest_tweets = latest_tweets[0:1]
vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
337/84: vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
337/85: vec_tweets
337/86: vec_tweets.shape
337/87:
def vectorize_tweet(normalized_tweet):
    vec=np.zeros((300))
    tic=time.time()
    for word in (normalized_tweet and model.vocab):
        vec+=model[word]
    toc=time.time()
    print('time to vectorize one tweet!'+str((tic-toc)/60))
    return preprocessing.normalize(vec.reshape(1,-1))
337/88:
def vectorize_tweet(normalized_tweet):
    vec=np.zeros((300))
    tic=time.time()
    for word in (normalized_tweet and model.vocab):
        vec+=model[word]
    toc=time.time()
    print('time to vectorize one tweet!'+str((toc-tic)/60))
    return preprocessing.normalize(vec.reshape(1,-1))
337/89:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
337/90:
with pd.option_context('display.max_colwidth', 300):
    print (recommendations)
337/91:
with pd.option_context('display.max_colwidth', 300):
    print (recommendations)
337/92:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
#recommendations = process_user_input(user_input, time_limit, topic)
track_list=[k for k in topic.split(',')]
file_name = "stream"

latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
#latest_tweets = latest_tweets[0:1]
# vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
# vectorized_user_input = vectorize_user_input(user_input, model)
# #find the top topn= 10  similar tweets
# recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
337/93:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
#recommendations = process_user_input(user_input, time_limit, topic)
track_list=[k for k in topic.split(',')]
file_name = "stream"

latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
latest_tweets
#latest_tweets = latest_tweets[0:1]
# vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
# vectorized_user_input = vectorize_user_input(user_input, model)
# #find the top topn= 10  similar tweets
# recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
337/94: no_data
337/95:
tweets = latest_tweets.copy()
tweets
337/96:
tweets = latest_tweets.copy()
tweets= tweets[0:1]
337/97:
tweets = latest_tweets.copy()
tweets= tweets[0:1]
tweets
337/98:
tweets.tweet= tweets.tweet.astype(str)
print('preprocessing starting...')
tic=time.time()
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))
toc=time.time()
print(str((toc-tic)/60)+'minutes to normalize')
337/99: model.cosine_similarities(model['food'],model['usa'].reshape(1,-1))
337/100:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        tic=time.time()
        self.text = self.text.lower()
        toc=time.time()
        print('lower'+str(toc-tic))
        return self
    def strip_html_tags(self):
        tic=time.time()
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        toc=time.time()
        print('str_html'+str(toc-tic))
        return self
    
    def remove_accented_chars(self):
        tic=time.time()
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        toc=time.time()
        print('remove accented'+str(toc-tic))
        return self
    
    def expand_contractions(self):
        tic=time.time()
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            tic=time.time()
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        toc=time.time()
        print('expand_contractions'+str(toc-tic))
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        tic=time.time()
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        toc=time.time()
        print('remove special'+str(toc-tic))
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        tic=time.time()
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        toc=time.time()
        print('lemmatize'+str(toc-tic))
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tic=time.time()
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens) 
        toc=time.time()
        print('remove_stop'+str(toc-tic))
        return self
#     def remove_stopwords(self, extra_stopwords=None):
#         tic=time.time()
#         if extra_stopwords is None:
#             extra_stopwords = []
#         text = nltk.word_tokenize(self.text)
#         stop_words = set(stopwords.words('english'))
#         new_sentence = []
#         for w in text:
#             if w not in stop_words and w not in extra_stopwords:
#                 new_sentence.append(w)
#         self.text = ' '.join(new_sentence)
#         return self
    def remove_hyphens(self):
        tic=time.time()
        self.text=self.text.replace("-"," ")
        return self
    def remove_urls(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        toc=time.time()
        print('remove_urls'+str(toc-tic))
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        toc=time.time()
        print('remove_mentions'+str(toc-tic))
        return self

    def remove_hashtags(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        return self
    def remove_twitter_reserved_words(self):
        tic=time.time()
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        toc=time.time()
        print('remove_twitter_reserved'+str(toc-tic))
        return self

    def remove_single_letter_words(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        toc=time.time()
        print('remove_single_letter'+str(toc-tic))
        return self
    def remove_numbers(self, preserve_years=False):
        tic=time.time()
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        toc=time.time()
        print('remove_numbers'+str(toc-tic))
        return self
    def remove_blank_spaces(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        toc=time.time()
        print('remove_blank'+str(toc-tic))
        return self
337/101:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        tic=time.time()
        self.text = self.text.lower()
        toc=time.time()
        print('lower'+str(toc-tic))
        return self
    def strip_html_tags(self):
        tic=time.time()
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        toc=time.time()
        print('str_html'+str(toc-tic))
        return self
    
    def remove_accented_chars(self):
        tic=time.time()
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        toc=time.time()
        print('remove accented'+str(toc-tic))
        return self
    
    def expand_contractions(self):
        tic=time.time()
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            tic=time.time()
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        toc=time.time()
        print('expand_contractions'+str(toc-tic))
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        tic=time.time()
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        toc=time.time()
        print('remove special'+str(toc-tic))
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        tic=time.time()
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        toc=time.time()
        print('lemmatize'+str(toc-tic))
        return self
# # Removing Stopwords
    def remove_stopwords(self):
        tic=time.time()
        tokens = tokenizer.tokenize(self.text)
        tokens = [token.strip() for token in tokens]
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        self.text = ' '.join(filtered_tokens) 
        toc=time.time()
        print('remove_stop'+str(toc-tic))
        return self
#     def remove_stopwords(self, extra_stopwords=None):
#         tic=time.time()
#         if extra_stopwords is None:
#             extra_stopwords = []
#         text = nltk.word_tokenize(self.text)
#         stop_words = set(stopwords.words('english'))
#         new_sentence = []
#         for w in text:
#             if w not in stop_words and w not in extra_stopwords:
#                 new_sentence.append(w)
#         self.text = ' '.join(new_sentence)
#         return self
    def remove_hyphens(self):
        tic=time.time()
        self.text=self.text.replace("-"," ")
        toc=time.time()
        print('remove_hyphens'+str(toc-tic))
        return self
    def remove_urls(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        toc=time.time()
        print('remove_urls'+str(toc-tic))
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        toc=time.time()
        print('remove_mentions'+str(toc-tic))
        return self

    def remove_hashtags(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        toc=time.time()
        print('remove_hashtags'+str(toc-tic))
        return self
    def remove_twitter_reserved_words(self):
        tic=time.time()
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        toc=time.time()
        print('remove_twitter_reserved'+str(toc-tic))
        return self

    def remove_single_letter_words(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        toc=time.time()
        print('remove_single_letter'+str(toc-tic))
        return self
    def remove_numbers(self, preserve_years=False):
        tic=time.time()
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        toc=time.time()
        print('remove_numbers'+str(toc-tic))
        return self
    def remove_blank_spaces(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        toc=time.time()
        print('remove_blank'+str(toc-tic))
        return self
337/102:
tweets = latest_tweets.copy()
tweets= tweets[0:1]
tweets
337/103:
tweets.tweet= tweets.tweet.astype(str)
print('preprocessing starting...')
tic=time.time()
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))
toc=time.time()
print(str((toc-tic)/60)+'minutes to normalize')
337/104:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

class TwitterPreprocessor:
    def __init__(self, text: str):
        self.text = text
    
    tokenizer = ToktokTokenizer()
    stopword_list = nltk.corpus.stopwords.words('english')
    stopword_list.remove('no')
    stopword_list.remove('not')  
    
    def fully_preprocess(self):
        return self \
            .remove_hyphens() \
            .remove_urls() \
            .remove_mentions() \
            .remove_hashtags() \
            .remove_twitter_reserved_words() \
            .remove_single_letter_words() \
            .remove_stopwords() \
            .remove_numbers() \
            .strip_html_tags() \
            .remove_accented_chars() \
            .expand_contractions() \
            .lemmatize_text() \
            .remove_special_characters() \
            .remove_stopwords() \
            .remove_blank_spaces() \
            .Lower()
    
    def Lower(self):
        tic=time.time()
        self.text = self.text.lower()
        toc=time.time()
        print('lower'+str(toc-tic))
        return self
    def strip_html_tags(self):
        tic=time.time()
        soup = BeautifulSoup(self.text, "html.parser")
        self.text = soup.get_text()
        toc=time.time()
        print('str_html'+str(toc-tic))
        return self
    
    def remove_accented_chars(self):
        tic=time.time()
        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        toc=time.time()
        print('remove accented'+str(toc-tic))
        return self
    
    def expand_contractions(self):
        tic=time.time()
        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
            tic=time.time()
            match = contraction.group(0)
            first_char = match[0]
            expanded_contraction = CONTRACTION_MAP.get(match) \
                                   if CONTRACTION_MAP.get(match) \
                                    else CONTRACTION_MAP.get(match.lower())                       
            expanded_contraction = first_char+expanded_contraction[1:]
            return expanded_contraction
        
        expanded_text = contractions_pattern.sub(expand_match, self.text)
        self.text = re.sub("'", "", expanded_text)
        toc=time.time()
        print('expand_contractions'+str(toc-tic))
        return self
    
# # Removing Special Characters
    def remove_special_characters(self):
        tic=time.time()
        self.text = re.sub('[^a-zA-Z0-9\s]', ' ', self.text)
        toc=time.time()
        print('remove special'+str(toc-tic))
        return self


# # Lemmatizing text
    def lemmatize_text(self):
        tic=time.time()
        nlp = spacy.load('en', parse = False, tag=False, entity=False)
        text = nlp(self.text)
        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
        toc=time.time()
        print('lemmatize'+str(toc-tic))
        return self
# # Removing Stopwords
#     def remove_stopwords(self):
#         tic=time.time()
#         tokens = tokenizer.tokenize(self.text)
#         tokens = [token.strip() for token in tokens]
#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
#         self.text = ' '.join(filtered_tokens) 
#         toc=time.time()
#         print('remove_stop'+str(toc-tic))
#         return self
    def remove_stopwords(self, extra_stopwords=None):
        tic=time.time()
        if extra_stopwords is None:
            extra_stopwords = []
        text = nltk.word_tokenize(self.text)
        stop_words = set(stopwords.words('english'))
        new_sentence = []
        for w in text:
            if w not in stop_words and w not in extra_stopwords:
                new_sentence.append(w)
        self.text = ' '.join(new_sentence)
        toc=time.time()
        print('remove_stop'+str(toc-tic))
        return self
    def remove_hyphens(self):
        tic=time.time()
        self.text=self.text.replace("-"," ")
        toc=time.time()
        print('remove_hyphens'+str(toc-tic))
        return self
    def remove_urls(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(
        r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
        r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=self.text)
        toc=time.time()
        print('remove_urls'+str(toc-tic))
        return self

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self
 
    def remove_mentions(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=self.text)
        toc=time.time()
        print('remove_mentions'+str(toc-tic))
        return self

    def remove_hashtags(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)
        toc=time.time()
        print('remove_hashtags'+str(toc-tic))
        return self
    def remove_twitter_reserved_words(self):
        tic=time.time()
        self.text = re.sub(r'\brt\b','',self.text,flags=re.IGNORECASE) #removing rt
        self.text = re.sub(r'\bvia\b','',self.text,flags=re.IGNORECASE) #removing via
        toc=time.time()
        print('remove_twitter_reserved'+str(toc-tic))
        return self

    def remove_single_letter_words(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=self.text)
        toc=time.time()
        print('remove_single_letter'+str(toc-tic))
        return self
    def remove_numbers(self, preserve_years=False):
        tic=time.time()
        text_list = self.text.split(' ')
        for text in text_list:
            if text.isnumeric():
                if preserve_years:
                    if utils.is_year(text):
                        text_list.remove(text)
                else:
                    text_list.remove(text)

        self.text = ' '.join(text_list)
        toc=time.time()
        print('remove_numbers'+str(toc-tic))
        return self
    def remove_blank_spaces(self):
        tic=time.time()
        self.text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=self.text)
        toc=time.time()
        print('remove_blank'+str(toc-tic))
        return self
337/105:
tweets.tweet= tweets.tweet.astype(str)
print('preprocessing starting...')
tic=time.time()
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))
toc=time.time()
print(str((toc-tic)/60)+'minutes to normalize')
337/106: model.cosine_similarities(model['canada'],model['usa'].reshape(1,-1))
337/107: model.cosine_similarities(model['tree'],model['usa'].reshape(1,-1))
337/108: model.cosine_similarities(model['tree'],model['leave'].reshape(1,-1))
337/109: model.cosine_similarities(model['tree'],model['leaf'].reshape(1,-1))
337/110: model.cosine_similarities(model['tree'],model['apple'].reshape(1,-1))
337/111: model.cosine_similarities(model['tree'],model['branch'].reshape(1,-1))
337/112: model.cosine_similarities(model['tree'],model['laptop'].reshape(1,-1))
337/113: model.cosine_similarities(model['dektop'],model['laptop'].reshape(1,-1))
337/114: model.cosine_similarities(model['desktop'],model['laptop'].reshape(1,-1))
337/115: model.cosine_similarities(model['desktop'],model['USA'].reshape(1,-1))
337/116: model.cosine_similarities(model['desktop'],model['usa'].reshape(1,-1))
337/117: model.cosine_similarities(model['usa'],model['USA'].reshape(1,-1))
337/118: model.cosine_similarities(model['usage'],model['USA'].reshape(1,-1))
337/119: model.cosine_similarities(model['usa'],model['USA'].reshape(1,-1))
337/120: model.cosine_similarities(model['IRAN'],model['Iran'].reshape(1,-1))
337/121:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

 
    
def fully_preprocess(text):
     return  \
     remove_hyphens( \
     remove_urls( \
     remove_mentions( \
     remove_hashtags( \
     remove_twitter_reserved_words( \
     remove_single_letter_words( \
     remove_stopwords( \
     remove_numbers(  \
     strip_html_tags(  \
     remove_accented_chars( \
     expand_contractions(  \
     lemmatize_text(  \
     remove_special_characters(  \
     remove_stopwords(  \
     remove_blank_spaces( \
     Lower(text))))))))))))))))
    
def Lower(text):
    tic=time.time()
    text = text.lower()
    toc=time.time()
    print('lower'+str(toc-tic))
    return text
def strip_html_tags(text):
    tic=time.time()
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text()
    toc=time.time()
    print('str_html'+str(toc-tic))
    return text

def remove_accented_chars(text):
    tic=time.time()
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    toc=time.time()
    print('remove accented'+str(toc-tic))
    return text

def expand_contractions(text):
    tic=time.time()
    contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                  flags=re.IGNORECASE|re.DOTALL)
    def expand_match(contraction):
        tic=time.time()
        match = contraction.group(0)
        first_char = match[0]
        expanded_contraction = CONTRACTION_MAP.get(match) \
                               if CONTRACTION_MAP.get(match) \
                                else CONTRACTION_MAP.get(match.lower())                       
        expanded_contraction = first_char+expanded_contraction[1:]
        return expanded_contraction

    expanded_text = contractions_pattern.sub(expand_match, text)
    text = re.sub("'", "", expanded_text)
    toc=time.time()
    print('expand_contractions'+str(toc-tic))
    return text

# # Removing Special Characters
def remove_special_characters(text):
    tic=time.time()
    text = re.sub('[^a-zA-Z0-9\s]', ' ', text)
    toc=time.time()
    print('remove special'+str(toc-tic))
    return text


# # Lemmatizing text
def lemmatize_text(text):
    tic=time.time()
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    text = nlp(text)
    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
    toc=time.time()
    print('lemmatize'+str(toc-tic))
    return text
# # Removing Stopwords
#     def remove_stopwords(self):
#         tic=time.time()
#         tokens = tokenizer.tokenize(self.text)
#         tokens = [token.strip() for token in tokens]
#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
#         self.text = ' '.join(filtered_tokens) 
#         toc=time.time()
#         print('remove_stop'+str(toc-tic))
#         return self
def remove_stopwords(text, extra_stopwords=None):
    tic=time.time()
    if extra_stopwords is None:
        extra_stopwords = []
    text = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    new_sentence = []
    for w in text:
        if w not in stop_words and w not in extra_stopwords:
            new_sentence.append(w)
    text = ' '.join(new_sentence)
    toc=time.time()
    print('remove_stop'+str(toc-tic))
    return text
def remove_hyphens(text):
    tic=time.time()
    text=text.replace("-"," ")
    toc=time.time()
    print('remove_hyphens'+str(toc-tic))
    return text
def remove_urls(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(
    r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
    r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
    toc=time.time()
    print('remove_urls'+str(toc-tic))
    return text

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self

def remove_mentions(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
    toc=time.time()
    print('remove_mentions'+str(toc-tic))
    return text

def remove_hashtags(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'#*'),repl='',string=text)
    toc=time.time()
    print('remove_hashtags'+str(toc-tic))
    return text
def remove_twitter_reserved_words(text):
    tic=time.time()
    text = re.sub(r'\brt\b','',text,flags=re.IGNORECASE) #removing rt
    text = re.sub(r'\bvia\b','',text,flags=re.IGNORECASE) #removing via
    toc=time.time()
    print('remove_twitter_reserved'+str(toc-tic))
    return text

def remove_single_letter_words(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
    toc=time.time()
    print('remove_single_letter'+str(toc-tic))
    return text
def remove_numbers(text, preserve_years=False):
    tic=time.time()
    text_list = text.split(' ')
    for text in text_list:
        if text.isnumeric():
            if preserve_years:
                if utils.is_year(text):
                    text_list.remove(text)
            else:
                text_list.remove(text)

    text = ' '.join(text_list)
    toc=time.time()
    print('remove_numbers'+str(toc-tic))
    return text
def remove_blank_spaces(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
    toc=time.time()
    print('remove_blank'+str(toc-tic))
    return text
337/122: fully_process(' The same people who were claiming 80% voted')
337/123: fully_preprocess(' The same people who were claiming 80% voted')
337/124:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

 
    
def fully_preprocess(text):
     tic = time.time()
     return  \
     remove_hyphens( \
     remove_urls( \
     remove_mentions( \
     remove_hashtags( \
     remove_twitter_reserved_words( \
     remove_single_letter_words( \
     remove_stopwords( \
     remove_numbers(  \
     strip_html_tags(  \
     remove_accented_chars( \
     expand_contractions(  \
     lemmatize_text(  \
     remove_special_characters(  \
     remove_stopwords(  \
     remove_blank_spaces( \
     Lower(text)))))))))))))))), time.times()-tic
    
def Lower(text):
    tic=time.time()
    text = text.lower()
    toc=time.time()
    print('lower'+str(toc-tic))
    return text
def strip_html_tags(text):
    tic=time.time()
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text()
    toc=time.time()
    print('str_html'+str(toc-tic))
    return text

def remove_accented_chars(text):
    tic=time.time()
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    toc=time.time()
    print('remove accented'+str(toc-tic))
    return text

def expand_contractions(text):
    tic=time.time()
    contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                  flags=re.IGNORECASE|re.DOTALL)
    def expand_match(contraction):
        tic=time.time()
        match = contraction.group(0)
        first_char = match[0]
        expanded_contraction = CONTRACTION_MAP.get(match) \
                               if CONTRACTION_MAP.get(match) \
                                else CONTRACTION_MAP.get(match.lower())                       
        expanded_contraction = first_char+expanded_contraction[1:]
        return expanded_contraction

    expanded_text = contractions_pattern.sub(expand_match, text)
    text = re.sub("'", "", expanded_text)
    toc=time.time()
    print('expand_contractions'+str(toc-tic))
    return text

# # Removing Special Characters
def remove_special_characters(text):
    tic=time.time()
    text = re.sub('[^a-zA-Z0-9\s]', ' ', text)
    toc=time.time()
    print('remove special'+str(toc-tic))
    return text


# # Lemmatizing text
def lemmatize_text(text):
    tic=time.time()
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    text = nlp(text)
    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
    toc=time.time()
    print('lemmatize'+str(toc-tic))
    return text
# # Removing Stopwords
#     def remove_stopwords(self):
#         tic=time.time()
#         tokens = tokenizer.tokenize(self.text)
#         tokens = [token.strip() for token in tokens]
#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
#         self.text = ' '.join(filtered_tokens) 
#         toc=time.time()
#         print('remove_stop'+str(toc-tic))
#         return self
def remove_stopwords(text, extra_stopwords=None):
    tic=time.time()
    if extra_stopwords is None:
        extra_stopwords = []
    text = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    new_sentence = []
    for w in text:
        if w not in stop_words and w not in extra_stopwords:
            new_sentence.append(w)
    text = ' '.join(new_sentence)
    toc=time.time()
    print('remove_stop'+str(toc-tic))
    return text
def remove_hyphens(text):
    tic=time.time()
    text=text.replace("-"," ")
    toc=time.time()
    print('remove_hyphens'+str(toc-tic))
    return text
def remove_urls(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(
    r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
    r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
    toc=time.time()
    print('remove_urls'+str(toc-tic))
    return text

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self

def remove_mentions(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
    toc=time.time()
    print('remove_mentions'+str(toc-tic))
    return text

def remove_hashtags(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'#*'),repl='',string=text)
    toc=time.time()
    print('remove_hashtags'+str(toc-tic))
    return text
def remove_twitter_reserved_words(text):
    tic=time.time()
    text = re.sub(r'\brt\b','',text,flags=re.IGNORECASE) #removing rt
    text = re.sub(r'\bvia\b','',text,flags=re.IGNORECASE) #removing via
    toc=time.time()
    print('remove_twitter_reserved'+str(toc-tic))
    return text

def remove_single_letter_words(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
    toc=time.time()
    print('remove_single_letter'+str(toc-tic))
    return text
def remove_numbers(text, preserve_years=False):
    tic=time.time()
    text_list = text.split(' ')
    for text in text_list:
        if text.isnumeric():
            if preserve_years:
                if utils.is_year(text):
                    text_list.remove(text)
            else:
                text_list.remove(text)

    text = ' '.join(text_list)
    toc=time.time()
    print('remove_numbers'+str(toc-tic))
    return text
def remove_blank_spaces(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
    toc=time.time()
    print('remove_blank'+str(toc-tic))
    return text
337/125: fully_preprocess(' The same people who were claiming 80% voted')
337/126:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

 
    
def fully_preprocess(text):
     tic = time.time()
     return  \
     remove_hyphens( \
     remove_urls( \
     remove_mentions( \
     remove_hashtags( \
     remove_twitter_reserved_words( \
     remove_single_letter_words( \
     remove_stopwords( \
     remove_numbers(  \
     strip_html_tags(  \
     remove_accented_chars( \
     expand_contractions(  \
     lemmatize_text(  \
     remove_special_characters(  \
     remove_stopwords(  \
     remove_blank_spaces( \
     Lower(text)))))))))))))))), time.time()-tic
    
def Lower(text):
    tic=time.time()
    text = text.lower()
    toc=time.time()
    print('lower'+str(toc-tic))
    return text
def strip_html_tags(text):
    tic=time.time()
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text()
    toc=time.time()
    print('str_html'+str(toc-tic))
    return text

def remove_accented_chars(text):
    tic=time.time()
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    toc=time.time()
    print('remove accented'+str(toc-tic))
    return text

def expand_contractions(text):
    tic=time.time()
    contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                  flags=re.IGNORECASE|re.DOTALL)
    def expand_match(contraction):
        tic=time.time()
        match = contraction.group(0)
        first_char = match[0]
        expanded_contraction = CONTRACTION_MAP.get(match) \
                               if CONTRACTION_MAP.get(match) \
                                else CONTRACTION_MAP.get(match.lower())                       
        expanded_contraction = first_char+expanded_contraction[1:]
        return expanded_contraction

    expanded_text = contractions_pattern.sub(expand_match, text)
    text = re.sub("'", "", expanded_text)
    toc=time.time()
    print('expand_contractions'+str(toc-tic))
    return text

# # Removing Special Characters
def remove_special_characters(text):
    tic=time.time()
    text = re.sub('[^a-zA-Z0-9\s]', ' ', text)
    toc=time.time()
    print('remove special'+str(toc-tic))
    return text


# # Lemmatizing text
def lemmatize_text(text):
    tic=time.time()
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    text = nlp(text)
    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
    toc=time.time()
    print('lemmatize'+str(toc-tic))
    return text
# # Removing Stopwords
#     def remove_stopwords(self):
#         tic=time.time()
#         tokens = tokenizer.tokenize(self.text)
#         tokens = [token.strip() for token in tokens]
#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
#         self.text = ' '.join(filtered_tokens) 
#         toc=time.time()
#         print('remove_stop'+str(toc-tic))
#         return self
def remove_stopwords(text, extra_stopwords=None):
    tic=time.time()
    if extra_stopwords is None:
        extra_stopwords = []
    text = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    new_sentence = []
    for w in text:
        if w not in stop_words and w not in extra_stopwords:
            new_sentence.append(w)
    text = ' '.join(new_sentence)
    toc=time.time()
    print('remove_stop'+str(toc-tic))
    return text
def remove_hyphens(text):
    tic=time.time()
    text=text.replace("-"," ")
    toc=time.time()
    print('remove_hyphens'+str(toc-tic))
    return text
def remove_urls(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(
    r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
    r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
    toc=time.time()
    print('remove_urls'+str(toc-tic))
    return text

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self

def remove_mentions(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
    toc=time.time()
    print('remove_mentions'+str(toc-tic))
    return text

def remove_hashtags(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'#*'),repl='',string=text)
    toc=time.time()
    print('remove_hashtags'+str(toc-tic))
    return text
def remove_twitter_reserved_words(text):
    tic=time.time()
    text = re.sub(r'\brt\b','',text,flags=re.IGNORECASE) #removing rt
    text = re.sub(r'\bvia\b','',text,flags=re.IGNORECASE) #removing via
    toc=time.time()
    print('remove_twitter_reserved'+str(toc-tic))
    return text

def remove_single_letter_words(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
    toc=time.time()
    print('remove_single_letter'+str(toc-tic))
    return text
def remove_numbers(text, preserve_years=False):
    tic=time.time()
    text_list = text.split(' ')
    for text in text_list:
        if text.isnumeric():
            if preserve_years:
                if utils.is_year(text):
                    text_list.remove(text)
            else:
                text_list.remove(text)

    text = ' '.join(text_list)
    toc=time.time()
    print('remove_numbers'+str(toc-tic))
    return text
def remove_blank_spaces(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
    toc=time.time()
    print('remove_blank'+str(toc-tic))
    return text
337/127: fully_preprocess(' The same people who were claiming 80% voted')
337/128: fully_preprocess(' The same people who were claiming 80% voted')
337/129:
def tokenize_title(title):
    tokens = [word for word in title.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [[word for word in title if word not in stoplist] for title in clean_words];
    filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
    return(filtered_word_list);
337/130:
def tokenize_tweet(tweet):
    tokens = [word for word in tweet.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_tweet(clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [[word for word in title if word not in stoplist] for title in clean_words];
    filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
    return(filtered_word_list);
337/131:
text='49  Labour bigwigs say they now need to commit to a second referendum. Yet in places such as Sunderland it lost 20 per cent of its vote share while the Brexit Party took an extraordinary 40 per cent of the vote. Apparently Labours traditional heartlands dont matter to them anymore!   
'
337/132: text='49  Labour bigwigs say they now need to commit to a second referendum. Yet in places such as Sunderland it lost 20 per cent of its vote share while the Brexit Party took an extraordinary 40 per cent of the vote. Apparently Labours traditional heartlands dont matter to them anymore!  '
337/133:
text='49  Labour bigwigs say they now need to commit to a second referendum. Yet in places such as Sunderland it lost 20 per cent of its vote share while the Brexit Party took an extraordinary 40 per cent of the vote. Apparently Labours traditional heartlands dont matter to them anymore!  '
tokens = [tokenize_tweet(word) for word in tweets];
#clean_words
clean_words = remove_punctuation(tokens);
#clean_text
clean_text = clean_text(clean_words);
339/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
339/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
339/3:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
339/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
339/5:
def get_titles(raw):
    titles = raw['title'];
    post_titles = [title for title in titles];
    post_titles = set(post_titles);
    return(post_titles);

def tokenize_title(title):
    tokens = [word for word in title.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [[word for word in title if word not in stoplist] for title in clean_words];
    filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
    return(filtered_word_list);
339/6:
def tweet_preprocess(row_tweet):
    return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]
339/7:
def vectorize_tweet(normalized_tweet):
    vec=np.zeros((300))
    for word in (normalized_tweet and model.vocab):
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
339/8:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tic=time.time()
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to normalize')
    tic=time.time()
    print('preprocessing finished!')
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to vectorize')
    print('tweets vectorizd!')
    return tweets
339/9:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input)
    word_vec = vectorize_tweet(normalized)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
339/10:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)
    #decimals = pd.Series([8], index=['similarity_score'])
    #vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
339/11:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
339/12:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
339/13:
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
339/14:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/15:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/16: latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
339/17:
track_list=[k for k in topic.split(',')]
latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
339/18: tweets=latest_tweets[0:1]
339/19: tweets.loc[:,'titles']=tweets.tweet.apply(lambda tweet: get_titles(tweets))
339/20:
def get_titles(raw):
    post_titles = [title for title in raw];
    post_titles = set(post_titles);
    return(post_titles);

def tokenize_title(title):
    tokens = [word for word in title.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [[word for word in title if word not in stoplist] for title in clean_words];
    filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
    return(filtered_word_list);
339/21: tweets.loc[:,'titles']=tweets.tweet.apply(lambda tweet: get_titles(tweet))
339/22: tweets
339/23: tweets=latest_tweets[0:1]
339/24: tweets
339/25: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:tokenize_title(tweet))
339/26: tweets
339/27: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:[tokenize_title(tweet)])
339/28: tweets
339/29: tweets=latest_tweets[0:1]
339/30: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:[tokenize_title(tweet)])
339/31: tweets
339/32: tweets.loc[:,'remove_pun']=tweets.loc[:,'tokenize'].apply(lambda tokens: remove_punctuation(tokens))
339/33:
for  word in tweets.tokenize[0]:
     print(word)
339/34:
for  word in tweets.tokenize[1]:
     print(word)
339/35:
for  word in tweets.loc[0,0]:
     print(word)
339/36:
for  word in tweets.iloc[0]['tokenize']:
     print(word)
339/37:
for  word in tweets.iloc[0]['tokenize']:
     print(type(word)
339/38:
for  word in tweets.iloc[0]['tokenize']:
     print(type(word))
339/39:
for  word in tweets.iloc[0]['tokenize']:
     print(type(str(word)))
339/40:
for  word in tweets.iloc[0]['tokenize']:
     print(((word)))
339/41:
def get_titles(raw):
    post_titles = [title for title in raw];
    post_titles = set(post_titles);
    return(post_titles);

def tokenize_title(title):
    tokens = [str(word) for word in title.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [[word for word in title if word not in stoplist] for title in clean_words];
    filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
    return(filtered_word_list);
339/42: tweets=latest_tweets[0:1]
339/43: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:[tokenize_title(tweet)])
339/44: tweets
339/45: tweets.loc[:,'remove_pun']=tweets.loc[:,'tokenize'].apply(lambda tokens: remove_punctuation(tokens))
339/46:
for  word in tweets.iloc[0]['tokenize']:
     print((type(word)))
339/47:
def get_titles(raw):
    post_titles = [title for title in raw];
    post_titles = set(post_titles);
    return(post_titles);

def tokenize_title(title):
    tokens = [str(word) for word in title.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [[word for word in title if word not in stoplist] for title in clean_words];
    filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
    return(filtered_word_list);
339/48: tweets=latest_tweets[0:1]
339/49: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:[tokenize_title(tweet)])
339/50: tweets
339/51:
for  word in tweets.iloc[0]['tokenize']:
     print((type(word)))
339/52: tweets=latest_tweets[0:1]
339/53: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:tokenize_title(tweet))
339/54: tweets
339/55:
for  word in tweets.iloc[0]['tokenize']:
     print((type(word)))
339/56: tweets.loc[:,'remove_pun']=tweets.loc[:,'tokenize'].apply(lambda tokens: remove_punctuation(tokens))
339/57: tweets
339/58: tweets.iloc[0]['remove_pun']
339/59: tweets.iloc[0]['tweet']
339/60: tweets.iloc[0]['remove_pun']
339/61: tweets=latest_tweets[0:1]
339/62: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/63: tweets
339/64: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:clean_text(remove_punctuation(tokenize_title(tweet)))
339/65: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:clean_text(remove_punctuation(tokenize_title(tweet))))
339/66: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/67: tweets.loc[:,'clean']=tweets.loc[:,'tokenize'].apply(lambda tweet:clean_text(tweet))
339/68: tweets.loc[:,'clean']=tweets.loc[:,'tokenize'].apply(lambda tweet:clean_text(model,tweet))
339/69: tweets
339/70:
for  word in tweets.iloc[0]['clean']:
     print((type(word)))
339/71:
for  word in tweets.iloc[0]['clean']:
     print((word))
339/72:
for  word in tweets.iloc[0]['tokenize']:
     print((word))
339/73:
def get_titles(raw):
    post_titles = [title for title in raw];
    post_titles = set(post_titles);
    return(post_titles);

def tokenize_title(title):
    tokens = [str(word) for word in title.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [word in clean_words if word not in stoplist];
    filtered_word_list = [for word in titles_nostopwords if word in model.vocab];
    return(filtered_word_list);
339/74:
def get_titles(raw):
    post_titles = [title for title in raw];
    post_titles = set(post_titles);
    return(post_titles);

def tokenize_title(title):
    tokens = [str(word) for word in title.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in titles_nostopwords if word in model.vocab];
    return(filtered_word_list);
339/75:
def get_titles(raw):
    post_titles = [title for title in raw];
    post_titles = set(post_titles);
    return(post_titles);

def tokenize_title(title):
    tokens = [str(word) for word in title.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in titles_nostopwords if word in model.vocab];
    return(filtered_word_list);
339/76: tweets=latest_tweets[0:1]
339/77: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/78: tweets.loc[:,'clean']=tweets.loc[:,'tokenize'].apply(lambda tweet:clean_text(model,tweet))
339/79: tweets
339/80: tweets.iloc[0][tweet]
339/81: tweets.iloc[0]['tweet']
339/82: tweets.iloc[0]['clean']
339/83: tweets=latest_tweets.copy()
339/84: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/85:
tweets=latest_tweets.copy()
tweets=weets.tweet.astype(str)
339/86:
tweets=latest_tweets.copy()
tweets=tweets.tweet.astype(str)
339/87: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/88:
tweets=latest_tweets.copy()
#tweets=tweets.tweet.astype(str)
339/89: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/90:
tweets=latest_tweets.copy()
tweets=tweets.tweet.astype(str)
339/91: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/92: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:tokenize_title(tweet))
339/93:
tweets=latest_tweets[0:3]
tweets=tweets.tweet.astype(str)
339/94: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:tokenize_title(tweet))
339/95: tweets.iloc[0]['tweet']
339/96:
tweets=latest_tweets[0:3]
tweets=tweets.tweet.astype(str)
339/97: tweets.iloc[0]['clean']
339/98: tweets.iloc[0]['tweet']
339/99: tweets
339/100:
def get_titles(raw):
    post_titles = [title for title in raw];
    post_titles = set(post_titles);
    return(post_titles);

def tokenize_title(title):
    tokens = [str(word) for word in str(title).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in titles_nostopwords if word in model.vocab];
    return(filtered_word_list);
339/101:
tweets=latest_tweets[0:3]
#tweets=tweets.tweet.astype(str)
339/102: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:tokenize_title(tweet))
339/103: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/104:
tweets=latest_tweets[0:3]
#tweets=tweets.tweet.astype(str)
339/105: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/106: tweets.loc[:,'clean']=tweets.loc[:,'tokenize'].apply(lambda tweet:clean_text(model,tweet))
339/107: tweets
339/108:
tweets=latest_tweets.copy()
#tweets=tweets.tweet.astype(str)
339/109: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/110: tweets.loc[:,'clean']=tweets.loc[:,'tokenize'].apply(lambda tweet:clean_text(model,tweet))
339/111: tweets
339/112:
def tokenize_tweet(title):
    tokens = [str(word) for word in str(tweet).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    tweet_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in tweet_nostopwords if word in model.vocab];
    return(filtered_word_list);
339/113:
def tweet_preprocess(row_tweet):
    return tokenize_tweet(remove_punctuation(clean_text(row_tweet)))
339/114:
tweets=latest_tweets.copy()
#tweets=tweets.tweet.astype(str)
339/115: tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet))
339/116:
def tweet_preprocess(row_tweet,model):
    return tokenize_tweet(remove_punctuation(clean_text(model,row_tweet)))
339/117: tweets.loc[:,'tokenize']=tweets.loc[:,'tweet'].apply(lambda tweet:remove_punctuation(tokenize_title(tweet)))
339/118:
tweets=latest_tweets.copy()
#tweets=tweets.tweet.astype(str)
339/119: tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
339/120:
def tokenize_tweet(tweet):
    tokens = [str(word) for word in str(tweet).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    tweet_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in tweet_nostopwords if word in model.vocab];
    return(filtered_word_list);
339/121:
tweets=latest_tweets.copy()
#tweets=tweets.tweet.astype(str)
339/122: tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
339/123:
def tweet_preprocess(row_tweet,model):
    return clean_text(remove_punctuation(tokenize_tweet(model,row_tweet)))
339/124:
tweets=latest_tweets.copy()
#tweets=tweets.tweet.astype(str)
339/125: tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
339/126:
def tweet_preprocess(row_tweet,model):
    return clean_text(model,remove_punctuation(tokenize_tweet(row_tweet)))
339/127:
tweets=latest_tweets.copy()
#tweets=tweets.tweet.astype(str)
339/128: tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
339/129: tweets
339/130:
def vectorize_tweet(normalized_tweet,model):
    vec=np.zeros((300))
    for word in normalized_tweet:
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
339/131:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to vectorize')
    print('tweets vectorizd!')
    return tweets
339/132:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to vectorize')
    print('tweets vectorizd!')
    return tweets
339/133:
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
339/134: recommendations = process_user_input(user_input, time_limit, topic)
339/135:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input,model)
    word_vec = vectorize_tweet(normalized)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
339/136: recommendations = process_user_input(user_input, time_limit, topic)
339/137: recommendations = process_user_input(user_input, time_limit, topic)
339/138:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input,model)
    word_vec = vectorize_tweet(normalized,model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
339/139: recommendations = process_user_input(user_input, time_limit, topic)
339/140: recommendations = process_user_input(user_input, time_limit, topic)
339/141:
tweets=latest_tweets.copy()
#tweets=tweets.tweet.astype(str)
339/142:
if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
339/144:
if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
else:
    tweets = tweets[0:1]
    vectorized_tweets = vectorize_latest_tweets(tweets, model)
    vectorized_user_input = vectorize_user_input(user_input, model)
    #find the top topn= 10  similar tweets
    recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
339/145:
if no_data:
        print('There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds')
else:
    tweets = tweets[0:1]
    vectorized_tweets = vectorize_latest_tweets(tweets, model)
    vectorized_user_input = vectorize_user_input(user_input, model)
    #find the top topn= 10  similar tweets
    recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
339/146:
if no_data:
        print('There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds')
else:
    vectorized_tweets = vectorize_latest_tweets(tweets, model)
    vectorized_user_input = vectorize_user_input(user_input, model)
    #find the top topn= 10  similar tweets
    recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
339/147:
tweets=latest_tweets.copy()
#tweets=tweets.tweet.astype(str)
339/148:
if no_data:
        print('There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds')
else:
    vectorized_tweets = vectorize_latest_tweets(tweets, model)
    vectorized_user_input = vectorize_user_input(user_input, model)
    #find the top topn= 10  similar tweets
    recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
339/149: recommendations
339/150: recommendations.iloc[0]['tweet']
339/151: recommendations
339/152:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
339/153:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    #twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
339/154:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
339/155: recommendations = process_user_input(user_input, time_limit, topic)
339/156: recommendations
339/157:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/158: recommendations = process_user_input(user_input, time_limit, topic)
339/159: recommendations
339/160:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
339/161: recommendations = process_user_input(user_input, time_limit, topic)
339/162: recommendations
339/163: recommendations
339/164: recommendations.shape
339/165:
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        #tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
339/166: recommendations = process_user_input(user_input, time_limit, topic)
339/167: recommendations.shape
339/168: recommendations
339/169:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    #twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
339/170: recommendations = process_user_input(user_input, time_limit, topic)
339/171:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained!")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
339/172:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'the' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/173: recommendations = process_user_input(user_input, time_limit, topic)
339/174: recommendations
339/175: recommendations
339/176:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/177: recommendations = process_user_input(user_input, time_limit, topic)
339/178: recommendations
339/179: recommendations.iloc[0]['tweet']
339/180:
with pd.option_context('display.max_colwidth', 300):
    print (recommendations)
339/181:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics canada' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/182:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics canada' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/183: recommendations = process_user_input(user_input, time_limit, topic)
339/184: recommendations.iloc[0]['tweet']
339/185:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics usa United States' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/186: recommendations = process_user_input(user_input, time_limit, topic)
339/187:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/188: recommendations = process_user_input(user_input, time_limit, topic)
339/189: recommendations.iloc[0]['tweet']
339/190: recommendations.iloc[0]['tweet']
339/191: recommendations
339/192: recommendations.iloc[1]['tweet']
339/193:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/194: recommendations = process_user_input(user_input, time_limit, topic)
339/195: recommendations.iloc[1]['tweet']
339/196:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Johnson & Johnson, one of the world s largest drug manufacturers, has gone on trial in a multi-billion dollar lawsuit by the US state of Oklahoma.'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
339/197: recommendations = process_user_input(user_input, time_limit, topic)
339/198: recommendations.iloc[1]['tweet']
339/199: recommendations
339/200:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print(str((toc-tic)/60)+' minutes to vectorize')
    print('tweets vectorizd!')
    return tweets
339/201:
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
339/202:
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
print('tweets with '+str(topic)+' obtained')
data = pd.read_json(data_dir+file_name+".json",lines=True)
339/203:
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
print('tweets with '+str(topic)+' obtained')
data = pd.read_json(data_dir+"stream.json",lines=True)
339/204: twitter_stream
339/205:
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= 't') # list of querries to track
print('tweets with '+str(topic)+' obtained')
data = pd.read_json(data_dir+"stream.json",lines=True)
339/206: data
339/207: data.shape
339/208:
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
        print('full tweet text obtained', tweets)
339/209:
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
    print('full tweet text obtained', tweets)
339/210: tweets.shape
339/211: data
339/212: data.tweet
339/213: data.extended_tweet
339/214:
track_list=[k for k in topic.split(',')]
file_name = "stream"
if os.path.exists(data_dir+file_name+'.json'):
    os.remove(data_dir+file_name+'.json')
#-----------------------------------------------
# Load credentials from json file
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
if no_data:
    return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
else:
    #tweets = tweets[0:1]
    vectorized_tweets = vectorize_latest_tweets(tweets, model)
    vectorized_user_input = vectorize_user_input(user_input, model)
    #find the top topn= 10  similar tweets
    recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
339/215:

vectorized_tweets = vectorize_latest_tweets(tweets, model)
vectorized_user_input = vectorize_user_input(user_input, model)
#find the top topn= 10  similar tweets
recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
339/216: recommendations
340/1: import pandas as pd
340/2:
import pandas as pd
import numpy as np
340/3: df=np.array([1,2,3],[1,3,5],[1,9,0])
340/4: df=np.array([[1,2,3],[1,3,5],[1,9,0]])
340/5: df
340/6: df=np.array([[1,2,3],[1,3,5],[1,9,0]],dtype=np.int)
340/7: df
340/8: print(repr(df))
340/9: print(repr(df))
340/10: df.shape
340/11: ddf=np.array([[1,2],[3,5]])
340/12: ddf
340/13: ddf=np.array([[[1],[2],[3,5]])
340/14: ddf=np.array([[[1],[2],[[3],[5]])
340/15: ddf=np.array([[1,2],[3,5]])
340/16: df=np.array([[1,2,3],[1,3,5],[1,9.2,0]],dtype=np.int)
340/17: print(repr(df))
340/18: df.shape
340/19: df.shape[0]
340/20: ddf=np.array([[1,2,2],[3,5,9]])
340/21: ddf
340/22: df.shape[0]
340/23: df.shape[1]
340/24: df.shape[2]
340/25: df.shape[1]
340/26: df.shape[0]
340/27: df.shape[1]
340/28: ddf.shape[1]
340/29: ddf.shape[0]
340/30: a=np.array([1,2])
340/31: a.shape
340/32: dd[0]
340/33: ddf[0]
340/34: ddf[0,0]
340/35: a[0]
340/36: a[1]
340/37: print(a)
340/38:
a[0]=2
print(a)
340/39: a.dtype
340/40: type(a)
340/41:
arr = np.array([np.nan, 1, 2])
print(repr(arr))

arr = np.array([np.nan, 'abc'])
print(repr(arr))
340/42: ddf=np.array([[1,2,2],np.nan)
340/43: ddf=np.array([[1,2,2],[np.nan])
340/44:
arr = np.array([np.nan, 1, 2])
print(repr(arr))
340/45:
arr = np.array([np.inf, 5])
print(repr(arr))
340/46:
# Will result in an OverflowError
np.array([np.inf, 3], dtype=np.int32)
340/47: a=[nap.nan,2,3,4]
340/48: a=[nP.nan,2,3,4]
340/49: a=[np.nan,2,3,4]
340/50: a[0]=2
340/51: ddf=np.array([[1,2,2],[3,5,9]])
340/52: ddf.astype(np.float)
340/53: ddf=np.array([[1,2,2],[3,5,9.2]])
340/54: ddf.astype(np.float)
340/55: ddf
340/56: ddf.astype(np.int)
340/57:
arr = np.arange(5)
print(repr(arr))
340/58:
arr = np.arange(5)
print(repr(arr))

arr = np.arange(5.1)
print(repr(arr))

arr = np.arange(-1, 4)
print(repr(arr))

arr = np.arange(-1.5, 4, 2)
print(repr(arr))
340/59: np.arange(2,-2)
340/60: a=np.arange(2,-2)
340/61: a.shape
340/62:
arr = np.arange(-1.5, 4, 2)
print(repr(arr))
340/63:
arr = np.arange(-1.5, 4, -2)
print(repr(arr))
340/64:
arr = np.arange(-1.5, -10, -2)
print(repr(arr))
340/65:
arr = np.arange(-1.5, -10, -2.1)
print(repr(arr))
340/66: np.arane(2)
340/67: np.arange(2)
340/68:
arr = np.linspace(5, 11, num=4)
print(repr(arr))
340/69:
arr = np.linspace(5, 11, num=4, endpoint=False)
print(repr(arr))

arr = np.linspace(5, 11, num=4, dtype=np.int32)
print(repr(arr))
340/70:
arr = np.arange(8)

reshaped_arr = np.reshape(arr, (2, 4))
print(repr(reshaped_arr))
340/71: arr.reshape(2,4)
340/72: arr
340/73: reshaped_arr = np.reshape(arr, (-1, 2, 2))
340/74:  np.reshape(arr, (-1, 2, 2))
340/75:  np.reshape(arr, (-1, 2, 2)).shape
340/76:  np.reshape(arr, (-1, 2, 2))
340/77:
arr = np.arange(7)

reshaped_arr = np.reshape(arr, (2, 4))
print(repr(reshaped_arr))
340/78: arr = np.arange(7)
340/79:  np.reshape(arr, (-1, 2, 2))
340/80: arr = np.arange(8)
340/81:  np.reshape(arr, (-1, 2, 2))
340/82:
arr = np.arange(8)
arr = np.reshape(arr, (2, 4))
flattened = arr.flatten()
print(repr(arr))
340/83:
arr = np.arange(8)
arr = np.reshape(arr, (2, 4))
arr
340/84:
flattened = arr.flatten()
print(repr(arr))
340/85: flattened.shape
340/86:
flattened = arr.flatten()
print(repr(fltttened))
340/87:
flattened = arr.flatten()
print(repr(flattened))
340/88:
arr = np.arange(8)
arr = np.reshape(arr, (4, 2))
transposed = np.transpose(arr)
print(repr(arr))
print('arr shape: {}'.format(arr.shape))
print(repr(transposed))
print('transposed shape: {}'.format(transposed.shape))
340/89: flattened.transpose
340/90: a=flattened.transpose
340/91:
a=flattened.transpose
a
340/92:
a=arr.transpose
a
340/93:
arr = np.arange(8)
arr1=arr.reshape(2,4)
340/94: arr1
340/95: arr
340/96: np.transpose(arr1)
340/97: np.zeros(2)
340/98: np.zeros(2,2)
340/99: np.zeros(2).shape
340/100: np.ones(2,4)
340/101: np.zeros((2,4)).shape
340/102: np.zeros((2,4))
340/103: np.zeros((2,4,3))
340/104: np.ones((3,2,1))
340/105: np.ones((3,2,4))
340/106: a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])
340/107: a
340/108: a.shape
340/109: a=np.arange(12)
340/110: a
340/111: a.reshape(3,4)
340/112: a[:2,1:3]
340/113: b=a.reshape(3,4)
340/114: b[:2,1:3]
340/115:
b=a.reshape(3,4)
b
340/116: a[:,1]
340/117: b[:,1]
340/118: a=np.array([[1,3],[4,1]])
340/119: a
340/120: a>2
340/121: a[b]
340/122: b=a>2
340/123: a[b]
340/124: a[b].shape
340/125: x=np.array([[1,2],[5,6]])
340/126: y=np.array([[1,3,4],[7,3,2]])
340/127: x.dot(y)
340/128: x.transpose
340/129: pd.transpose(x)
340/130: np.transpose(x)
340/131: x.T
340/132: np.title(x,(2,1))
340/133: np.tile(x,(2,1))
340/134: np.tile(x,(2,2))
340/135: np.tile(x,(2,1))
340/136: x
340/137: np.tile(x,(2,3))
340/138: x+1
340/139: x+np.array[[0,1]]
340/140: x+np.array([[0,1]])
340/141:
# Compute outer product of vectors
v = np.array([1,2,3])  # v has shape (3,)
w = np.array([4,5])    # w has shape (2,)
# To compute an outer product, we first reshape v to be a column
# vector of shape (3, 1); we can then broadcast it against w to yield
# an output of shape (3, 2), which is the outer product of v and w:
# [[ 4  5]
#  [ 8 10]
#  [12 15]]
print(np.reshape(v, (3, 1)) * w)
340/142: v
340/143: w
340/144: (3, 1)) * w
340/145: (3, 1) * w
340/146: np.reshape(v, (3, 1))
341/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
341/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')
    print("tweets obtained and now filtering...")
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('tweets are filtered', tweets)
    return tweets, no_data
341/3:
#MyListener() saves the data into a .json file with name stream_query
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, query, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
341/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
341/5:
import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
from contractions import CONTRACTION_MAP
import unicodedata

 
    
def fully_preprocess(text):
     tic = time.time()
     return  \
     remove_hyphens( \
     remove_urls( \
     remove_mentions( \
     remove_hashtags( \
     remove_twitter_reserved_words( \
     remove_single_letter_words( \
     remove_stopwords( \
     remove_numbers(  \
     strip_html_tags(  \
     remove_accented_chars( \
     expand_contractions(  \
     lemmatize_text(  \
     remove_special_characters(  \
     remove_stopwords(  \
     remove_blank_spaces( \
     Lower(text)))))))))))))))), time.time()-tic
    
def Lower(text):
    tic=time.time()
    text = text.lower()
    toc=time.time()
    print('lower'+str(toc-tic))
    return text
def strip_html_tags(text):
    tic=time.time()
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text()
    toc=time.time()
    print('str_html'+str(toc-tic))
    return text

def remove_accented_chars(text):
    tic=time.time()
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    toc=time.time()
    print('remove accented'+str(toc-tic))
    return text

def expand_contractions(text):
    tic=time.time()
    contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), 
                                  flags=re.IGNORECASE|re.DOTALL)
    def expand_match(contraction):
        tic=time.time()
        match = contraction.group(0)
        first_char = match[0]
        expanded_contraction = CONTRACTION_MAP.get(match) \
                               if CONTRACTION_MAP.get(match) \
                                else CONTRACTION_MAP.get(match.lower())                       
        expanded_contraction = first_char+expanded_contraction[1:]
        return expanded_contraction

    expanded_text = contractions_pattern.sub(expand_match, text)
    text = re.sub("'", "", expanded_text)
    toc=time.time()
    print('expand_contractions'+str(toc-tic))
    return text

# # Removing Special Characters
def remove_special_characters(text):
    tic=time.time()
    text = re.sub('[^a-zA-Z0-9\s]', ' ', text)
    toc=time.time()
    print('remove special'+str(toc-tic))
    return text


# # Lemmatizing text
def lemmatize_text(text):
    tic=time.time()
    nlp = spacy.load('en', parse = False, tag=False, entity=False)
    text = nlp(text)
    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
    toc=time.time()
    print('lemmatize'+str(toc-tic))
    return text
# # Removing Stopwords
#     def remove_stopwords(self):
#         tic=time.time()
#         tokens = tokenizer.tokenize(self.text)
#         tokens = [token.strip() for token in tokens]
#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
#         self.text = ' '.join(filtered_tokens) 
#         toc=time.time()
#         print('remove_stop'+str(toc-tic))
#         return self
def remove_stopwords(text, extra_stopwords=None):
    tic=time.time()
    if extra_stopwords is None:
        extra_stopwords = []
    text = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    new_sentence = []
    for w in text:
        if w not in stop_words and w not in extra_stopwords:
            new_sentence.append(w)
    text = ' '.join(new_sentence)
    toc=time.time()
    print('remove_stop'+str(toc-tic))
    return text
def remove_hyphens(text):
    tic=time.time()
    text=text.replace("-"," ")
    toc=time.time()
    print('remove_hyphens'+str(toc-tic))
    return text
def remove_urls(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(
    r'(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))'
    r'[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})'), repl='', string=text)
    toc=time.time()
    print('remove_urls'+str(toc-tic))
    return text

#     def remove_punctuation(self):
#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))
#         return self

def remove_mentions(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'@\w*'), repl='', string=text)
    toc=time.time()
    print('remove_mentions'+str(toc-tic))
    return text

def remove_hashtags(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'#*'),repl='',string=text)
    toc=time.time()
    print('remove_hashtags'+str(toc-tic))
    return text
def remove_twitter_reserved_words(text):
    tic=time.time()
    text = re.sub(r'\brt\b','',text,flags=re.IGNORECASE) #removing rt
    text = re.sub(r'\bvia\b','',text,flags=re.IGNORECASE) #removing via
    toc=time.time()
    print('remove_twitter_reserved'+str(toc-tic))
    return text

def remove_single_letter_words(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'(?<![\w\-])\w(?![\w\-])'),repl=' ',string=text)
    toc=time.time()
    print('remove_single_letter'+str(toc-tic))
    return text
def remove_numbers(text, preserve_years=False):
    tic=time.time()
    text_list = text.split(' ')
    for text in text_list:
        if text.isnumeric():
            if preserve_years:
                if utils.is_year(text):
                    text_list.remove(text)
            else:
                text_list.remove(text)

    text = ' '.join(text_list)
    toc=time.time()
    print('remove_numbers'+str(toc-tic))
    return text
def remove_blank_spaces(text):
    tic=time.time()
    text = re.sub(pattern=re.compile(r'\s{2,}|\t'),repl='',string=text)
    toc=time.time()
    print('remove_blank'+str(toc-tic))
    return text
341/6:
def tokenize_tweet(tweet):
    tokens = [word for word in tweet.lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_tweet(clean_words):
    stoplist = set(stopwords.words('english'));
    titles_nostopwords = [[word for word in title if word not in stoplist] for title in clean_words];
    filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];
    return(filtered_word_list);
341/7:
def tweet_preprocess(row_tweet):
    return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]
341/8:
def vectorize_tweet(normalized_tweet):
    vec=np.zeros((300))
    tic=time.time()
    for word in (normalized_tweet and model.vocab):
        vec+=model[word]
    toc=time.time()
    print('time to vectorize one tweet!'+str((toc-tic)/60))
    return preprocessing.normalize(vec.reshape(1,-1))
341/9:
def vectorize_latest_tweets(tweets, model):
    tweets.tweet= tweets.tweet.astype(str)
    print('preprocessing starting...')
    tic=time.time()
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to normalize')
    tic=time.time()
    print('preprocessing finished!')
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet))
    toc=time.time()
    print(str((toc-tic)/60)+'minutes to vectorize')
    print('tweets vectorizd!')
    return tweets
341/10:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input)
    word_vec = vectorize_tweet(normalized)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
341/11:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)
    #decimals = pd.Series([8], index=['similarity_score'])
    #vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
341/12:
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        latest_tweets = latest_tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
341/13:
#downloaded pretrained model

model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
341/14:
# Load credentials from json file
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
341/15:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
341/16:
def tweet_preprocess(row_tweet):
    return clean_tweet(remove_punctuation(tokenize_tweet(row_tweet)))
341/17:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
topic = 'brexit' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'
recommendations = process_user_input(user_input, time_limit, topic)
342/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
342/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('full tweet text obtained', tweets)
    return tweets, no_data
342/3:
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
342/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
342/5:
def tokenize_tweet(tweet):
    tokens = [str(word) for word in str(tweet).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    tweet_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in tweet_nostopwords if word in model.vocab];
    return(filtered_word_list);
342/6:
def tweet_preprocess(row_tweet,model):
    return clean_text(model,remove_punctuation(tokenize_tweet(row_tweet)))
342/7:
# def tweet_preprocess(row_tweet):
#     return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]
342/8:
def vectorize_tweet(normalized_tweet,model):
    vec=np.zeros((300))
    for word in normalized_tweet:
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
342/9:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print(str((toc-tic)/60)+' minutes to vectorize')
    print('tweets vectorizd!')
    return tweets
342/10:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input,model)
    word_vec = vectorize_tweet(normalized,model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
342/11:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)
    #decimals = pd.Series([8], index=['similarity_score'])
    #vectorized_tweets.round(decimals)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
342/12:
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        #tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
342/13:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
342/14:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Johnson & Johnson, one of the world s largest drug manufacturers, has gone on trial in a multi-billion dollar lawsuit by the US state of Oklahoma.'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
342/15: recommendations = process_user_input(user_input, time_limit, topic)
342/16:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('full tweet text obtained', tweets)
    print(tweets.shape[0])
    return tweets, no_data
342/17: recommendations = process_user_input(user_input, time_limit, topic)
342/18:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    print('full tweet text obtained', tweets)
    print(str(tweets.shape[0])+'tweets obtained!')
    return tweets, no_data
342/19:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    toc = time.time()
    print()
    print('full tweet text obtained in '+str((toc-tic)/60)+' minutes', tweets)
    print(str(tweets.shape[0])+'tweets obtained!')
    return tweets, no_data
342/20:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes to vectorize')
    return tweets
342/21: recommendations = process_user_input(user_input, time_limit, topic)
342/22:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
342/23:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes')
    return tweets
342/24:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+'tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
342/25: recommendations = process_user_input(user_input, time_limit, topic)
342/26: recommendations.iloc[1]['tweet']
342/27: recommendations
342/28:
with pd.option_context('display.max_colwidth', 300):
    print (recommendations)
344/1:
import pandas as pd
import numpy as np
344/2: data=pd.read_csv('zoo.csv',delimiter=',')
344/3: data
344/4: data.columsn
344/5: data.columns
344/6: data=pd.read_csv('zoo.csv',delimiter=',',names=['a','b','c'])
344/7: data.columns
344/8: data=pd.read_csv('zoo.csv',sep=',',names=['a','b','c'])
344/9: data.columns
344/10: data.sample
344/11: data=pd.read_csv('zoo.csv',sep=',')
344/12: data.columns
344/13: data.sample
344/14: data.head()
344/15: data.sample(4)
344/16: type(data[['animal']])
344/17: data.animal
344/18: type(data.animal)
344/19: type(data['animal'])
344/20: data[data.animal=='zebra']
344/21: type(data[data.animal=='zebra'])
344/22: list('abcd')
344/23: list('12m3mdnfgn')
344/24: np.array([3] * 4, dtype='int32')
344/25: np.array([3])
344/26: np.array([3]),shape
344/27: np.array([3]).shape
344/28: np.full((4,),3)
344/29: pd.Series(1, index=list(range(4))
344/30: pd.Series(1, index=list(range(4)))
344/31: dic={'A':np.array([1,2,3],'B':'foo','C':np.Series(1,index=list(range(4))))}
344/32:
 df2 = pd.DataFrame({'A': 1.,
   ...:                     'B': pd.Timestamp('20130102'),
   ...:                     'C': pd.Series(1, index=list(range(4)), dtype='float32'),
   ...:                     'D': np.array([3] * 4, dtype='int32'),
   ...:                     'E': pd.Categorical(["test", "train", "test", "train"]),
   ...:                     'F': 'foo'})
344/33: dic={'A':np.array([1,2,3],C':np.Series(1,index=list(range(4))))}
344/34: dic={'A':np.array([1,2,3],C':np.Series(1,index=list(range(4)))}
344/35: dic={'A':np.array([1,2,3]),C':np.Series(1,index=list(range(4)))}
344/36: dic={'A':np.array([1,2,3]),C':np.Series(1,index=list(range(4))}
344/37: dic={'A':np.array([1,2,3]),'C':np.Series(1,index=list(range(4))}
344/38: dic={'A':np.array([1,2,3]),'C':np.Series(1,index=list(range(4)))}
344/39: dic={'A':np.array([1,2,3]),'C':pd.Series(1,index=list(range(4)))}
344/40: dic={'A':np.array([1,2,3]),'C':pd.Series(1,index=list(range(4))),'B': 'foo'}
344/41: df=pd.DataFrame(dic)
344/42: dic={'A':np.array([1,2,3,4]),'C':pd.Series(1,index=list(range(4))),'B': 'foo'}
344/43: df=pd.DataFrame(dic)
344/44: df
344/45: pd.Categorical(['1','t'])
344/46: x=pd.Categorical(['1','t','r','2'])
344/47: df.D=x
344/48: df.loc[:,'C']=x
344/49: df
344/50: df=pd.DataFrame(dic)
344/51: df
344/52: x=pd.Categorical(['1','t','r','2'])
344/53: df.loc[:,'D']=x
344/54: df
344/55: df.loc[:,'E']=[1,2,34,5]
344/56: df
344/57: df.loc[:,'E']={1,2,34,5}
344/58: df
344/59: df.loc[:,'E']={1,2,34,5}
344/60: df
344/61: df['F']={1,2,22,3}
344/62: df
344/63: df.describe()
344/64: df['D']=np.Categorical('A','B','S','FF')
344/65: df['D']=pd.Categorical('A','B','S','FF')
344/66: df['D']=pd.Categorical('A','B','S','F')
344/67: df['D']=pd.Categorical(['A','B','S','F'])
344/68: df.describe()
344/69: df['D']=pd.Categorical(['A','B','S','F'])
344/70: df
344/71: df.describe()
344/72: df
344/73: df.dtypes
344/74: df.sort_values(by='A')
344/75: df.sort_values(by=['A','B'])
344/76: df.sort_values(by=['A','E'])
344/77: df
344/78: df=pd.DataFrame(dic)
344/79: df
344/80: x=pd.Categorical(['1','t','r','2'])
344/81: df.loc[:,'D']=x
344/82: df
344/83: df.loc[:,'E']={1,2,34,5}
344/84: df['F']={1,2,22,3}
344/85: df
344/86: df['D']=pd.Categorical(['A','B','S','F'])
344/87: df.describe()
344/88: df.sort_values(by=['A','E'])
344/89: df.sort_values(by=['A','E'],ascending=False)
344/90: df[0:2]
344/91: df[0:2].dtype
344/92: df[0:2].dtypes
344/93: type(df[0:2])
344/94: df['A'[0]]
344/95: df
344/96: df['A'
344/97: df['A']
344/98: df.loc['A'[0]]
344/99: df.loc('A'[0])
344/100: df.loc(A[0])
344/101: df.loc[:,['A','B']
344/102: df.loc[:,['A','B']]
344/103: df[['A','B']]
344/104: type(df.loc[:,['A','B']])
344/105: type(df[['A','B']])
344/106: df.loc['A']>0
344/107: df.loc['A']
344/108: df
344/109: df.loc[['A']]
344/110: df.loc['A']
344/111: df['A']
344/112: df['A']>0
344/113: type(df['A']>0)
344/114: df['A'].isin([1,2])
344/115: type(df['A'].isin([1,2]))
344/116: data=pd.Dataframe({'A':1,2,3,'B':5,7,11})
344/117: data=pd.DataFrame({'A':1,2,3,'B':5,7,11})
344/118: dic={'A':1,2,3,'B':5,7,11}
344/119: dic={'A':{1,2,3},'B':{5,7,11}
344/120: dic={'A':{1,2,3},'B':{5,7,11}}
344/121: data=pd.DataFrame(dic)
344/122: data
344/123: dic={'A':[1,2,3],'B':5}
344/124: data=pd.DataFrame(dic)
344/125: data
344/126: dic={'A':[1,2,3],'B':[5,7,11]}
344/127: data=pd.DataFrame(dic)
344/128: data
344/129: data.isin([1,7])
344/130: data['A'].isin([1,7])
344/131: data(data['A'].isin([1,7]))
344/132: data[data['A'].isin([1,7])]
344/133: data['C']=np.array([1,3,2])
344/134: data
344/135: data.mean()
344/136: type(data.mean())
344/137: (data.mean())
344/138: (data.mean(axis=1))
344/139: type(data.mean(axis=1))
344/140: a=data.A
344/141: a.value_counts()
344/142: x=pd.Series([1,2,3,1,2,1,3,3,4])
344/143: x
344/144: x.values_counts()
344/145: x.value_counts()
344/146: type(x.value_counts())
344/147: f = pd.DataFrame(np.random.randn(10, 4))
344/148: f
344/149: df = pd.DataFrame(np.random.randn(10, 4))
344/150: df
344/151: pieces = [df[:3], df[3:7], df[7:]]
344/152: pieces
344/153: pieces.shape
344/154: len(pieces
344/155: len(pieces)
344/156: pieces
344/157: pd.concat(pieces)
344/158:
left = pd.DataFrame({'key': ['foo', 'foo'], 'lval': [1, 2]})
right = pd.DataFrame({'key': ['foo', 'foo'], 'rval': [4, 5]})
344/159: pd.merge(left,right)
344/160: left
344/161: df.iloc[1]
344/162: left.iloc[1]
344/163: left[0]
344/164: left[0:1]
344/165: type(left[0:1])
344/166: type(left.iloc[1])
344/167: (left.iloc[1])
344/168: (left.iloc[0])
344/169: data=pd.DataFrame({'A':[1,2,3],'B':[5,7,-1]})
344/170: x=pd.Series([1,-9,-4])
344/171: data.append(x)
344/172: data.append(x,ignore_index=True)
344/173: data
344/174:
data=pd.DataFrame({'A':[1,2,3],'B':[5,7,-1]})
data
344/175:
data=pd.DataFrame({'A':[1,2,3],'B':[5,7,-1]})
data
344/176:
x=pd.Series([1,-9,-4])
x
344/177: data.append(x,ignore_index=True)
344/178:
x=pd.Series([1,-9])
x
344/179: data.append(x,ignore_index=True)
344/180:
x=pd.Series({'A':[1],'B':7})
x
344/181:
x=pd.Series({'A':1,'B':7})
x
344/182:
data=pd.DataFrame({'A':[1,2,3],'B':[5,7,-1]})
data
344/183:
x=pd.Series({'A':1,'B':7})
x
344/184: data.append(x,ignore_index=True)
344/185: data.groupby('A')
344/186: df=data.groupby('A')
344/187: df
344/188: df.sum
344/189: df.sum()
344/190: data['C']=[-1,3,2]
344/191: data.groupby('A').sum()
344/192: data['A']=[1,1,2]
344/193: data.groupby('A').sum()
344/194:
data['A']=[1,1,2]
data
344/195: type(data.groupby('A').sum())
344/196: type(data.groupby('A').size())
344/197: (data.groupby('A').size())
344/198:
In [135]: ts = pd.Series(np.random.randn(1000),
   .....:                index=pd.date_range('1/1/2000', periods=1000))
   .....: 

In [136]: ts = ts.cumsum()
344/199: ts
344/200:
In [135]: ts = pd.Series(np.random.randn(1000),
   .....:                index=pd.date_range('1/1/2000', periods=1000))
   .....: 

In [136]: tss = ts.cumsum()
344/201: tss
345/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
345/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+'tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
345/3:
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
345/4:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
345/5:
def tokenize_tweet(tweet):
    tokens = [str(word) for word in str(tweet).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    tweet_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in tweet_nostopwords if word in model.vocab];
    return(filtered_word_list);
345/6:
def tweet_preprocess(row_tweet,model):
    return clean_text(model,remove_punctuation(tokenize_tweet(row_tweet)))
345/7:
# def tweet_preprocess(row_tweet):
#     return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]
345/8:
def vectorize_tweet(normalized_tweet,model):
    vec=np.zeros((300))
    for word in normalized_tweet:
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
345/9:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes')
    return tweets
345/10:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
345/11:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
345/12:
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        #tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
345/13:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
345/14:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'politics' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'Johnson & Johnson, one of the world s largest drug manufacturers, has gone on trial in a multi-billion dollar lawsuit by the US state of Oklahoma.'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
345/15:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = '' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'no collusion no obstruction'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
345/16: recommendations = process_user_input(user_input, time_limit, topic)
345/17:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'trump' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'no collusion no obstruction'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
345/18: recommendations = process_user_input(user_input, time_limit, topic)
345/19: recommendations
345/20: recommendations.iloc[1]['tweet']
345/21: recommendations.iloc[0]['tweet']
345/22:
with pd.option_context('display.max_colwidth', 300):
    print (recommendations)
345/23:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets #[0:topn]
345/24: recommendations = process_user_input(user_input, time_limit, topic)
345/25: recommendations.iloc[0]['tweet']
345/26: recommendations
345/27:
with pd.option_context('display.max_colwidth', 300):
    print (recommendations)
345/28:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'USA' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
345/29: recommendations = process_user_input(user_input, time_limit, topic)
345/30: recommendations.iloc[0]['tweet']
345/31: recommendations
345/32:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
time_limit=10
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
345/33: recommendations = process_user_input(user_input, time_limit, topic)
345/34: recommendations.iloc[0]['tweet']
345/35:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
345/36: recommendations = process_user_input(user_input, time_limit, topic)
345/37: recommendations.iloc[0]['tweet']
345/38: recommendations
345/39: recommendations
345/40:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
print('tweets with '+str(topic)+' obtained')
data = pd.read_json(data_dir+file_name+".json",lines=True)
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
345/41:
for i in data.index:
    data[i]
345/42:
for i in data.index:
    data.loc[i,:]
345/43:
for i in data.index:
    print(data.loc[i,:])
345/44:
data=data[0:3]
for i in data.index:
    print(data.loc[i,:])
345/45:
data=data[0:1]
for i in data.index:
    print(data.loc[i,:])
345/46:
d=data[0:1]
for i in d.index:
    print(data.loc[i,:])
345/47:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
print('tweets with topic '+str(topic)+' obtained')
data = pd.read_json(data_dir+file_name+".json",lines=True)
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
345/48:
d=data[0:1]
for i in d.index:
    print(data.loc[i,:])
345/49:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
print('tweets with topic\" '+str(topic)+' \"obtained')
data = pd.read_json(data_dir+file_name+".json",lines=True)
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
347/1:
import pandas as pd
import numpy as np
347/2: s=pd.Series(np.random.randn(5),index=['a','b','c','d','e'])
347/3: s[[0]]
347/4: s[0]
347/5:
s=pd.Series(np.random.randn(5),index=['a','b','c','d','e'])
s
347/6: type(s[[0]])
347/7: type(s[0])
347/8: np.size(s)
347/9: np.mean(s)
347/10: s.mean()
347/11: exp(s)
347/12: np.var(s)
347/13: data=np.DataFrame({'A':np.arrange(3)})
347/14: data=pd.DataFrame({'A':np.arrange(3)})
347/15: data=pd.DataFrame({'A':np.range(3)})
347/16: data=pd.DataFrame({'A':[1,2,3]})
347/17:
data=pd.DataFrame({'A':[1,2,3]})
data
347/18:
data=pd.DataFrame({'A':[1,2,3],'B':[2,3,-1]})
data
347/19:
data=pd.DataFrame({'A':[1,2,3],'B':[2,3,-1]})
data.size()
347/20:
data=pd.DataFrame({'A':[1,2,3],'B':[2,3,-1]})
data.size
347/21:
data=pd.DataFrame({'A':[1,2,3],'B':[2,3,-1]})
data.mean
347/22:
data=pd.DataFrame({'A':[1,2,3],'B':[2,3,-1]})
data
347/23: data.mean
347/24: data.apply(lambda x:x.mean)
347/25: data.A.apply(lambda x:x.mean)
347/26: np.exp(data)
347/27: 1 in data
347/28: 2 in data
347/29: [1,2,3] in data
347/30:
d={'one': pd.Series([1,2,3],index=['a','b','c']),'two':pd.Series([3,4,4],index=['a','b','c'])}
d
347/31: data
347/32: data.iloc[0,1]
347/33: data.shape[0]
347/34: data.iloc[0]
347/35: type(data.iloc[0])
347/36: data.iloc[0]
347/37: x=data.iloc[0]
347/38:
x=data.iloc[0]
x
347/39: x[0]
347/40: x.index
347/41: data.colnames
347/42: data.columns
347/43: data.iloc[:1]
347/44: data.iloc[:2]
347/45: type(data.iloc[:2])
347/46: data.iloc[1:]
347/47: data.iloc[0:]
347/48: data.iloc[1:]
347/49: data
347/50: data.iloc[-1]
347/51: data.iloc[-2]
347/52: data.iloc[1:2]
347/53: data.iloc[0:1]
347/54: data.iloc[0:2]
347/55: df.loc[0]
347/56: df.loc['A']
347/57: data.loc[0]
347/58: data.loc['A']
347/59: data.loc[:,'A']
347/60: data.iloc[:,'A']
347/61: data.iloc[:,0]
347/62: data.loc['A']
347/63: data.loc[['A']]
347/64: data.iloc[['A']]
347/65: data.loc[['A']]
347/66: data[['A']]
347/67: type(data.iloc[:,0])
347/68: tye(data[['A']])
347/69: type(data[['A']])
347/70: data.loc[:,'A']
347/71: type(data.loc[:,'A'])
347/72: type(data.loc[:,0])
347/73: type(data.loc[:,'A'])
347/74: data.index
347/75: data.index=list('abs')
347/76: data
347/77: data.loc['a','A']
347/78: data.loc['a',0]
347/79: data[['C']]=[3,-7,4]
347/80: data.index=list('absd')
347/81: data.index=[0,1,2]
347/82: data[['C']]=[3,-7,4]
347/83:
df=pd.DataFrame(d)
df
347/84:
>>> df = pd.DataFrame(np.arange(12).reshape(3,4),
...                   columns=['A', 'B', 'C', 'D']
347/85:
>>> df = pd.DataFrame(np.arange(12).reshape(3,4),
...                   columns=['A', 'B', 'C', 'D'])
347/86: df
347/87: df.ioc[0]
347/88: df.iloc[0]
347/89: df.iloc[:,0]
347/90: df.iloc[1:2]
347/91: df.iloc[1:3]
347/92: df[['A']]
347/93: df.A
347/94: df.loc[:,'A']
347/95: df.iloc[:,0]
347/96: df[0]
347/97: df['A']
347/98: df.loc[df['A']>0,['B','C']]
347/99: df.loc[0,df.iloc[0]>2]
347/100: df.loc[0:2,df.iloc[0]>2]
347/101: df.loc[0:2,df.iloc[0]>1]
347/102: df.loc[0:1,df.iloc[0]>1]
347/103: df.iloc[0:2]
347/104: df.loc[0:2,1:2]
347/105: df.loc[0:2,1]
347/106: df.loc[0:2,'A']
347/107: df.loc[0:2,df.iloc[0]>1]
347/108: df.loc[0:1,df.iloc[0]>1]
347/109: df.iloc[0:1,0:2]
347/110: df.loc[0:1,:]
347/111: df.loc[0:1,['A','B']]
347/112: df[0]
347/113: df[0:2
347/114: df[0:2]
347/115: df[0:1]
347/116: df[df.iloc[0]>0]
347/117: df.iloc[0]>0
347/118: df.loc[0]>0
347/119: df[df.loc[0]>0]
347/120: df[df.iloc[:,0]>0]
347/121: df[df.A>0]
347/122: df[df['A'>0]
347/123: df[df[['A']>0]
347/124: df[df['A']>0]
347/125: df[df[['A']]>0]
347/126: df.A==1
347/127: df
347/128: df.loc[:,'E']=df.A*df.B
347/129: df
347/130: np.dot(df.A,df.B)
347/131: df
347/132: df.drop('E')
347/133: df.drop(columns='E')
347/134: df.loc[df.A>0 and df.B<9,'E']
347/135: df.loc[(df.A>0 and df.B<9),'E']
347/136: df.loc[(df.A>0 & df.B<9),'E']
347/137: df.loc[(df.A>0 & df.B<9),'E']=False
347/138: df.loc[df.A>0 ,'E']=False
347/139: df.loc[(df.A>0)&(df.B<2) ,'E']=False
347/140: df
347/141: df.loc[(df.A>0)&(df.B>1) ,'E']=False
347/142: df
347/143: df['F']=1
347/144: df.loc[:,'F']=0
347/145: df
347/146: df.drop(columns='F')
347/147: df.drop(columns='E')
347/148: df['E']=df['C'].apply(lambda x:True if x>2 else False)
347/149: df
347/150: np.where(df.E==False)
347/151: df[np.where(df.E==False)]
347/152: df.loc[np.where(df.E==False),:]
347/153: df.loc[np.where(df.E=False),:]
347/154: np.where(df.E=False)
347/155: np.where(df.E==False)
347/156: df.drop(columns='E')
347/157: df['E']=np.where(df.C>2,True,False)
347/158: df
347/159: df
347/160: condchoice=[x==0,x<=4,x>4]
347/161: choicelise=['no','yes','maybe']
347/162: df['G']=np.select(df.A,condchoice,choicelise)
347/163: np.select(df.A,condchoice,choicelise)
347/164: condchoice=[x=0,x<=4,x>4]
347/165: condchoice=[x==0,x<=4,x>4]
347/166: choicelise=['no','yes','maybe']
347/167: np.select(df.A,condchoice,choicelise)
347/168:
x=df.A
condchoice=[x==0,x<=4,x>4]
347/169: choicelise=['no','yes','maybe']
347/170: np.select(dcondchoice,choicelise)
347/171: np.select(condchoice,choicelise)
347/172: df['G']=np.select(condchoice,choicelise)
347/173: df
347/174:
x=df.A
condchoice=[x==0,x<=4,x>4]
347/175: choicelise=['no','yes','maybe']
347/176: df['G']=np.select(condchoice,choicelise)
347/177: df
347/178:
x=df.A
condchoice=[x==0,x<=4,x>8]
347/179: choicelise=['no','yes','maybe']
347/180: df['G']=np.select(condchoice,choicelise)
347/181: df
347/182: df['G']=np.select(condchoice,choicelise,default='haha')
347/183: df
348/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
348/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/3:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
print('tweets with topic\" '+str(topic)+' \"obtained')
data = pd.read_json(data_dir+file_name+".json",lines=True)
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
348/4:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes')
    return tweets
348/5:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
348/6:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets #[0:topn]
348/7:
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        #tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
348/8:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
348/9:
def get_full_text_tweet()
for i in data.index:  
       tweets.time[i] = data.created_at[i]
       if pd.isnull(data.retweeted_status[i]):
             if pd.isnull(data.extended_tweet[i]):
                    tweets.tweet[i] = data.text[i]
             else:   
                if "full_text" in data.extended_tweet[i].keys():
                     tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                else:
                     tweets.tweet[i]=data.text[i] 
       else:
            if 'extended_tweet' in data.retweeted_status[i].keys():
                if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                    tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
            else:
                 tweets.tweet[i] = data.retweeted_status[i]['text']     
tweets = tweets.sort_values('time', ascending=False)
tweets=tweets.drop_duplicates()
tweets.dropna(subset=['tweet'])
348/10:
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
348/11:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
348/12:
def tokenize_tweet(tweet):
    tokens = [str(word) for word in str(tweet).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    tweet_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in tweet_nostopwords if word in model.vocab];
    return(filtered_word_list);
348/13:
def tweet_preprocess(row_tweet,model):
    return clean_text(model,remove_punctuation(tokenize_tweet(row_tweet)))
348/14:
# def tweet_preprocess(row_tweet):
#     return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]
348/15:
def vectorize_tweet(normalized_tweet,model):
    vec=np.zeros((300))
    for word in normalized_tweet:
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
348/16:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes')
    return tweets
348/17:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
348/18:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets #[0:topn]
348/19:
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        #tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
348/20:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
347/184: df.apply(lambda x:x+1)
347/185: df.drop(columns=['E','G'])
347/186: df.apply(lambda x:x+1)
347/187: df=df.drop(columns=['E','G'])
347/188: df.apply(lambda x:x+1)
347/189: df.apply(lambda x:x.loc[0,:]+1)
347/190: x.loc[0,:]
347/191: df.loc[0,:]
347/192: df.apply(lambda x:x.loc[0,:]+1,axis=1)
347/193: df.apply(lambda x:x.A,axis=1)
347/194:
df=df.drop(columns=['E','G'])
df
347/195: df
347/196: df.apply(lambda x:x.loc[:,0],axis=1)
347/197: df.apply(lambda x:x.A,axis=1)
347/198: df.loc[:,0]
347/199: df.loc[:,'A']
347/200: df.iloc[0,:]
347/201: df.iloc[0]
348/21:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
print('tweets with topic\" '+str(topic)+' \"obtained')
data = pd.read_json(data_dir+file_name+".json",lines=True)
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
348/22:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
348/23:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
print('tweets with topic\" '+str(topic)+' \"obtained')
data = pd.read_json(data_dir+file_name+".json",lines=True)
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
348/24:
d=data[0:2]
for i in d.index:
    tweets.time[i] = data.created_at[i]
    
    #full_tweet = get_full_text_tweet(data.loc[i,:])
348/25:
d=data[0:2]
for i in d.index:
    tweets.time[i] = data.created_at[i]
    print(data.loc[i,:])
    
    #full_tweet = get_full_text_tweet(data.loc[i,:])
348/26:
d=data[0:2]
for i in d.index:
    tweets.time[i] = data.created_at[i]
    print(data.loc[i,:])
    x=data.loc[i,:]
    
    #full_tweet = get_full_text_tweet(data.loc[i,:])
348/27: x
348/28: x.retweeted_status
348/29:
def get_full_text_tweet(tweet):
       if pd.isnull(tweet.retweeted_status):
             if pd.isnull(tweet.extended_tweet):
                    full_text = tweet.text
             else:   
                if "full_text" in tweet.extended_tweet.keys():
                     full_text=tweet.extended_tweet["full_text"]

                else:
                     full_text=tweet.text 
       else:
            if 'extended_tweet' in tweet.retweeted_status.keys():
                if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                    full_text= tweet.retweeted_status['extended_tweet']["full_text"]
            else:
                 full_text = tweet.retweeted_status['text']  
       return full_text
348/30:
d=data[0:2]
for i in d.index:
    tweets.time[i] = data.created_at[i]
    #print(data.loc[i,:])
    full_tweet = get_full_text_tweet(data.loc[i,:])
    print(full_tweet)
    
    #full_tweet = get_full_text_tweet(data.loc[i,:])
348/31:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
    tweets.time[i] = data.created_at[i]
    tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
348/32:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
348/33: get_latest_tweets(data_dir, auth, time_limit, topic)
348/34:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
            tweets = tweets.sort_values('time', ascending=False)
            tweets=tweets.drop_duplicates()
            tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/35: get_latest_tweets(data_dir, auth, time_limit, topic)
348/36:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with '+str(topic)+' obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
            tweets = tweets.sort_values('time', ascending=False)
            tweets=tweets.drop_duplicates()
            tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/37: get_latest_tweets(data_dir, auth, time_limit, topic)
348/38:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with topic\" '+str(topic)+'\" obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
            tweets = tweets.sort_values('time', ascending=False)
            tweets=tweets.drop_duplicates()
            tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/39: get_latest_tweets(data_dir, auth, time_limit, topic)
348/40:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with topic\" '+str(topic)+'\" obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
            #tweets = tweets.sort_values('time', ascending=False)
            tweets=tweets.drop_duplicates()
            tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/41: get_latest_tweets(data_dir, auth, time_limit, topic)
348/42:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with topic\" '+str(topic)+'\" obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
            #tweets = tweets.sort_values('time', ascending=False)
            #tweets=tweets.drop_duplicates()
            #tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/43: get_latest_tweets(data_dir, auth, time_limit, topic)
348/44:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with topic\" '+str(topic)+'\" obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
    tweets = tweets.sort_values('time', ascending=False)
    weets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/45:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with topic\" '+str(topic)+'\" obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
    tweets = tweets.sort_values('time', ascending=False)
    weets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/46: get_latest_tweets(data_dir, auth, time_limit, topic)
348/47: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
348/48: tweets
348/49: tweets.time
348/50: tweets.tweet
347/202:
def p(x):
    return x.sum()
347/203: df.apply(lambda x:p(x),axis=1)
347/204: df
348/51:

                
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
    return full_text
348/53:

                
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
  return full_text
348/55:

                
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
348/56: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
348/57: tweets.tweet
348/58:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with topic \"'+str(topic)+'\" obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/59: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
348/60: tweets.tweet
348/61: tweets.isna.sum
348/62: tweets.isna.sum()
348/63: tweets.isna
348/64: tweets.tweet.isna
348/65:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
print('tweets with topic\" '+str(topic)+' \"obtained')
data = pd.read_json(data_dir+file_name+".json",lines=True)
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
348/66:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with topic \"'+str(topic)+'\" obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/67: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
348/68:  tweets.dropna(subset=['tweet'])
348/69: tweets.isna().sum()
348/70:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    print('tweets with topic \"'+str(topic)+'\" obtained')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/71: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
348/72: tweets.isna().sum()
347/205: df
347/206: df.drop(0)
347/207:
>>> df = pd.DataFrame(np.arange(12).reshape(3,4),
...                   columns=['A', 'B', 'C', 'D'])
347/208: df.drop([1,2])
348/73: tweets.isna()
348/74:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track

data = pd.read_json(data_dir+file_name+".json",lines=True)
print(str(data.shap[0])+'tweets with topic\" '+str(topic)+' \"obtained')
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
348/75:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track

data = pd.read_json(data_dir+file_name+".json",lines=True)
print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
348/76:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
            tweets.time[i] = data.created_at[i]
            tweets.tweet[i] = get_full_text_tweet(data.loc[i,:])
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/77: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
348/78: data
348/79: tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x))
348/80: tweets['tweet'] = data.apply(lambda x: print(x))
348/81: tweets['tweet'] = data.apply(lambda x: print(x.size()))
348/82: tweets['tweet'] = data.apply(lambda x: print(x.size)
348/83: tweets['tweet'] = data.apply(lambda x: print(x.size))
348/84: tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
348/85: tweets
348/86: data
348/87: data.iloc[20362]
348/88: data.iloc[index=20362]
348/89: data.iloc[index='20362']
348/90: data
348/91: data.tail()
348/92: data.tail(50)
348/93: data[-1:-100]
348/94: data[1:200]
348/95: data[2000:2200]
348/96: data[2000:3000]
348/97: data[1000:3000]
348/98: data[1000:40000]
348/99: data[3000:40000]
348/100: data[30000:40000]
348/101: data[3000:40000]
348/102: data[5000:40000]
348/103: data
348/104: tweets.shape
348/105: tweets.shape
348/106:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track

data = pd.read_json(data_dir+file_name+".json",lines=True)
print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    #To get the full-text of the tweet
    for i in data.index:  
           tweets.time[i] = data.created_at[i]
           if pd.isnull(data.retweeted_status[i]):
                 if pd.isnull(data.extended_tweet[i]):
                        tweets.tweet[i] = data.text[i]
                 else:   
                    if "full_text" in data.extended_tweet[i].keys():
                         tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                    else:
                         tweets.tweet[i]=data.text[i] 
           else:
                if 'extended_tweet' in data.retweeted_status[i].keys():
                    if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                        tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                else:
                     tweets.tweet[i] = data.retweeted_status[i]['text']     
    tweets = tweets.sort_values('time', ascending=False)
    tweets=tweets.drop_duplicates()
    tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
348/107: data.shape
348/108:

tweets.shape
348/109:

tweets.shape
348/110: data.shape
348/111:
T=pd.DataFrame(columns=['time','tweet'],index=data.index)
T['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
T = T.sort_values('time', ascending=False)
T=T.drop_duplicates()
T.dropna(subset=['tweet'])
348/112: T.shape
348/113: T['time']=data.data.created_at
348/114: T['time']=data.created_at
348/115: T.shape
348/116: data.shape
348/117: tweets.shape
348/118:
T=pd.DataFrame(columns=['time','tweet'],index=data.index)
T['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
T['time']=data.created_at
T = T.sort_values('time', ascending=False)
T=T.drop_duplicates()
T.dropna(subset=['tweet'])
347/209: df.A.isnull
347/210: df.A.isnull
347/211: df.A.isnull()
347/212: df.isnull()
348/119: data.retweeted_status
348/120: type(data.retweeted_status)
348/121: type(data.iloc[0,:])
348/122: type(data.iloc[0,:].retweeted_status)
348/123: pd.isnull(data.iloc[0,:].retweeted_status)
348/124: data.iloc[0,:].retweeted_status
348/125: data.iloc[0,:].retweeted_status.isnull()
347/213: d={}
347/214:
if pd.isnull(d):
    print('dd')
347/215: d={'d'}
347/216:
if pd.isnull(d):
    print('dd')
347/217: d={'d':[1,2]}
347/218:
if pd.isnull(d):
    print('dd')
347/219:
if np.isnull(d):
    print('dd')
347/220: df.apply(lambda x:type(x),axis=1)
347/221: df.apply(lambda x:x.sum(),axis=1)
348/126: data.iloc[0,:]
348/127: type(data.iloc[0,:])
347/222: len(d)
347/223:
dd={}
len(dd)
348/128: data.iloc[0,:]extended_tweet
348/129: data.iloc[0,:].extended_tweet
348/130: type(data.iloc[0,:].extended_tweet)
348/131: type(data.iloc[1,:].extended_tweet)
348/132: type(data.iloc[3,:].extended_tweet)
348/133: type(data.iloc[10,:].extended_tweet)
348/134: type(data.iloc[20,:].extended_tweet)
348/135:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if len(tweet.extended_tweet)==0:
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
348/136:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/137: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
348/138:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
348/139:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
348/140: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
349/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
349/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    toc=time.time()
    print('tweets with '+str(topic)+' obtained in '+str((tic-toc)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    l=time.time()
    print('reading the file takes '+str((l-toc)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    tocc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((tocc-toc)/60)+' minutes', tweets)
    return tweets, no_data
349/3:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
349/4:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    l=time.time()
    print('reading the file takes '+str((l-toc)/60)+' minutes')
    print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
349/5: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
349/6:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
349/7:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    l=time.time()
    print('reading the file takes '+str((l-toc)/60)+' minutes')
    print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
349/8: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
349/9:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
349/10:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    toc=time.time()
    print('tweets with '+str(topic)+' obtained in '+str((tic-toc)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    l=time.time()
    print('reading the file takes '+str((l-toc)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    tocc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((tocc-toc)/60)+' minutes', tweets)
    return tweets, no_data
349/11:
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
349/12:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
349/13: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
349/14:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    l=time.time()
    print('reading the file takes '+str((l-toc)/60)+' minutes')
    print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
349/15: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
349/16:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    ll=time.time()
    print('reading the file takes '+str((ll-l)/60)+' minutes')
    print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
349/17: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
349/18:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    ll=time.time()
    print('reading the file takes '+str((ll-l)/60)+' minutes')
    #print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
349/19: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
349/20:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
349/21: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
349/22:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    ll=time.time()
    print('reading the file takes '+str((ll-l)/60)+' minutes')
    #print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
349/23: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
350/1:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
350/2:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
350/3:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    ll=time.time()
    print('reading the file takes '+str((ll-l)/60)+' minutes')
    #print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
350/4:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
350/5:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
350/6:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
350/7:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
time_limit=20
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
350/8: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
350/9: ?StreamListener
350/10: ?StreamListener.on_data
350/11: ?Stream
350/12: data.columns
350/13:
file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track

data = pd.read_json(data_dir+file_name+".json",lines=True)
print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
    tweets['time']=data.created_at
    tweets = tweets.sort_values('time', ascending=False)
    tweets = tweets.drop_duplicates()
    tweets = tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
350/14: data.columns
350/15: d=data.copy()
350/16:
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
       'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
       'possibly_sensitive', 'quote_count', 'quoted_status',
       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
       'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
       'source', 'text', 'timestamp_ms', 'truncated', 'user',
       'withheld_in_countries']
data=pd.DataFraem(columns=col_list)
350/17:
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
       'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
       'possibly_sensitive', 'quote_count', 'quoted_status',
       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
       'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
       'source', 'text', 'timestamp_ms', 'truncated', 'user',
       'withheld_in_countries']
data=pd.DataFrame(columns=col_list)
350/18:
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
       'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
       'possibly_sensitive', 'quote_count', 'quoted_status',
       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
       'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
       'source', 'text', 'timestamp_ms', 'truncated', 'user',
       'withheld_in_countries']
data=pd.DataFrame(columns=col_list)
data
350/19:
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
       'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
       'possibly_sensitive', 'quote_count', 'quoted_status',
       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
       'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
       'source', 'text', 'timestamp_ms', 'truncated', 'user',
       'withheld_in_countries']

data=pd.DataFrame(columns=col_list,ignore_index = True)
data
350/20:
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
       'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
       'possibly_sensitive', 'quote_count', 'quoted_status',
       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
       'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
       'source', 'text', 'timestamp_ms', 'truncated', 'user',
       'withheld_in_countries']

data=pd.DataFrame(columns=col_list)
data
350/21: ?StreamListener.on_data
350/22:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=5):
        self.start_time = time.time()
        self.limit = time_limit
        #self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, raw_data):
        if (time.time() - self.start_time) < self.limit:
            data=rawdata
            print(data)
            return True
        else:
            #self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
350/23:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
       'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
       'possibly_sensitive', 'quote_count', 'quoted_status',
       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
       'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
       'source', 'text', 'timestamp_ms', 'truncated', 'user',
       'withheld_in_countries']
#data=pd.DataFrame(columns=col_list,ignore_index = True)
data='ddfbalaaajhdj'
#file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track

#data = pd.read_json(data_dir+file_name+".json",lines=True)
print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
    tweets['time']=data.created_at
    tweets = tweets.sort_values('time', ascending=False)
    tweets = tweets.drop_duplicates()
    tweets = tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
350/24:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=5):
        self.start_time = time.time()
        self.limit = time_limit
        #self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, raw_data):
        if (time.time() - self.start_time) < self.limit:
            data=raw_data
            print(data)
            return True
        else:
            #self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
350/25:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
       'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
       'possibly_sensitive', 'quote_count', 'quoted_status',
       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
       'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
       'source', 'text', 'timestamp_ms', 'truncated', 'user',
       'withheld_in_countries']
#data=pd.DataFrame(columns=col_list,ignore_index = True)
data='ddfbalaaajhdj'
#file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track

#data = pd.read_json(data_dir+file_name+".json",lines=True)
print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
    tweets['time']=data.created_at
    tweets = tweets.sort_values('time', ascending=False)
    tweets = tweets.drop_duplicates()
    tweets = tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
350/26: data
350/27:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
       'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
       'possibly_sensitive', 'quote_count', 'quoted_status',
       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
       'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
       'source', 'text', 'timestamp_ms', 'truncated', 'user',
       'withheld_in_countries']
global data
data='ba;alalajd'
#data=pd.DataFrame(columns=col_list,ignore_index = True)
#file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track

#data = pd.read_json(data_dir+file_name+".json",lines=True)
print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
    tweets['time']=data.created_at
    tweets = tweets.sort_values('time', ascending=False)
    tweets = tweets.drop_duplicates()
    tweets = tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
350/28: data
350/29:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=5):
        self.start_time = time.time()
        self.limit = time_limit
        #self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, raw_data):
        if (time.time() - self.start_time) < self.limit:
            data=raw_data
            print(data)
            return True
        else:
            #self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
350/30:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
       'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
       'possibly_sensitive', 'quote_count', 'quoted_status',
       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
       'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
       'source', 'text', 'timestamp_ms', 'truncated', 'user',
       'withheld_in_countries']
global data
data='ba;alalajd'
#data=pd.DataFrame(columns=col_list,ignore_index = True)
#file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track

#data = pd.read_json(data_dir+file_name+".json",lines=True)
print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"obtained')
tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
print('tweets are saved!')
no_data = False
if data.empty:
    no_data = True
else:
    data = data[data.lang=='en']
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
    tweets['time']=data.created_at
    tweets = tweets.sort_values('time', ascending=False)
    tweets = tweets.drop_duplicates()
    tweets = tweets.dropna(subset=['tweet'])
toc = time.time()
print(str(tweets.shape[0])+' tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
350/31: data
350/32:
#-------new-------------------------
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
        global data
        data='ba;alalajd'
#data=pd.DataFrame(columns=col_list,ignore_index = True)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track

#data = pd.read_json(data_dir+file_name+".json",lines=True)
350/33:
#-------new-------------------------
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
        global data
        data='ba;alalajd'
#data=pd.DataFrame(columns=col_list,ignore_index = True)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track

#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
test()
350/34: data
350/35:
#-------new-------------------------
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
        global data
        data='ba;alalajd'
#data=pd.DataFrame(columns=col_list,ignore_index = True)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        return data
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
test()
350/36: data
350/37:
#-------new-------------------------
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
        global data
        data='ba;alalajd'
#data=pd.DataFrame(columns=col_list,ignore_index = True)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        return data
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
data=test()
350/38: data
350/39:
#-------new-------------------------
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
        global data
        #data='ba;alalajd'
#data=pd.DataFrame(columns=col_list,ignore_index = True)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        return data
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
data=test()
350/40: data
350/41:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
data='sjkjd'   
#data=pd.DataFrame(columns=col_list,ignore_index = True)
def test():

#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
test()
350/42: data
351/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
351/2:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    ll=time.time()
    print('reading the file takes '+str((ll-l)/60)+' minutes')
    #print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
351/3:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
351/4:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=5):
        self.start_time = time.time()
        self.limit = time_limit
        #self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, raw_data):
        if (time.time() - self.start_time) < self.limit:
            data=raw_data
            print(data)
            return True
        else:
            #self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
351/5:
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
351/6:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
data='sjkjd'   
#data=pd.DataFrame(columns=col_list,ignore_index = True)
def test():

#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
test()
351/7:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
351/8:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
data='sjkjd'   
#data=pd.DataFrame(columns=col_list,ignore_index = True)
def test():

#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
test()
351/9: data
351/10: data
351/11:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
#data=pd.DataFrame(columns=col_list,ignore_index = True)
def test():

#file_name = "stream"
        print("streaming tweets...")
        global data
        data='ddd'
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3, data),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        return data
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
data=test()
351/12:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data, data_dir, time_limit=5):
        self.start_time = time.time()
        self.limit = time_limit
        #self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, raw_data):
        if (time.time() - self.start_time) < self.limit:
            data=raw_data
            print(data)
            return True
        else:
            #self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
351/13:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
#data=pd.DataFrame(columns=col_list,ignore_index = True)
def test():

#file_name = "stream"
        print("streaming tweets...")
        global data
        data='ddd'
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data,data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        return data
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
data=test()
351/14: data
351/15:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data, data_dir, time_limit=5):
        self.start_time = time.time()
        self.limit = time_limit
        self.data = data
        #self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, raw_data):
        if (time.time() - self.start_time) < self.limit:
            self.data = raw_data
            print(data)
            return True
        else:
            #self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
351/16: ?Mylistener.data
351/17: ?Mylistener.on_error
351/18:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data, data_dir, time_limit=5):
        self.start_time = time.time()
        self.limit = time_limit
        self.data = data
        #self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, raw_data):
        if (time.time() - self.start_time) < self.limit:
            self.data.append(raw_data)
            print(raw_data)
            return True
        else:
            #self.saveFile.close()
            return False
            
    def on_error(self, status):
        print(status)
        return True
351/19:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
data=pd.DataFrame(columns=col_list,ignore_index = True)
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
         data=pd.DataFrame(columns=col_list,ignore_index = True)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data, data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        return data
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
data=test()
351/20:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
data=pd.DataFrame(columns=col_list,ignore_index = True)
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
        data=pd.DataFrame(columns=col_list,ignore_index = True)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data, data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        return data
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
data=test()
351/21:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
data=pd.DataFrame(columns=col_list)
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
        data=pd.DataFrame(columns=col_list)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data, data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        return data
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
data=test()
351/22:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
data=pd.DataFrame(columns=col_list)
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
        data=pd.DataFrame(columns=col_list)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data, data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
test()
351/23:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
data=pd.DataFrame(columns=col_list)
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
        data=pd.DataFrame(columns=col_list)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data, data_dir, time_limit=3),tweet_mode='extended')
        #twitter_stream.filter(track= topic) # list of querries to track
        
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
test()
351/24: data
351/25:
#--------------------new----------------------------------------------
#MyListener() saves the data into a .json file with name stream
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=5):
        self.start_time = time.time()
        self.limit = time_limit
        #self.saveFile = open(data_dir+"stream.json", 'a')
        super()

    def on_data(self, raw_data):
        if (time.time() - self.start_time) < self.limit:
            print(raw_data)
            return True
        else:
            #self.saveFile.close()
            return False
            
    def on_error(self, status):
        print(status)
        return True
351/26:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
data=pd.DataFrame(columns=col_list)
def test():
        col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
        data=pd.DataFrame(columns=col_list)
#file_name = "stream"
        print("streaming tweets...")
        tic = time.time()
        twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended')
        twitter_stream.filter(track= topic) # list of querries to track
        
#data = pd.read_json(data_dir+file_name+".json",lines=True)
   
test()
351/27: twitter_stream
351/28:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
data=pd.DataFrame(columns=col_list)
#file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended')
twitter_stream.filter(track= topic) # list of querries to track
        
#data = pd.read_json(data_dir+file_name+".json",lines=True)
351/29: twitter_stream
351/30: ?twitter_stream
351/31: ?twitter_stream.filter
351/32: ?Stream
351/33:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
data=pd.DataFrame(columns=col_list)
#file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended',z=2)
twitter_stream.filter(track= topic) # list of querries to track
        
#data = pd.read_json(data_dir+file_name+".json",lines=True)
351/34:
#-------new-------------------------
col_list=['contributors', 'coordinates', 'created_at', 'display_text_range',
        'entities', 'extended_entities', 'extended_tweet', 'favorite_count',
        'favorited', 'filter_level', 'geo', 'id', 'id_str',
        'in_reply_to_screen_name', 'in_reply_to_status_id',
        'in_reply_to_status_id_str', 'in_reply_to_user_id',
        'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',
        'possibly_sensitive', 'quote_count', 'quoted_status',
        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',
        'reply_count', 'retweet_count', 'retweeted', 'retweeted_status',
        'source', 'text', 'timestamp_ms', 'truncated', 'user',
        'withheld_in_countries']
   
data=pd.DataFrame(columns=col_list)
#file_name = "stream"
print("streaming tweets...")
tic = time.time()
twitter_stream = Stream(auth, MyListener(data_dir, time_limit=3),tweet_mode='extended',z=2)
twitter_stream.filter(track= topic) # list of querries to track
        
#data = pd.read_json(data_dir+file_name+".json",lines=True)
352/1:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
352/2:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
352/3:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    toc=time.time()
    print('tweets with '+str(topic)+' obtained in '+str((tic-toc)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    l=time.time()
    print('reading the file takes '+str((l-toc)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    tocc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((tocc-toc)/60)+' minutes', tweets)
    return tweets, no_data
352/4: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
352/5:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
352/6:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    toc=time.time()
    print('tweets with '+str(topic)+' obtained in '+str((tic-toc)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    l=time.time()
    print('reading the file takes '+str((l-toc)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    tocc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((tocc-toc)/60)+' minutes', tweets)
    return tweets, no_data
352/7: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
352/8:
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        #query_fname = format_filename(query)
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
352/9: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
352/10:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    toc=time.time()
    print('tweets with '+str(topic)+' obtained in '+str((tic-toc)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    l=time.time()
    print('reading the file takes '+str((l-toc)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    ll=time.time()
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    tocc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((tocc-toc)/60)+' minutes'+str((tocc-ll)/60), tweets)
    return tweets, no_data
352/11:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    ll=time.time()
    print('reading the file takes '+str((ll-l)/60)+' minutes')
    #print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes'+str((toc-ll)/60), tweets)
    return tweets, no_data
352/12:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    toc=time.time()
    print('tweets with '+str(topic)+' obtained in '+str((tic-toc)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    l=time.time()
    print('reading the file takes '+str((l-toc)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    ll=time.time()
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        #To get the full-text of the tweet
        for i in data.index:  
               tweets.time[i] = data.created_at[i]
               if pd.isnull(data.retweeted_status[i]):
                     if pd.isnull(data.extended_tweet[i]):
                            tweets.tweet[i] = data.text[i]
                     else:   
                        if "full_text" in data.extended_tweet[i].keys():
                             tweets.tweet[i]=data.extended_tweet[i]["full_text"]

                        else:
                             tweets.tweet[i]=data.text[i] 
               else:
                    if 'extended_tweet' in data.retweeted_status[i].keys():
                        if "full_text" in data.retweeted_status[i]['extended_tweet'].keys():
                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet']["full_text"]
                    else:
                         tweets.tweet[i] = data.retweeted_status[i]['text']     
        tweets = tweets.sort_values('time', ascending=False)
        tweets=tweets.drop_duplicates()
        tweets.dropna(subset=['tweet'])
    tocc = time.time()
    print(str(tweets.shape[0])+' tweets obtained in '+str((tocc-toc)/60)+' minutes'+str((tocc-ll)/60), tweets)
    return tweets, no_data
352/13: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
352/14:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('raw tweets are obtained in '+str((l-tic)/60)+' minutes')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    ll=time.time()
    print('reading the file takes '+str((ll-l)/60)+' minutes')
    #print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('tweets are saved!')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full text tweets obtained in '+str((toc-tic)/60)+' minutes'+str((toc-ll)/60), tweets)
    return tweets, no_data
352/15: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
352/16:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text=tweet.extended_tweet["full_text"]

            else:
                 full_text=tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text= tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
352/17: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
352/18:
def get_latest_tweets(data_dir, auth, time_limit, topic):
    file_name = "stream"
    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),tweet_mode='extended')
    twitter_stream.filter(track= topic) # list of querries to track
    l=time.time()
    print('stream data are saved in '+str((l-tic)/60)+' minutes')
    print('reading stream file...')
    data = pd.read_json(data_dir+file_name+".json",lines=True)
    ll=time.time()
    print('reading the stream file takes '+str((ll-l)/60)+' minutes')
    #print(str(data.shape[0])+'tweets with topic\" '+str(topic)+' \"read in'++str((toc-l)/60)+' minutes')
    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
    print('getting full_text tweets...')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang=='en']
        tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)
        tweets['tweet'] = data.apply(lambda x:get_full_text_tweet(x),axis=1)
        tweets['time']=data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+' full_text tweets obtained in '+str((toc-tic)/60)+' minutes', tweets)
    return tweets, no_data
352/19: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
352/20:
#this files have not been used yet
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
352/21:
def tokenize_tweet(tweet):
    tokens = [str(word) for word in str(tweet).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    tweet_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in tweet_nostopwords if word in model.vocab];
    return(filtered_word_list);
352/22:
def tweet_preprocess(row_tweet,model):
    return clean_text(model,remove_punctuation(tokenize_tweet(row_tweet)))
352/23:
# def tweet_preprocess(row_tweet):
#     return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]
352/24:
def vectorize_tweet(normalized_tweet,model):
    vec=np.zeros((300))
    for word in normalized_tweet:
        vec+=model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
352/25:
def vectorize_latest_tweets(tweets, model):
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes')
    return tweets
352/26:
def vectorize_user_input(user_input, model):
    
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    return vectorized_input
352/27:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets #[0:topn]
352/28:
def process_user_input(user_input, time_limit, topic):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        #tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)
    return recommendations
352/29:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
352/30:
def vectorize_latest_tweets(tweets, model):
    
    print('vectorizing tweets...')
    tic=time.time()
    tweets.loc[:,'normalized']=tweets.loc[:,'tweet'].apply(lambda tweet: tweet_preprocess(tweet,model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes')
    return tweets
352/31:
def vectorize_user_input(user_input, model):
    
    print('vectorizing user_input...')
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}
    print('user_input is vectorized!')
    return vectorized_input
352/32:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
time_limit=5
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
352/33: recommendations = process_user_input(user_input, time_limit, topic)
352/34:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit=5
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
352/35: recommendations = process_user_input(user_input, time_limit, topic, topn)
352/36:
def process_user_input(user_input, time_limit, topic, topn):

    track_list=[k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'
    else:
        #tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn)
        print('top '+str(topn)+' similar tweets are obtained!')
    return recommendations
352/37: recommendations = process_user_input(user_input, time_limit, topic, topn)
352/38: recommendations.iloc[0]['tweet']
352/39: recommendations = process_user_input(user_input, time_limit, topic, topn)
352/40: recommendations.iloc[0]['tweet']
352/41: recommendations
352/42:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit=20
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
352/43: recommendations = process_user_input(user_input, time_limit, topic, topn)
352/44: recommendations.iloc[0]['tweet']
352/45: recommendations
352/46: recommendations
352/47:
with pd.option_context('display.max_colwidth', 300):
    print (recommendations)
352/48: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
352/49:
def vectorize_user_input(user_input, model):
    
    print('vectorizing user_input...')
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = np.Series({'raw_input': user_input, 'normalized': normalized, 'vector': word_vec})
    print('user_input is vectorized!')
    return vectorized_input
352/50: x=vectorize_user_input(user_input, model)
352/51:
def vectorize_user_input(user_input, model):
    
    print('vectorizing user_input...')
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = pd.Series({'raw_input': user_input, 'normalized': normalized, 'vector': word_vec})
    print('user_input is vectorized!')
    return vectorized_input
352/52: x=vectorize_user_input(user_input, model)
352/53: tweets.columns
352/54: tweets['normalized']=np.Series()
352/55: tweets['normalized']=''
352/56:
tweets['normalized']=''
tweets
352/57:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets = np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos 
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
354/1:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit = 20
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
354/2:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
354/3:
def get_latest_tweets(data_dir, auth, time_limit, topic):

    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit), tweet_mode='extended')
    twitter_stream.filter(track = topic) # list of querries to track
    l = time.time()
    print('stream data are saved in '+ str((l-tic)/60)+' minutes')
    print('reading stream file...')
    data = pd.read_json(data_dir+file_name + ".json", lines=True)
    ll = time.time()
    print('reading the stream file takes ' + str((ll-l)/60) +' minutes')
    tweets = pd.DataFrame(columns=['time', 'tweet'], index=data.index)
    print('getting full_text tweets...')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang == 'en']
        tweets=pd.DataFrame(columns = ['time','tweet'], index=data.index)
        tweets['tweet'] = data.apply(lambda x: get_full_text_tweet(x), axis = 1)
        tweets['time'] = data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset = ['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+ ' full_text tweets obtained in ' + str((toc-tic)/60) + ' minutes', tweets)
    return tweets, no_data
354/4:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text = tweet.extended_tweet["full_text"]

            else:
                 full_text = tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text = tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
354/5:
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit):
        self.start_time = time.time()
        self.limit = time_limit
        self.saveFile = open(data_dir + "stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
354/6:
#this files have not been used yet
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
354/7:
def tokenize_tweet(tweet):
    tokens = [str(word) for word in str(tweet).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    tweet_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in tweet_nostopwords if word in model.vocab];
    return(filtered_word_list);
354/8:
def tweet_preprocess(row_tweet,model):
    return clean_text(model,remove_punctuation(tokenize_tweet(row_tweet)))
354/9:
def vectorize_tweet(normalized_tweet,model):
    vec=np.zeros((300))
    for word in normalized_tweet:
        vec += model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
354/10:
def vectorize_latest_tweets(tweets, model):
    
    print('vectorizing tweets...')
    tic = time.time()
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda tweet: tweet_preprocess(tweet, model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet: vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes')
    return tweets
354/11:
def vectorize_user_input(user_input, model):
    
    print('vectorizing user_input...')
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}      
    print('user_input is vectorized!')
    return vectorized_input
354/12:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets = np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos 
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
354/13:
def process_user_input(user_input, time_limit, topic, topn):

    track_list = [k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name + '.json'):
        os.remove(data_dir + file_name + '.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return 'There is no data with topic: '+ topic +' in  '+ str(time_limit) +' seconds'
    else:
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets, topn)
        print('top '+ str(topn) + ' similar tweets are obtained!')
    return recommendations
354/14:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
354/15:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit = 20
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
354/16: recommendations = process_user_input(user_input, time_limit, topic, topn)
354/17:
def get_latest_tweets(data_dir, auth, time_limit, topic):

    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit), tweet_mode='extended')
    twitter_stream.filter(track = topic) # list of querries to track
    l = time.time()
    print('stream data are saved in '+ str((l-tic)/60)+' minutes')
    print('reading stream file...')
    data = pd.read_json(data_dir+"stream.json", lines=True)
    ll = time.time()
    print('reading the stream file takes ' + str((ll-l)/60) +' minutes')
    tweets = pd.DataFrame(columns=['time', 'tweet'], index=data.index)
    print('getting full_text tweets...')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang == 'en']
        tweets=pd.DataFrame(columns = ['time','tweet'], index=data.index)
        tweets['tweet'] = data.apply(lambda x: get_full_text_tweet(x), axis = 1)
        tweets['time'] = data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset = ['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+ ' full_text tweets obtained in ' + str((toc-tic)/60) + ' minutes', tweets)
    return tweets, no_data
354/18: recommendations = process_user_input(user_input, time_limit, topic, topn)
354/19: recommendations.iloc[0]['tweet']
354/20: recommendations
353/1: runfile('/Users/shahla/Dropbox/SharpestMinds/twitter.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
353/2: runfile('/Users/shahla/Dropbox/SharpestMinds/twitter.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
355/1:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy.streaming import StreamListener
import time
import string
from twython import Twython
import json
from sklearn import preprocessing
import gensim
import os
from nltk.corpus import stopwords


# -----------------------------------------------------------------------------
def get_latest_tweets(data_dir, auth, time_limit, topic):

    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit),
                            tweet_mode='extended')
    twitter_stream.filter(track=topic)  # list of querries to track
    t = time.time()
    print('stream data are saved in ' + str((t-tic)/60) + ' minutes')
    print('reading stream file...')
    data = pd.read_json(data_dir+"stream.json", lines=True)
    ll = time.time()
    print('reading the stream file takes ' + str((ll-t)/60) + ' minutes')
    tweets = pd.DataFrame(columns=['time', 'tweet'], index=data.index)
    print('getting full_text tweets...')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang == 'en']
        tweets = pd.DataFrame(columns=['time', 'tweet'], index=data.index)
        tweets['tweet'] = data.apply(lambda x: get_full_text_tweet(x), axis=1)
        tweets['time'] = data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset=['tweet'])
    toc = time.time()
    print(str(tweets.shape[0]) + ' full_text tweets obtained in ' +
          str((toc-tic)/60) + ' minutes', tweets)
    return tweets, no_data
# ----------------------------------------------------------------------------


def get_full_text_tweet(tweet):
    if pd.isnull(tweet.retweeted_status):
        if pd.isnull(tweet.extended_tweet):
            full_text = tweet.text
        else:
            if "full_text" in tweet.extended_tweet.keys():
                full_text = tweet.extended_tweet["full_text"]

            else:
                full_text = tweet.text
    else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text = tweet.retweeted_status['extended_tweet']
                ["full_text"]
        else:
            full_text = tweet.retweeted_status['text']
    return full_text


# -----------------------------------------------------------------------------
# MyListener() saves the data into a .json file with name stream


class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit=60):
        self.start_time = time.time()
        self.limit = time_limit
        self.saveFile = open(data_dir+"stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False

    def on_error(self, status):
        print(status)
        return True


# -----------------------------------------------------------------------------
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
# -----------------------------------------------------------------------------


def tokenize_tweet(tweet):
    tokens = [str(word) for word in str(tweet).lower().split()]
    return(tokens)


def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation))
                   for word in tokens]
    return(clean_words)


def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'))
    tweet_nostopwords = [word for word in clean_words if word not in stoplist]
    filtered_word_list = [word for word in tweet_nostopwords
                          if word in model.vocab]
    return(filtered_word_list)


# -----------------------------------------------------------------------------
def tweet_preprocess(row_tweet, model):
    return clean_text(model, remove_punctuation(tokenize_tweet(row_tweet)))
# -----------------------------------------------------------------------------


def vectorize_tweet(normalized_tweet, model):
    vec = np.zeros((300))
    for word in normalized_tweet:
        vec += model[word]
    return preprocessing.normalize(vec.reshape(1, -1))
# -----------------------------------------------------------------------------


def vectorize_latest_tweets(tweets, model):
    tic = time.time()
    print('vectorizing tweets...')
    tweets.loc[:, 'normalized'] = tweets.tweet.apply(
            lambda tweet: tweet_preprocess(tweet, model))
    tweets.loc[:, 'vector'] = tweets.normalized.apply(
            lambda tweet: vectorize_tweet(tweet, model))
    toc = time.time()
    print('tweets vectorizd in ' + str((toc-tic)/60) + ' minutes')
    return tweets
# -----------------------------------------------------------------------------


def vectorize_user_input(user_input, model):
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input,
                        'normalized': normalized, 'vector': word_vec}
    return vectorized_input
# -----------------------------------------------------------------------------


def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets = np.vstack(vectorized_tweets.vector.apply(lambda x:
                           x.tolist()))
    cos = model.cosine_similarities(vectorized_input['vector'].
                                    reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:, 'similarity_score'] = cos
    vectorized_tweets = vectorized_tweets.sort_values(
            by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
# -----------------------------------------------------------------------------


def process_user_input(user_input, time_limit, topic):
    track_list = [k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name+'.json'):
        os.remove(data_dir+file_name+'.json')
    # -----------------------------------------------
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return'There is no data with topic: ' + topic+' in  '
        + str(time_limit) + ' seconds'
    else:
        # tweets = tweets[0:1]
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        # find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input,
                                                   vectorized_tweets, topn=10)
    return recommendations
# -----------------------------------------------------------------------------
# downloaded pretrained model


model = gensim.models.KeyedVectors.load_word2vec_format(
        'GoogleNews-vectors-negative300.bin', binary=True)
# -----------------------------------------------------------------------------
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada'  # it can be a list of topics,  comman means 'or'
time_limit = 20
user_input = 'tariff'
with open("twitter_credentials.json", "r") as file:
    credentials = json.load(file)
# Instantiate an object
python_tweets = Twython(credentials['CONSUMER_KEY'],
                        credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'],
                           credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'],
                      credentials['ACCESS_SECRET'])
#  ----------------------------------------------------------------------------
recommendations = process_user_input(user_input, time_limit, topic)
354/21: recommendations = process_user_input(user_input, time_limit, topic, topn)
355/2: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
355/3:
track_list = [k for k in topic.split(',')]
tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
355/4:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy.streaming import StreamListener
import time
import string
from twython import Twython
import json
from sklearn import preprocessing
import gensim
import os
from nltk.corpus import stopwords


def get_latest_tweets(data_dir, auth, time_limit, topic):

    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit), tweet_mode='extended')
    twitter_stream.filter(track = topic) # list of querries to track
    t = time.time()
    print('stream data are saved in '+ str((t-tic)/60)+' minutes')
    print('reading stream file...')
    data = pd.read_json(data_dir+"stream.json", lines=True)
    ll = time.time()
    print('reading the stream file takes ' + str((ll-t)/60) +' minutes')
    tweets = pd.DataFrame(columns=['time', 'tweet'], index=data.index)
    print('getting full_text tweets...')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang == 'en']
        tweets=pd.DataFrame(columns = ['time','tweet'], index=data.index)
        tweets['tweet'] = data.apply(lambda x: get_full_text_tweet(x), axis = 1)
        tweets['time'] = data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset = ['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+ ' full_text tweets obtained in ' + str((toc-tic)/60) + ' minutes', tweets)
    return tweets, no_data
# ---------------------------------------------
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text = tweet.extended_tweet["full_text"]

            else:
                 full_text = tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text = tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text 
#--------------------------------------------
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit):
        self.start_time = time.time()
        self.limit = time_limit
        self.saveFile = open(data_dir + "stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
# ----------------------------------------
def tokenize_tweet(tweet):
    tokens = [str(word) for word in str(tweet).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens]
    return(clean_words)

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'))
    tweet_nostopwords = [word for word in clean_words if word not in stoplist]
    filtered_word_list = [word for word in tweet_nostopwords if word in model.vocab]
    return(filtered_word_list)
# ----------------------------------------------------
def tweet_preprocess(row_tweet,model):
    return clean_text(model,remove_punctuation(tokenize_tweet(row_tweet)))
# ------------------------------------------------------


def vectorize_tweet(normalized_tweet,model):
    vec=np.zeros((300))
    for word in normalized_tweet:
        vec += model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
# -----------------------------------------------------


def vectorize_latest_tweets(tweets, model):
    
    print('vectorizing tweets...')
    tic = time.time()
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda tweet: tweet_preprocess(tweet, model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet: vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes')
    return tweets

# ---------------------------------------------------

def vectorize_user_input(user_input, model):
    
    print('vectorizing user_input...')
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}      
    print('user_input is vectorized!')
    return vectorized_input
# ---------------------------------------------------

def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets = np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos 
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
# --------------------------------------------------------

def process_user_input(user_input, time_limit, topic, topn):

    track_list = [k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name + '.json'):
        os.remove(data_dir + file_name + '.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return 'There is no data with topic: '+ topic +' in  '+ str(time_limit) +' seconds'
    else:
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets, topn)
        print('top '+ str(topn) + ' similar tweets are obtained!')
    return recommendations
# ---------------------------------------------------


#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)

# -----------------------------------------------------

data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit = 20
user_input = 'tariff'
with open(data_dir + "twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

# --------------------------------------------------

recommendations = process_user_input(user_input, time_limit, topic, topn)
355/5:
with open("/Users/shahla/Dropbox/SharpestMinds/twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])

# --------------------------------------------------

recommendations = process_user_input(user_input, time_limit, topic, topn)
353/3: runfile('/Users/shahla/Dropbox/SharpestMinds/twitter.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
353/4: runfile('/Users/shahla/Dropbox/SharpestMinds/twitter.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
353/5: runfile('/Users/shahla/Dropbox/SharpestMinds/twitter.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
358/1:
import pandas as pd
import numpy as np
358/2: A=np.array([[1,2,3],[1.1,3.1,3.2]])
358/3:
A=np.array([[1,2,3],[1.1,3.1,3.2]])
A
358/4: c=A.sum(axis=0)
358/5: type(c)
358/6: c
358/7: c.shape
358/8: c
358/9: d=A.sum(axis=1)
358/10: D
358/11: d
358/12: d.shape
358/13: A/d
358/14: A/c
358/15: 100*A/c
358/16:
x=np.array([1,2,3])
y=np.array([[4],[5]])
358/17: x
358/18: y
358/19: x+y
358/20: np.random.randn(5)
358/21: np.random.randn(5).shape
358/22: assert(x.shape==(1,5))
358/23: assert(x.shape==(5,))
358/24: x.shape
358/25: assert(x.shape==(3,))
358/26: x=np.random.randn((1,2))
358/27: x=np.random.randn(1,2)
358/28: x=np.random.randn(3,2,3)
358/29: x
358/30: ?np.flatten
358/31: ?np.flatten()
358/32: ?x.flatten()
358/33: x.flatten()
358/34: x.flatten().dim
358/35: x.flatten().shape
358/36: x.flatten('C').shape
358/37: x.flatten('C')
358/38: x.flatten('F')
358/39: x.flatten()
358/40: x.flatten('F')
358/41: x=np.random.randn(3,2)
358/42: x
358/43: x.flatten('F')
358/44: x.flatten()
358/45: x.shape[0]
358/46: x.reshape(x.shape[0],-1)
358/47: x.shape
358/48: x.reshape(x.shape[0],-1).shape
358/49: x.reshape(x.shape[0],-1)
358/50: x=np.random.randn(3,2,4)
358/51: x
358/52: x.flatten('F')
358/53: x.shape
358/54: x.reshape(x.shape[0],-1)
358/55: x.reshape(x.shape[0],-1).shape
358/56: x.reshape(x.shape[0],-1)
358/57: x%2
358/58:
x = np.random.randn(1,3,3,1,4,1,2)
x
358/59: x.squeesz
358/60: x.squeeze
358/61: y=x.squeeze
358/62: y
358/63: y=x.squeeze()
358/64: y
358/65: y.shape
358/66:
x = np.random.randn(1,3,1,2)
x
358/67: y=x.squeeze()
358/68: y.shape
358/69: y
358/70: np.squeeze(x)
358/71:
x = np.random.randn(1,3,1)
x
358/72:
x = np.random.randn(1,3,1)
x
358/73: y=x.squeeze()
358/74: y.shape
358/75: y
358/76: x=np.array([[[1,2,3]]])
358/77: x
358/78: x.shape
358/79: x.squeeze()
358/80: x.squeeze().shape
358/81: df=pd.DataFrame(x)
358/82:
x = np.random.randn(1,3,2)
x
358/83: y=x.squeeze()
358/84: y.shape
358/85: y
358/86: np.squeeze(x)
358/87: x=np.array([[[1,2,3]]])
358/88: x
358/89: x.shape
358/90:
x = np.random.randn(1,3,2)
x
358/91: df=pd.DataFrame(x)
358/92: df=pd.DataFrame(x)
358/93:
x = np.random.randn(3,2)
x
358/94: df=pd.DataFrame(x)
358/95: df
358/96: df=pd.DataFrame(x,columns=list('ab'))
358/97: df
358/98: df[0]
358/99: df['a']
358/100: type(df['a'])
358/101: df['a'].shape
358/102: model={}
358/103: type(model)
358/104: model[1]='s'
358/105: model
359/1:
# Package imports
import numpy as np
import matplotlib.pyplot as plt
from testCases_v2 import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

%matplotlib inline

np.random.seed(1) # set a seed so that the results are consistent
360/1:
import pandas as pd
import numpy as np
360/2:
hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]
for i, n_h in enumerate(hidden_layer_sizes):
    print(i,n_h)
360/3: enumerate(hidden_layer_sizes)
360/4:
hidden_layer_sizes = ['cat','dog','monkey']
for i, n_h in enumerate(hidden_layer_sizes):
    print(i,n_h)
360/5:
hidden_layer_sizes = ['cat','dog','monkey']
for i, n_h in enumerate(hidden_layer_sizes):
    print(i,n_h,'\n')
360/6:
hidden_layer_sizes = ['cat','dog','monkey']
for i, n_h in enumerate(hidden_layer_sizes):
    print(i,n_h)
361/1:
import numpy as np
import h5py
import matplotlib.pyplot as plt
from testCases_v4 import *
from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward

%matplotlib inline
plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

%load_ext autoreload
%autoreload 2

np.random.seed(1)
361/2:
x=(1,2,3)
type(x)
361/3:
x=(1,2,3,4)
type(x)
361/4:
x=(1,2,3,4)
x[0]
361/5: l=[1,23,1]
361/6:
l=[1,23,1]
l
361/7:
l=[1,23,1]
l[0]
361/8:
x=(1,2,3,4)
x[0]=0
361/9:
l=[1,23,1]
l[0]=0
361/10: 4//2
361/11: 4//3
361/12: 4//7
361/13: 9//7
361/14: 9/2
361/15: 9.1/2.1
361/16: 9.1//2.1
361/17:
l=[1,2,3]
l.append(3)
l
361/18:
t=(1,2,3,3)
t
361/19:
t=(1,2,3,3)
t.count
361/20:
t=(1,2,3,3)
t[0]
361/21:
t=(1,2,3,3)
t(0)
361/22: l=[1,2,'r']
361/23:
l=[1,2,'r']
l
361/24: x=np.array([[1,2]])
361/25:
x=np.array([[1,2]])
x.shape
361/26:
x=np.array([[1,2]])
x
361/27:
x=np.array([[1,2]])
x.squeeze()
361/28:
x=np.array([[1,2]])
x=x.squeeze()
x.shae
361/29:
x=np.array([[1,2]])
x=x.squeeze()
x.shape
361/30: x=np.array([[1,2]])
361/31:
x=np.array([[1,2]])
x.shape
361/32: x=x.squeeze
361/33:
x=x.squeeze()
x.shape
361/34:
x=x.squeeze()
x.shape()
361/35:
x=x.squeeze
x.shape()
361/36:
x=np.array([[1,2]])
x.shape
361/37:
x=x.squeeze
x.shape()
361/38:
x=np.array([[1,2]])
x.shape
361/39:
x=np.squeeze(x)
x.shape()
361/40:
x=np.array([[1,2]])
x.shape
361/41: x=np.squeeze(x)
361/42:
x=np.array([[1,2]])
x.shape
361/43: x
361/44:
x=np.squeeze(x)
x
361/45:
x=np.squeeze(x)
x.shape
361/46:
x=np.array([[1,2]])
x.shape
361/47: x
361/48:
x=np.squeeze(x)
x.shape
361/49: x
361/50:
x=np.array([[(1,2)]])
x.shape
361/51: x
361/52:
x=np.squeeze(x)
x.shape
361/53: x
361/54: x.shape==()
361/55:
x=np.array([[(1,2)]])
x.shape
361/56: x
361/57:
x=np.squeeze(x)
x.shape
361/58: x.shape==()
361/59:
x=np.array([[1,2]])
x.shape
361/60: x
361/61:
x=np.squeeze(x)
x.shape
361/62: x.shape==()
361/63:
x=np.array([[1]])
x.shape
361/64: x
361/65:
x=np.squeeze(x)
x.shape
361/66: x.shape==()
361/67:
for l in reversed(range(10)):
    print(l)
361/68:
for l in reversed('jshdjk'):
    print(l)
361/69:
for l in (range(10)):
    print(l)
361/70:
for l in (range(-10)):
    print(l)
361/71:
for l in (range(-10,0,1)):
    print(l)
361/72:
for l in (range(10,0,-1)):
    print(l)
363/1:
import numpy as np
import pandas as pd
363/2: ?pd.array
363/3: ?pd.concat
363/4: A=pd.array([1,2,3])
363/5: A
363/6: A=pd.array([[1,2,3]])
363/7: A=pd.array([1,2,3],[1,2,33])
363/8: A=np.array([1,2,3],[1,2,33])
363/9: A=pd.array([1,2,3])
363/10: A.shape
363/11: A=pd.array([[1,2,3]])
363/12: A=np.array([1,2,3])
363/13: A=np.array([[1,2,3]])
363/14: A=np.array([[1,2,3].['4'.'s','y']])
363/15: A=np.array([[1,2,3].['4','s','y']])
363/16: A=np.array([[1,2,3],['4','s','y']])
363/17: A.shape
363/18: A.dtype
363/19: ?np.array
363/20: A.astype(int)
363/21: A=np.array([[1,2,3]])
363/22: A.dtype
363/23: ?np.array
363/24: A.astype(int)
363/25: arange(3)
363/26: np.arange(3)
363/27: A
363/28: A.T
363/29: A.T.shape
363/30: z=np.zeros(m)
363/31: z=np.zeros(10)
363/32: z=np.zeros(3)
363/33: z
363/34: z.shape
363/35: z.reshape(3,1)
363/36: z.reshape(1,3)
363/37: range(3)
363/38:
for i in range(3)
print(i)
363/39:
for i in range(3):
   print(i)
363/40: np.arange(3)
363/41: np.arange(3)
363/42: np.arange(3).shape
364/1:
import pandas as pd
import numpy as np
364/2: x=np.random.randn(3,5)
364/3: x=np.random.randn(3,5)x
364/4:
x=np.random.randn(3,5)
x
364/5:
x=np.random.randn(3,5)<01
x
364/6:
x=np.random.randn(3,5)<1
x
364/7:
x=np.random.randn(3,5)<.3
x
365/1:
import numpy as np
import pandas as pd
365/2:
name = input("Enter a name: ")
print(name)
365/3:
l=[2]
len(l)
367/1:
#sources
#https://www.kaggle.com/manish2104/predictive-model-90-accuracy-titanic
import numpy as np
import pandas as pd
import nltk
import plotly
import re
        
import os
plotly.offline.init_notebook_mode() # run at the start of every notebook
#import cufflinks as cf

#cf.go_offline()
#cf.getThemes()
from plotly import __version__
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)
import plotly.graph_objs as go
%matplotlib inline
import random
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")
from IPython.display import display

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
367/2:
#sources
#https://www.kaggle.com/manish2104/predictive-model-90-accuracy-titanic
import numpy as np
import pandas as pd
import nltk
#import plotly
import re
        
import os
plotly.offline.init_notebook_mode() # run at the start of every notebook
#import cufflinks as cf

#cf.go_offline()
#cf.getThemes()
from plotly import __version__
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)
import plotly.graph_objs as go
%matplotlib inline
import random
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")
from IPython.display import display

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
367/3:
#sources
#https://www.kaggle.com/manish2104/predictive-model-90-accuracy-titanic
import numpy as np
import pandas as pd
import nltk
import plotly
import re
        
import os
plotly.offline.init_notebook_mode() # run at the start of every notebook
#import cufflinks as cf

#cf.go_offline()
#cf.getThemes()
#from plotly import __version__
#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)
import plotly.graph_objs as go
%matplotlib inline
import random
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")
from IPython.display import display

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
367/4:
#sources
#https://www.kaggle.com/manish2104/predictive-model-90-accuracy-titanic
import numpy as np
import pandas as pd
import nltk
import plotly
import re
        
import os
plotly.offline.init_notebook_mode() # run at the start of every notebook
#import cufflinks as cf

#cf.go_offline()
#cf.getThemes()
#from plotly import __version__
#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)
#import plotly.graph_objs as go
%matplotlib inline
import random
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")
from IPython.display import display

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
367/5:
#sources
#https://www.kaggle.com/manish2104/predictive-model-90-accuracy-titanic
import numpy as np
import pandas as pd
import nltk
import plotly
import re
        
import os
#plotly.offline.init_notebook_mode() # run at the start of every notebook
#import cufflinks as cf

#cf.go_offline()
#cf.getThemes()
#from plotly import __version__
#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)
#import plotly.graph_objs as go
%matplotlib inline
import random
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")
from IPython.display import display

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
367/6:
#sources
#https://www.kaggle.com/manish2104/predictive-model-90-accuracy-titanic
import numpy as np
import pandas as pd
import nltk
#import plotly
import re
        
import os
#plotly.offline.init_notebook_mode() # run at the start of every notebook
#import cufflinks as cf

#cf.go_offline()
#cf.getThemes()
#from plotly import __version__
#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)
#import plotly.graph_objs as go
%matplotlib inline
import random
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")
from IPython.display import display

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
367/7:
#sources
#https://www.kaggle.com/manish2104/predictive-model-90-accuracy-titanic
import numpy as np
import pandas as pd
import nltk
#import plotly
import re
        
import os
#plotly.offline.init_notebook_mode() # run at the start of every notebook
#import cufflinks as cf

#cf.go_offline()
#cf.getThemes()
#from plotly import __version__
#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
#init_notebook_mode(connected=True)
#import plotly.graph_objs as go
%matplotlib inline
import random
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")
from IPython.display import display

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
367/8:
from sklearn.model_selection import cross_val_score
rf = RandomForestClassifier(n_estimators=100)
scores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = "accuracy")
368/1:
%matplotlib inline

import itertools
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

from sklearn import datasets

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import cross_val_score, train_test_split

from mlxtend.plotting import plot_learning_curves
from mlxtend.plotting import plot_decision_regions

np.random.seed(0)
368/2:
%matplotlib inline

import itertools
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

from sklearn import datasets

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import cross_val_score, train_test_split

#from mlxtend.plotting import plot_learning_curves
#from mlxtend.plotting import plot_decision_regions

np.random.seed(0)
368/3:
iris = datasets.load_iris()
X, y = iris.data[:, 0:2], iris.target
    
clf1 = DecisionTreeClassifier(criterion='entropy', max_depth=1)
clf2 = KNeighborsClassifier(n_neighbors=1)    

bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=0.8, max_features=0.8)
bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.8, max_features=0.8)
368/4:
label = ['Decision Tree', 'K-NN', 'Bagging Tree', 'Bagging K-NN']
clf_list = [clf1, clf2, bagging1, bagging2]

fig = plt.figure(figsize=(10, 8))
gs = gridspec.GridSpec(2, 2)
grid = itertools.product([0,1],repeat=2)

for clf, label, grd in zip(clf_list, label, grid):        
    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')
    print ("Accuracy: %.2f (+/- %.2f) [%s]" %(scores.mean(), scores.std(), label))
        
    clf.fit(X, y)
    ax = plt.subplot(gs[grd[0], grd[1]])
    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)
    plt.title(label)

plt.show()
368/5:
%matplotlib inline

import itertools
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

from sklearn import datasets

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import cross_val_score, train_test_split

#from mlxtend.plotting import plot_learning_curves
from mlxtend.plotting import plot_decision_regions

np.random.seed(0)
368/6:
iris = datasets.load_iris()
X, y = iris.data[:, 0:2], iris.target
    
clf1 = DecisionTreeClassifier(criterion='entropy', max_depth=1)
clf2 = KNeighborsClassifier(n_neighbors=1)    

bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=0.8, max_features=0.8)
bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.8, max_features=0.8)
368/7:
#plot learning curves
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
plt.figure()
plot_learning_curves(X_train, y_train, X_test, y_test, bagging1, print_model=False, style='ggplot')
plt.show()
369/1:
import pandas as pd
import numpy as np
369/2:
a=[1,2,3,10]
n=4
369/3: A=np.array(a)
369/4:
A=np.array(a)
A
369/5:
A=np.array(a)
A.shape
369/6:
A=np.array(a).reshape(n,1)
A.shape
369/7:
A=np.array(a).reshape(n,1)
A.shape
A.columns
369/8:
A=np.array(a).reshape(n,1)
A.shape
A[0]
369/9:
A=np.array(a).reshape(n,1)
A.shape
A=PandasDataFrame(A)
369/10:
A=np.array(a).reshape(n,1)
A.shape
B=pd.DataFrame(A)
369/11:
A=np.array(a).reshape(n,1)
A.shape
B=pd.DataFrame(a)
369/12: B
369/13: B.shape
369/14:
A=np.array(a).reshape(n,1)
A.shape
B=pd.DataFrame(a,columns=['a'])
369/15: B.shape
369/16: B
369/17:
# For a given array arr[],  
# returns the maximum j - i 
# such that arr[j] > arr[i] 
def maxIndexDiff(arr, n): 
    maxDiff = 0; 
    LMin = [0] * n 
    RMax = [0] * n 
  
    # Construct LMin[] such that  
    # LMin[i] stores the minimum  
    # value from (arr[0], arr[1],  
    # ... arr[i])  
    LMin[0] = arr[0] 
    for i in range(1, n): 
        LMin[i] = min(arr[i], LMin[i - 1]) 
  
    # Construct RMax[] such that  
    # RMax[j] stores the maximum  
    # value from (arr[j], arr[j+1], 
    # ..arr[n-1])  
    RMax[n - 1] = arr[n - 1] 
    for j in range(n - 2, -1, -1): 
        RMax[j] = max(arr[j], RMax[j + 1]); 
  
    # Traverse both arrays from left 
    # to right to find optimum j - i 
    # This process
369/18:  LMin = [0] * n
369/19: LMIN
369/20: LMin
369/21: LMin[0] = arr[0]
369/22: LMin[0] = a[0]
369/23: maxDifference(n, a)
369/24:
def maxDifference(n, a):
    maxDiff = 0; 
    l = [0] * n 
    r = [0] * n 
    l[0] = a[0]
    for i in range(1, n): 
        l[i] = min(a[i], l[i - 1]) 
        RMax[n - 1] = arr[n - 1] 
    for j in range(n - 2, -1, -1)
        r[j] = max(a[j], r[j + 1])
    maxDiff = -1
    while (j < n and i < n): 
        if (l[i] < r[j]): 
            maxDiff = max(maxDiff, j - i) 
            j = j + 1
        else: 
            i = i+1
  
    return maxDiff
369/25: maxDifference(n, a)
369/26:
def maxDifference(n, a):
    maxDiff = 0; 
    l = [0] * n 
    r = [0] * n 
    l[0] = a[0]
    for i in range(1, n): 
        l[i] = min(a[i], l[i - 1]) 
        RMax[n - 1] = arr[n - 1] 
    for j in range(n - 2, -1, -1):
        r[j] = max(a[j], r[j + 1])
    maxDiff = -1
    while (j < n and i < n): 
        if (l[i] < r[j]): 
            maxDiff = max(maxDiff, j - i) 
            j = j + 1
        else: 
            i = i+1
  
    return maxDiff
369/27: maxDifference(n, a)
369/28:
def maxDifference(n, a):
    maxDiff = 0; 
    l = [0] * n 
    r = [0] * n 
    l[0] = a[0]
    for i in range(1, n): 
        l[i] = min(a[i], l[i - 1]) 
        RMax[n - 1] = a[n - 1] 
    for j in range(n - 2, -1, -1):
        r[j] = max(a[j], r[j + 1])
    maxDiff = -1
    while (j < n and i < n): 
        if (l[i] < r[j]): 
            maxDiff = max(maxDiff, j - i) 
            j = j + 1
        else: 
            i = i+1
  
    return maxDiff
369/29: maxDifference(n, a)
369/30:
def maxDifference(n, a):
    maxDiff = 0; 
    l = [0] * n 
    r = [0] * n 
    l[0] = a[0]
    for i in range(1, n): 
        l[i] = min(a[i], l[i - 1]) 
        r[n - 1] = a[n - 1] 
    for j in range(n - 2, -1, -1):
        r[j] = max(a[j], r[j + 1])
    maxDiff = -1
    while (j < n and i < n): 
        if (l[i] < r[j]): 
            maxDiff = max(maxDiff, j - i) 
            j = j + 1
        else: 
            i = i+1
  
    return maxDiff
369/31: maxDifference(n, a)
369/32: a
369/33:
def maxDifference(n, a):
    maxDiff = 0; 
    l = [0] * n 
    r = [0] * n 
    l[0] = a[0]
    for i in range(1, n): 
        l[i] = min(a[i], l[i - 1]) 
    r[n - 1] = a[n - 1] 
    for j in range(n - 2, -1, -1):
        r[j] = max(a[j], r[j + 1])
    maxDiff = -1
    while (j < n and i < n): 
        if (l[i] < r[j]): 
            maxDiff = max(maxDiff, j - i) 
            j = j + 1
        else: 
            i = i+1
  
    return maxDiff
369/34: maxDifference(n, a)
369/35:
def maxDifference(n, a):
    maxDiff = 0; 
    l = [0] * n 
    r = [0] * n 
    l[0] = a[0]
    for i in range(1, n): 
        l[i] = min(a[i], l[i - 1]) 
    r[n - 1] = a[n - 1] 
    for j in range(n - 2, -1, -1):
        r[j] = max(a[j], r[j + 1])
    maxDiff = -1
    while (j < n and i < n): 
        if (l[i] < r[j]): 
            maxDiff = max(maxDiff, j - i) 
            j = j + 1
        else: 
            i = i+1
  
    return maxDiff
369/36: maxDifference(n, a)
369/37: B[0]
369/38:
A=np.array(a).reshape(n,1)
A[0]
369/39: B.loc[:,'diff']=0
369/40:
def maxDiff(a):
    vmin = a[0]
    dmax = 0
    for i in range(len(a)):
        if (a[i] < vmin):
            vmin = a[i]
        elif (a[i] - vmin > dmax):
            dmax = a[i] - vmin
    return dmax
369/41: maxDiff(a)
369/42:
a=[1,2,1,2,10]
n=4
369/43: maxDiff(a)
369/44:
a=[2,3,2,10]
n=4
369/45: maxDiff(a)
369/46:
a=[2,2]
n=4
369/47: maxDiff(a)
369/48:
def maxDiff(a):
    vmin = a[0]
    dmax = -1
    for i in range(len(a)):
        if (a[i] < vmin):
            vmin = a[i]
        elif (a[i] - vmin > dmax):
            dmax = a[i] - vmin
    return dmax
369/49: maxDiff(a)
369/50:
def maxDiff(a):
    vmin = a[0]
    dmax = 0
    for i in range(len(a)):
        if (a[i] < vmin):
            vmin = a[i]
        elif (a[i] - vmin > dmax):
            dmax = a[i] - vmin
    return dmax
369/51: maxDiff(a)
369/52:
a=[2,2]
n=4
369/53:
def maxDiff(a):
    vmin = a[0]
    dmax = 0
    for i in range(len(a)):
        if (a[i] < vmin):
            vmin = a[i]
        elif (a[i] - vmin > dmax):
            dmax = a[i] - vmin
    return dmax
369/54: maxDiff(a)
369/55:
def maxDiff(a):
    vmin = a[0]
    dmax = -1
    for i in range(len(a)):
        if (a[i] < vmin):
            vmin = a[i]
        elif (a[i] - vmin > dmax):
            dmax = a[i] - vmin
    return dmax
369/56: maxDiff(a)
369/57:
def maxDiff(a):
    vmin = a[0]
    dmax = -1
    for i in range(len(a)):
        if (a[i] < vmin):
            vmin = a[i]
        elif (a[i] - vmin > dmax):
            dmax = a[i] - vmin
            print(dmax)
    return dmax
369/58: maxDiff(a)
369/59:
def maxDiff(a):
    vmin = a[0]
    dmax = -1
    for i in range(len(a)):
        if (a[i] < vmin):
            vmin = a[i]
        elif (a[i] - vmin > (dmax+1):
            dmax = a[i] - vmin
            print(dmax)
    return dmax
369/60:
def maxDiff(a):
    vmin = a[0]
    dmax = -1
    for i in range(len(a)):
        if (a[i] < vmin):
            vmin = a[i]
        elif (a[i] - vmin > (dmax+1)):
            dmax = a[i] - vmin
            print(dmax)
    return dmax
369/61: maxDiff(a)
369/62:
a=[2,3]
n=4
369/63: maxDiff(a)
369/64: len(a)
369/65:
def maxDiff(a,n):
    vmin = a[0]
    dmax = -1
    for i in range(n):
        if (a[i] < vmin):
            vmin = a[i]
        elif (a[i] - vmin > (dmax+1)):
            dmax = a[i] - vmin
            print(dmax)
    return dmax
369/66: maxDiff(a,2)
369/67: range(2)
369/68: range(1,2)
369/69:
for i in range(n):
    print(i)
369/70: n
369/71:
for i in range(1,n):
    print(i)
369/72: pd.DataFrame(columns=['d','dd'])
369/73: df=pd.DataFrame(columns=['d','dd'])
369/74:
df=pd.DataFrame(columns=['d','dd'])
df
369/75:

A
369/76:

B
369/77: np.df['a']
369/78: np.diff['a']
369/79: np.diff[A.a]
369/80: np.diff[B.a]
369/81: B.a
369/82: B.a.diff
369/83: B.a
369/84: type(B.a)
369/85: B.a.diff()
369/86: B['c']= B.a.diff()>0
369/87:
B['c']= B.a.diff()>0
B
369/88:
B['c']= B.a.diff()>0
B['d']=B[B.a.diff()>0]
369/89:
B['c']= B.a.diff()>0
B['d']=B.a[B.a.diff()>0]
369/90:
B['c']= B.a.diff()>0
B['d']=B.a[B.a.diff()>0]
B
369/91:
B['c']= B.a.diff()>0
B['d']=B.loc[:,'a'][B.a.diff()>0]
B
369/92:
B['c']= B.a.diff()>0
B['d']=B.loc[:,'a'][B.a.diff()>0]*2
B
369/93:
B['c']= B.a.diff()>0
B['d']=B.loc[:,'a'][B.a.diff()>0]*B.a
B
369/94: B.a.rolling(2).sum()
370/1:
import math
import os
import random
import re
import sys
for i in range(1,n+1):
    print(str(n)+'X'+str(i)+'=', i*n,'\n')
370/2:
import math
import os
import random
import re
import sys
n=int(input())
for i in range(1,n+1):
    print(str(n)+'X'+str(i)+'=', i*n,'\n')
370/3:
import math
import os
import random
import re
import sys
n=int(input())
for i in range(1,n+1):
    print(str(n)+'X'+str(i)+'=', i*n,'\n')
370/4:
#!/bin/python3

import math
import os
import random
import re
import sys
for i in range(1,11):
    print(str(n)+'X'+str(i)+'=', i*n,'\n')


if __name__ == '__main__':
    n = int(input())
370/5: df=pd.DataFrame({'A':np.arrange(10)})
370/6:
import pandas as pd
import Numy as np
df=pd.DataFrame({'A':np.arrange(10)})
370/7:
import pandas as pd
import Numpy as np
df=pd.DataFrame({'A':np.arrange(10)})
370/8:
import pandas as pd
import numpy as np
df=pd.DataFrame({'A':np.arrange(10)})
370/9:
import pandas as pd
import numpy as np
df=pd.DataFrame({'A':pd.arrange(10)})
370/10:
import pandas as pd
import numpy as np
df=pd.DataFrame({'A':pd.arange(10)})
370/11:
import pandas as pd
import numpy as np
df=pd.DataFrame({'A':n.arange(10)})
370/12:
import pandas as pd
import numpy as np
df=pd.DataFrame({'A':np.arange(10)})
370/13: df
370/14:  df.expanding(2).sum()
370/15:  df.expanding(2)
370/16:  df.expanding(2).sum()
370/17:  df.expanding(2).len()
370/18:  df.expanding(2).sum()
370/19:  df.expanding(3).sum()
370/20:  df.expanding(3,center=True).sum()
370/21:  df.expanding(3).sum()
372/1:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text = tweet.extended_tweet["full_text"]

            else:
                 full_text = tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text = tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
372/2:
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit):
        self.start_time = time.time()
        self.limit = time_limit
        self.saveFile = open(data_dir + "stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
372/3:
from tweepy import Stream
import pandas as pd
import numpy as np
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import time
import argparse
import string
from twython import Twython  
import json
from sklearn import preprocessing
import gensim 
import re, string
import nltk
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk import re
import os
import sys
372/4:
def get_latest_tweets(data_dir, auth, time_limit, topic):

    print("streaming tweets...")
    tic = time.time()
    twitter_stream = Stream(auth, MyListener(data_dir, time_limit), tweet_mode='extended')
    twitter_stream.filter(track = topic) # list of querries to track
    t = time.time()
    print('stream data are saved in '+ str((t-tic)/60)+' minutes')
    print('reading stream file...')
    data = pd.read_json(data_dir+"stream.json", lines=True)
    ll = time.time()
    print('reading the stream file takes ' + str((ll-t)/60) +' minutes')
    tweets = pd.DataFrame(columns=['time', 'tweet'], index=data.index)
    print('getting full_text tweets...')
    no_data = False
    if data.empty:
        no_data = True
    else:
        data = data[data.lang == 'en']
        tweets=pd.DataFrame(columns = ['time','tweet'], index=data.index)
        tweets['tweet'] = data.apply(lambda x: get_full_text_tweet(x), axis = 1)
        tweets['time'] = data.created_at
        tweets = tweets.sort_values('time', ascending=False)
        tweets = tweets.drop_duplicates()
        tweets = tweets.dropna(subset = ['tweet'])
    toc = time.time()
    print(str(tweets.shape[0])+ ' full_text tweets obtained in ' + str((toc-tic)/60) + ' minutes', tweets)
    return tweets, no_data
372/5:
def get_full_text_tweet(tweet):
   if pd.isnull(tweet.retweeted_status):
         if pd.isnull(tweet.extended_tweet):
                full_text = tweet.text
         else:   
            if "full_text" in tweet.extended_tweet.keys():
                 full_text = tweet.extended_tweet["full_text"]

            else:
                 full_text = tweet.text 
   else:
        if 'extended_tweet' in tweet.retweeted_status.keys():
            if "full_text" in tweet.retweeted_status['extended_tweet'].keys():
                full_text = tweet.retweeted_status['extended_tweet']["full_text"]
        else:
             full_text = tweet.retweeted_status['text']    
   return full_text
372/6:
#MyListener() saves the data into a .json file with name stream
class MyListener(StreamListener):
    """Custom StreamListener for streaming data."""

    def __init__(self, data_dir, time_limit):
        self.start_time = time.time()
        self.limit = time_limit
        self.saveFile = open(data_dir + "stream.json", 'a')
        super(MyListener, self).__init__()

    def on_data(self, data):
        if (time.time() - self.start_time) < self.limit:
            self.saveFile.write(data)
            return True
        else:
            self.saveFile.close()
            return False
            

    def on_error(self, status):
        print(status)
        return True
372/7:
#this files have not been used yet
def format_filename(fname):
    """Convert file name into a safe string.
    Arguments:
        fname -- the file name to convert
    Return:
        String -- converted file name
    """
    return ''.join(convert_valid(one_char) for one_char in fname)


def convert_valid(one_char):
    """Convert a character into '_' if invalid.
    Arguments:
        one_char -- the char to convert
    Return:
        Character -- converted char
    """
    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits)
    if one_char in valid_chars:
        return one_char
    else:
        return '_'
372/8:
def tokenize_tweet(tweet):
    tokens = [str(word) for word in str(tweet).lower().split()];
    return(tokens);

def remove_punctuation(tokens):
    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];
    return(clean_words);

def clean_text(model, clean_words):
    stoplist = set(stopwords.words('english'));
    tweet_nostopwords = [word for word in clean_words if word not in stoplist];
    filtered_word_list = [word for word in tweet_nostopwords if word in model.vocab];
    return(filtered_word_list);
372/9:
def tweet_preprocess(row_tweet,model):
    return clean_text(model,remove_punctuation(tokenize_tweet(row_tweet)))
372/10:
def vectorize_tweet(normalized_tweet,model):
    vec=np.zeros((300))
    for word in normalized_tweet:
        vec += model[word] 
    return preprocessing.normalize(vec.reshape(1,-1))
372/11:
def vectorize_latest_tweets(tweets, model):
    
    print('vectorizing tweets...')
    tic = time.time()
    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda tweet: tweet_preprocess(tweet, model))
    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet: vectorize_tweet(tweet, model))
    toc=time.time()
    print('tweets vectorizd in '+ str((toc-tic)/60)+' minutes')
    return tweets
372/12:
def vectorize_user_input(user_input, model):
    
    print('vectorizing user_input...')
    normalized = tweet_preprocess(user_input, model)
    word_vec = vectorize_tweet(normalized, model)
    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}      
    print('user_input is vectorized!')
    return vectorized_input
372/13:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):
    vec_tweets = np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos 
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
372/14:
def process_user_input(user_input, time_limit, topic, topn):

    track_list = [k for k in topic.split(',')]
    file_name = "stream"
    if os.path.exists(data_dir+file_name + '.json'):
        os.remove(data_dir + file_name + '.json')
    #-----------------------------------------------
    # Load credentials from json file
    tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)
    if no_data:
        return 'There is no data with topic: '+ topic +' in  '+ str(time_limit) +' seconds'
    else:
        vectorized_tweets = vectorize_latest_tweets(tweets, model)
        vectorized_user_input = vectorize_user_input(user_input, model)
        #find the top topn= 10  similar tweets
        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets, topn)
        print('top '+ str(topn) + ' similar tweets are obtained!')
    return recommendations
372/15:
#downloaded pretrained model
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
372/16:
#data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
data_dir = '../stream/data/'
372/17:

topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit = 20
user_input = 'tariff'
with open(data_dir + "twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
372/18:
#data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
data_dir = '../stream/data/'
print(data_dir)
372/19:
#data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
data_dir = '../stream/data/'
from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir(data_dir) if isfile(join(data_dir, f))]
372/20:
#data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
data_dir = '../data/'
from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir(data_dir) if isfile(join(data_dir, f))]
372/21:
#data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
data_dir = '.../data/'
from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir(data_dir) if isfile(join(data_dir, f))]
372/22:
#data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
data_dir = './stream/data/'
from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir(data_dir) if isfile(join(data_dir, f))]
372/23:
#data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
data_dir = './stream/data/'
from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir(data_dir) if isfile(join(data_dir, f))]
onlyfiles
372/24:

topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit = 20
user_input = 'tariff'
with open(data_dir + "twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
372/25:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/''
topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit = 20
user_input = 'tariff'
with open(data_dir + "twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
372/26:
data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit = 20
user_input = 'tariff'
with open(data_dir + "twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
372/27:
data_dir = './'
topic = 'Canada' #it can be a list of topics,  comman means 'or'
topn = 10
time_limit = 20
user_input = 'tariff'
with open(data_dir + "twitter_credentials.json", "r") as file:  
     credentials = json.load(file)
# Instantiate an object
python_tweets = Twython( credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])
auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])
372/28: recommendations = process_user_input(user_input, time_limit, topic, topn)
372/29: recommendations.iloc[0]['tweet']
372/30: recommendations
371/1: runfile('/Users/shahla/Dropbox/SharpestMinds/data_collection.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
371/2: runfile('/Users/shahla/Dropbox/SharpestMinds/data_preprocessing.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
371/3: runfile('/Users/shahla/Dropbox/SharpestMinds/data_preprocessing.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
371/4:
print('normalizing latest tweets...')
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda tweet: tweet_preprocess(tweet, model))
tweets.loc[:,'normalized']
371/5:
def tweet_preprocess(row_tweet, model):
    return clean_text(model, remove_punctuation(tokenize_tweet(row_tweet)))
371/6:
print('normalizing latest tweets...')
tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda tweet: tweet_preprocess(tweet, model))
tweets.loc[:,'normalized']
371/7: topic = input()
371/8: topic
371/9: input('d'+)
371/10: runfile('/Users/shahla/Dropbox/SharpestMinds/data_preprocessing.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
371/11: runfile('/Users/shahla/Dropbox/SharpestMinds/data_preprocessing.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
371/12: import genism
371/13: import gensim
371/14: tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, topic)
371/15:
vectorized_tweets = vectorize_latest_tweets(tweets, model)
vectorized_tweets
371/16:
print('uploading the model...')
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
vectorized_tweets = vectorize_latest_tweets(tweets, model)
vectorized_tweets
371/17:
import tweepy
import numpy as np
import time
import string
from twython import Twython
import json
from sklearn import preprocessing
from data_collection import get_latest_tweets
import gensim
from nltk.corpus import stopwords
371/18: vectorized_tweets = vectorize_latest_tweets(tweets, model)
371/19: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
371/20: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
371/21: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
371/22:
def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn, model):
    vec_tweets = np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
    vectorized_tweets.loc[:,'similarity_score'] = cos 
    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
    return vectorized_tweets[0:topn]
371/23: recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets, topn, model)
371/24: vectorized_user_input = vectorize_user_input(user_input, model)
371/25:
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
vectorized_tweets = vectorize_latest_tweets(tweets, model)
371/26: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
374/1: runfile('/Users/shahla/Dropbox/SharpestMinds/data_collection.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
374/2: runfile('/Users/shahla/Dropbox/SharpestMinds/data_collection.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
374/3: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
374/4: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
374/5: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
374/6:
    vectorized_tweets = vectorize_latest_tweets(tweets, model)
    vectorized_user_input = vectorize_user_input(user_input, model)
    print('finding the top'+str(topn)+'tweets...')
    # find the top topn= 10  similar tweets
    recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets, topn, model)
    print('top '+ str(topn) + ' similar tweets are obtained!')
return recommendations[0:topn]
378/1: runfile('/Users/shahla/Dropbox/SharpestMinds/hello.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
378/2: runfile('/Users/shahla/Dropbox/SharpestMinds/hello.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
378/3: runfile('/Users/shahla/Dropbox/SharpestMinds/hello.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
378/4: runfile('/Users/shahla/Dropbox/SharpestMinds/hello.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
378/5: runfile('/Users/shahla/Dropbox/SharpestMinds/hello.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
378/6: runfile('/Users/shahla/Dropbox/SharpestMinds/hello.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
379/1: runfile('/Users/shahla/Dropbox/SharpestMinds/hello.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
380/1: runfile('/Users/shahla/Dropbox/SharpestMinds/hello.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/1: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/2: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/3:
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
       vectorized_tweets = vectorize_latest_tweets(tweets, model)
381/4:
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
       vectorized_tweets = vectorize_latest_tweets(tweets, model)
381/5: model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)
381/6: vectorized_tweets = vectorize_latest_tweets(tweets, model)
381/7: tweets, no_data = get_latest_tweets_for_topic(track_list, data_dir, time_limit)
381/8: topic='tariff'
381/9: track_list = [k for k in topic.split(',')]
381/10:
vectorized_tweets = vectorize_latest_tweets(tweets, model)
vectorized_user_input = vectorize_user_input(user_input, model)
print('finding the top '+str(topn)+' tweets...')
381/11: tweets, no_data = get_latest_tweets_for_topic(track_list, data_dir, time_limit)
381/12: data_dir='./'
381/13:
tweets, no_data = get_latest_tweets_for_topic(track_list, data_dir, time_limit)
    if no_data:
        s
381/14: tweets, no_data = get_latest_tweets_for_topic(track_list, data_dir, time_limit)
381/15: time_limit=10
381/16: tweets, no_data = get_latest_tweets_for_topic(track_list, data_dir, time_limit)
381/17:
vectorized_tweets = vectorize_latest_tweets(tweets, model)
vectorized_user_input = vectorize_user_input(user_input, model)
381/18:
vec_tweets = np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)
vectorized_tweets.loc[:,'similarity_score'] = cos 
vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
381/19:
vec_tweets = np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))
cos=model.cosine_similarities(vectorized_user_input['vector'].reshape(-1,), vec_tweets)
vectorized_tweets.loc[:,'similarity_score'] = cos 
vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)
381/20: vectorized_tweets[0:10]
381/21: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/22: recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets, topn, model)
381/23: topn=10
381/24: recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets, topn, model)
381/25: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/26: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/27: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/28: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/29: list(recommendations['tweet'])
381/30: runfile('/Users/shahla/Dropbox/SharpestMinds/generate-recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/31: runfile('/Users/shahla/Dropbox/SharpestMinds/app.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/32: runfile('/Users/shahla/Dropbox/SharpestMinds/generate_recomendations.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/33: runfile('/Users/shahla/Dropbox/SharpestMinds/app.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/34: runfile('/Users/shahla/Dropbox/SharpestMinds/app.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
381/35: runfile('/Users/shahla/Dropbox/SharpestMinds/app.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
383/1: runfile('/Users/shahla/Dropbox/SharpestMinds/app.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
383/2: runfile('/Users/shahla/Dropbox/SharpestMinds/app.py', wdir='/Users/shahla/Dropbox/SharpestMinds')
383/3: fetch_tweets(iran,10)
383/4: fetch_tweets('iran',10)
383/5: fetch_tweets('iran',10)
383/6: fetch_tweets('iran nuclear energy',20)
383/7: fetch_tweets('nuclear',10)
383/8: topic = 'kate midelton'track_list = [k for k in topic.split(',')]
383/9: topic='kate midelton'
383/10: track_list = [k for k in topic.split(',')]
383/11: track_list
383/12: track_list = [k for k in topic.split(' ')]
383/13: track_list
384/1:
import pandas as pd
b2=pd.read_csv("B2.csv")
384/2:
import pandas as pd
b2=pd.read_csv("shahla/desktop/B2.csv")
384/3:
import pandas as pd
b2=pd.read_csv("/Users/shahla/Desktop/B2.csv")
384/4: b2
384/5: import matplotlib.pyplot as plt
384/6: b2.x
384/7: b2['x']
384/8: b2.columns
384/9: b2['X']
384/10: b2.columns
384/11:
plt.scatter(b2['X'], b2['Residual'])
#plt.title('Scatter plot pythonspot.com')
plt.xlabel('x')
plt.ylabel('Residual')
plt.show()
384/12: b1=pd.read_csv("/Users/shahla/Desktop/B1.csv")
384/13: b1
384/14:
plt.scatter(b1['t'], b2['Residual'])
#plt.title('Scatter plot pythonspot.com')
plt.xlabel('t')
plt.ylabel('Residual')
plt.show()
384/15:
plt.scatter(b1['t'], b1['Residual'])
#plt.title('Scatter plot pythonspot.com')
plt.xlabel('t')
plt.ylabel('Residual')
plt.show()
384/16:
plt.scatter(b1['t'], b1['Residual'])
#plt.title('Scatter plot pythonspot.com')
plt.xlabel('t')
plt.ylabel('Residual')
plt.show()
savefig('b1.pdf')
384/17:
plt.scatter(b1['t'], b1['Residual'])
#plt.title('Scatter plot pythonspot.com')
plt.xlabel('t')
plt.ylabel('Residual')
plt.show()
savefig('/Users/shahla/Desktop/b1.pdf')
384/18:
plt.scatter(b1['t'], b1['Residual'])
#plt.title('Scatter plot pythonspot.com')
plt.xlabel('t')
plt.ylabel('Residual')
plt.show()
plt.savefig('/Users/shahla/Desktop/b1.pdf')
385/1: runfile('/Users/shahla/Desktop/untitled1.py', wdir='/Users/shahla/Desktop')
385/2: runfile('/Users/shahla/Desktop/untitled1.py', wdir='/Users/shahla/Desktop')
385/3: runfile('/Users/shahla/Desktop/untitled1.py', wdir='/Users/shahla/Desktop')
386/1:
 import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline
386/2:
 import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
386/3: data=pd.read_csv('../input/covtype.csv')
386/4: data=pd.read_csv('../covtype.csv')
386/5: data=pd.read_csv('../covtype.csv')
386/6: data=pd.read_csv('covtype.csv')
386/7: data.info()
386/8: data.shap
386/9: data.shape
386/10: data.isnull().sum()
386/11: data.head
386/12: data.describe()
386/13: from sklearn.ensemble import RandomForestClassifier
386/14:
 import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
386/15: train,test = train_test_split(data,test_size=0.2,random_state=999)
386/16: list(data.columns)
386/17: data[0:54]
386/18: data[0:54].columns
386/19: data[0:53].columns
386/20:
x=data[0:40]
x.columns
386/21:
x=data[:,0:40]
x.columns
386/22: x=data[:,0:40]
386/23: x=data.iloc(0:54)
386/24: x=data.iloc[0:54]
386/25:
x=data.iloc[0:54]
x.columns
386/26:
x=data.iloc[0:53]
x.columns
386/27:
x=data.iloc[0:53]
x.shap
386/28:
x=data.iloc[0:53]
x.shape
386/29:
x=data.iloc[:,0:53]
x.shape
386/30:
x=data.iloc[:,0:53]
x.columns
386/31:
x=data.iloc[:,0:54]
x.columns
386/32:
x=data.iloc[:,0:55]
x.columns
386/33:
# Extracting all numerical features from data
num_fea = data.iloc[:, :10]

# extracting all binary features from data
binary_fea = data.iloc[:, 10:-1]


# statistics of numerical features
num_fea.describe()
386/34: binary_fea.describe()
386/35: target = data['Cover_Type']
389/1:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
389/2: data=pd.read_csv('covtype.csv')
389/3: data.shape
389/4: data.info()
389/5: data.isnull().sum()
389/6: train,test = train_test_split(data,test_size=0.2,random_state=999)
389/7: list(data.columns)
389/8:
x=data.iloc[:,0:55]
target = data['Cover_Type']
389/9: x.columns
389/10:
x=data.iloc[:,0:56]
target = data['Cover_Type']
389/11: x.columns
389/12:
x=data.iloc[:,0:54]
target = data['Cover_Type']
389/13: x.columns
389/14:
x=data.iloc[:,0:55]
target = data['Cover_Type']
389/15: x.columns
389/16:
x=data.iloc[:,0:54]
target = data['Cover_Type']
389/17: x.columns
389/18: num_fea.cor()
389/19:
# Extracting all numerical features from data
num_fea = data.iloc[:, :10]

# extracting all binary features from data
binary_fea = data.iloc[:, 10:-1]


# statistics of numerical features
num_fea.describe()
389/20: num_fea.cor()
389/21: num_fea.corr()
389/22: cor_matrix = num_fea.corr()
389/23: cor_matrix.style.background_gradient(cmap='coolwarm')
389/24:
import seaborn as sns

f, ax = pl.subplots(figsize=(10, 8))

sns.heatmap(cor_matrix, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)
389/25:

import seaborn as sns

f, ax = plt.subplots(figsize=(10, 8))

sns.heatmap(cor_matrix, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)
389/26:

import seaborn as sns

f, ax = plt.subplots(figsize=(10, 8))

sns.heatmap(cor_matrix, mask=np.zeros_like(cor_matrix, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)
389/27:

import seaborn as sns

f, ax = plt.subplots(figsize=(10, 8))

sns.heatmap(cor_matrix, mask=np.zeros_like(cor_matrix, dtype=np.bool), cmap='coolwarm',
            square=True, ax=ax)
389/28:
cor_matrix.style.background_gradient(cmap='coolwarm')
plt.colorbar()
389/29: cor_matrix.style.background_gradient(cmap='coolwarm')
389/30: RF=RandomForestRegressor()
389/31:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
389/32: RF=RandomForestRegressor()
389/33: model=RandomForestRegressor()
389/34: model=RandomForestRegressor()
389/35:
model=RandomForestRegressor()
model.get_parameters
389/36:
model=RandomForestRegressor()
model.get_parameters()
389/37:
model=RandomForestRegressor()
model.get_params()
389/38:
model=RandomForestRegressor()
model.get_params(bootstrap=False, max_depth=3, n_estimators=1)
389/39: model = RandomForestRegressor(bootstrap=False, max_depth=3, n_estimators=1)
389/40:
model=RandomForestRegressor()
model.get_params()
389/41:
x=train.iloc[:,0:54]
target = train['Cover_Type']
389/42:
x_train = train.iloc[:,0:54]
y_train = train['Cover_Type']
x_test = test.iloc[:,0:54]
y_test = test['Cover_Type']
389/43: model.fit(x_train,y_train)
389/44:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
389/45: data=pd.read_csv('covtype.csv')
389/46: data.shape
389/47: data.info()
389/48: data.isnull().sum()
389/49: data.describe()
389/50: train,test = train_test_split(data,test_size=0.2,random_state=999)
389/51: list(data.columns)
389/52:
x_train = train.iloc[:,0:54]
y_train = train['Cover_Type']
x_test = test.iloc[:,0:54]
y_test = test['Cover_Type']
389/53: model = RandomForestRegressor(bootstrap=False, max_depth=3, n_estimators=1)
389/54: model.fit(x_train,y_train)
389/55:
estimator = model.estimators_[5]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = iris.feature_names,
                class_names = iris.target_names,
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')
389/56:
estimator = model.estimators_[1]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = iris.feature_names,
                class_names = iris.target_names,
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')
389/57:
estimator = model.estimators_[0]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = iris.feature_names,
                class_names = iris.target_names,
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')
389/58:
estimator = model.estimators_[0]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = x_train.columns,
                class_names = y_train.columns,
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')
389/59:
estimator = model.estimators_[0]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = x_train.columns,
                class_names = 'll',
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')
389/60:
estimator = model.estimators_[0]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = x_train.columns,
                class_names = 'Cover_Type',
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')
389/61:
model = model.fit(x_train,y_train)
model.predict(x_test,y_test)
389/62:
model = model.fit(x_train,y_train)
pred = model.predict(x_test,y_test)
389/63:
model = model.fit(x_train,y_train)
y_pred = model.predict(x_test)
389/64:
from sklearn.metrics import accuracy_score
accuracy_score(y_test,pred)
389/65:
from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)
389/66: y_test
389/67: y_test.dim
389/68: len(y_test)
389/69: len(y_pred)
389/70: data['Cover_Type']
389/71: model = RandomForestClassifier(bootstrap=False, max_depth=3, n_estimators=1)
389/72:
model = model.fit(x_train,y_train)
y_pred = model.predict(x_test)
389/73: len(y_pred)
389/74:
from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)
389/75:
estimator = model.estimators_[0]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = x_train.columns,
                class_names = 'Cover_Type',
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')
389/76: data['Cover_Type'].range
389/77: data['Cover_Type'].describe
389/78: data['Cover_Type'].info()
389/79: data['Cover_Type'].info
389/80: data['Cover_Type'].dtypes
389/81:
data['Cover_Type'].dtypes
data['Cover_Type'].value_counts()
389/82:
data['Cover_Type'].dtypes
data['Cover_Type'].value_counts()
data['Cover_Type'].astype('category')
389/83:
x_train = train.iloc[:,0:54]
y_train = train['Cover_Type']
x_test = test.iloc[:,0:54]
y_test = test['Cover_Type'].astype('category')
389/84:
x_train = train.iloc[:,0:54]
y_train = train['Cover_Type'].astype('category')
x_test = test.iloc[:,0:54]
y_test = test['Cover_Type'].astype('category')
389/85:
data['Cover_Type'].dtypes
data['Cover_Type'].value_counts()
389/86: model = RandomForestClassifier(bootstrap=False, max_depth=3, n_estimators=1)
389/87:
model = model.fit(x_train,y_train)
y_pred = model.predict(x_test)
389/88: model = RandomForestClassifier(bootstrap=False, max_depth=3, n_estimators=1)
389/89:
model = model.fit(x_train,y_train)
y_pred = model.predict(x_test)
389/90:
from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)
389/91:
model = model.fit(x_train,y_train)
y_pred = model.predict(x_test)
389/92: data=pd.read_csv('covtype.csv')
389/93: data.shape
389/94: data.info()
389/95: train,test = train_test_split(data,test_size=0.2,random_state=999)
389/96: list(data.columns)
389/97:
x_train = train.iloc[:,0:54]
y_train = train['Cover_Type']
x_test = test.iloc[:,0:54]
y_test = test['Cover_Type']
389/98:
# Extracting all numerical features from data
num_fea = data.iloc[:, :10]

# extracting all binary features from data
binary_fea = data.iloc[:, 10:-1]


# statistics of numerical features
num_fea.describe()
389/99: binary_fea.describe()
389/100:
data['Cover_Type'].dtypes
data['Cover_Type'].value_counts()
389/101:
Cor = con_variables.iloc[:,0:10]
Cor_matrxi = Cor.corr(method='pearson', min_periods=1)
print(Cor_matrxi)
389/102:
Cor = con_variables.iloc[:,0:10]
Cor_matrix = Cor.corr(method='pearson', min_periods=1)
print(Cor_matrix)
389/103: cor_matrix = num_fea.corr()
389/104: cor_matrix.style.background_gradient(cmap='coolwarm')
389/105:
model=RandomForestRegressor()
model.get_params()
389/106: model = RandomForestClassifier(bootstrap=False, max_depth=3, n_estimators=1)
389/107:
model = model.fit(x_train,y_train)
y_pred = model.predict(x_test)
389/108:
estimator = model.estimators_[0]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = x_train.columns,
                class_names = 'Cover_Type',
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')
389/109:
for n_estimators = 20:30:
        model = model = RandomForestClassifier(bootstrap=False, max_depth=3)
389/110:
for n_estimators from  20:30:
        model = model = RandomForestClassifier(bootstrap=False, max_depth=3)
389/111:
for n_estimators in range(20,30):
        model = model = RandomForestClassifier(bootstrap=False, max_depth=3)
389/112: range(20,30)
389/113:
for i in range(20,30):
    print(i)
389/114:
for n_estimators in range(20,31):
        model = model = RandomForestClassifier(bootstrap=False, max_depth=3)
389/115:
for n_estim in range(20,31):
        model = model = RandomForestClassifier(n_estimators=n_estim)
389/116:
for n_estim in range(20,31):
        model = model = RandomForestClassifier(n_estimators=n_estim)
        model = model.fit(x_train,y_train)
        y_pred = model.predict(x_test)
        acc = accuracy_score(y_test,y_pred)
389/117:
acc = 0
n_estim = 0
for n_estim in range(20,31):
        model = RandomForestClassifier(n_estimators=n_estim)
        model = model.fit(x_train,y_train)
        y_pred = model.predict(x_test)
        accuracy = accuracy_score(y_test,y_pred)
        if accuracy>acc:
            acc=accuracy
            n_estimators = n_estim
389/118:
acc = 0
n_estimators = 0
for n_estim in range(20,31):
        model = RandomForestClassifier(n_estimators=n_estim)
        model = model.fit(x_train,y_train)
        y_pred = model.predict(x_test)
        accuracy = accuracy_score(y_test,y_pred)
        if accuracy>acc:
            acc=accuracy
            n_estimators = n_estim
389/119:

        model = RandomForestClassifier(n_estimators=n_estim)
        model = model.fit(x_train,y_train)
        y_pred = model.predict(x_test)
        accuracy = accuracy_score(y_test,y_pred)
        if accuracy>acc:
            acc=accuracy
            n_estimators = n_estim
389/120:

model = RandomForestClassifier(n_estimators=20)
model = model.fit(x_train,y_train)
y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test,y_pred)
389/121: accuracy
389/122: np.linspace(1, 45, num = 3)
389/123: np.linspace(1, 45, num = 1)
389/124: np.linspace(1, 45, num = 5)
389/125:

RFReg = RandomForestClassifier(n_estimators = 20, random_state = 1, n_jobs = -1) 

param_grid = { 
    'max_features' : ["auto", "sqrt", "log2"],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
}
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 50)
CV_rfc.fit(X_train, y_train)
389/126:

RFReg = RandomForestClassifier(n_estimators = 20, random_state = 1, n_jobs = -1) 

param_grid = { 
    'max_features' : ["auto", "sqrt", "log2"],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
}
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 50)
CV_rfc.fit(x_train, y_train)
391/1:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
391/2: data=pd.read_csv('covtype.csv')
391/3: data.shape
391/4: data.info()
391/5: data.isnull().sum()
391/6: train,test = train_test_split(data,test_size=0.2,random_state=999)
391/7: list(data.columns)
391/8:
x_train = train.iloc[:,0:54]
y_train = train['Cover_Type']
x_test = test.iloc[:,0:54]
y_test = test['Cover_Type']
391/9:
# Extracting all numerical features from data
num_fea = data.iloc[:, :10]

# extracting all binary features from data
binary_fea = data.iloc[:, 10:-1]


# statistics of numerical features
num_fea.describe()
391/10: binary_fea.describe()
391/11:
data['Cover_Type'].dtypes
data['Cover_Type'].value_counts()
391/12:
Cor = con_variables.iloc[:,0:10]
Cor_matrix = Cor.corr(method='pearson', min_periods=1)
print(Cor_matrix)
391/13:
Cor = num_fea.iloc[:,0:10]
Cor_matrix = Cor.corr(method='pearson', min_periods=1)
print(Cor_matrix)
391/14:
model=RandomForestRegressor()
model.get_params()
391/15:
model=RandomForestClassifier()
model.get_params()
391/16: model = RandomForestClassifier(bootstrap=False, max_depth=3, n_estimators=1)
391/17:
model = model.fit(x_train,y_train)
y_pred = model.predict(x_test)
391/18:
from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)
391/19:
estimator = model.estimators_[0]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = x_train.columns,
                class_names = 'Cover_Type',
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')
391/20:

model = RandomForestClassifier(n_estimators=20)
model = model.fit(x_train,y_train)
y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test,y_pred)
391/21: accuracy
391/22: np.linspace(0.1, 1.0, 10)
391/23:

RFReg = RandomForestClassifier(n_estimators = 20, random_state = 1, n_jobs = -1) 

param_grid = { 
    'criterian' : ['gini','entropy']
    'max_features' : ["auto", "sqrt", "log2"],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
}
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, random_state=1)
CV_rfc.fit(x_train, y_train)
391/24:

RFReg = RandomForestClassifier(n_estimators = 20, random_state = 1, n_jobs = -1) 

param_grid = { 
    'criterian' : ['gini','entropy']
    'max_features' : ['auto', 'sqrt', 'log2'],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
}
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, random_state=1)
CV_rfc.fit(x_train, y_train)
391/25:
param_grid = { 
    'criterian' : ['gini','entropy']
    #'max_features' : ['auto', 'sqrt', 'log2'],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
     }
391/26:
param_grid = { 
    'criterian' : ['gini','entropy']
    #'max_features' : ['auto', 'sqrt', 'log2'],
   # 'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
     }
391/27:
param_grid = { 
    'criterian' : ['gini','entropy']
    #'max_features' : ['auto', 'sqrt', 'log2'],
   # 'min_samples_split' : np.linspace(0.1, 1.0, 10),
     #'max_depth' : [x for x in range(1,20)]
     }
391/28:
param_grid = { 
    'criterian' : ['gini','entropy']
    'max_features' : ['auto', 'sqrt', 'log2'],
   # 'min_samples_split' : np.linspace(0.1, 1.0, 10),
     #'max_depth' : [x for x in range(1,20)]
     }
391/29:

RFReg = RandomForestClassifier(n_estimators = 20, random_state = 1, n_jobs = -1) 

param_grid = { 
    'criterian' : ['gini','entropy'],
    'max_features' : ['auto', 'sqrt', 'log2'],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, random_state=1)
CV_rfc.fit(x_train, y_train)
391/30:

RFReg = RandomForestClassifier(n_estimators = 20, random_state = 1, n_jobs = -1) 

param_grid = { 
    'criterian' : ['gini','entropy'],
    'max_features' : ['auto', 'sqrt', 'log2'],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20)
CV_rfc.fit(x_train, y_train)
391/31:

RFReg = RandomForestClassifier(n_estimators = 20, n_jobs = -1) 

param_grid = { 
    'criterian' : ['gini','entropy'],
    'max_features' : ['auto', 'sqrt', 'log2'],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, 
                            random_state=1)
CV_rfc.fit(x_train, y_train)
391/32:

RFReg = RandomForestClassifier(n_estimators = 20, n_jobs = -1) 

param_grid = { 
   # 'criterian' : ['gini','entropy'],
    'max_features' : ['auto', 'sqrt', 'log2'],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
     'max_depth' : [x for x in range(1,20)]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, 
                            random_state=1)
CV_rfc.fit(x_train, y_train)
391/33: CV_rfc.best_params_
391/34: CV_rfc.best_score_
391/35: CV_rfc.get_params
391/36:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
391/37:

RFReg = RandomForestClassifier(n_estimators = 20, n_jobs = -1) 

param_grid = { 
    'criterian' : ['gini','entropy'],
    'max_features' : ['auto', 'sqrt', 'log2'],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
    "min_samples_leaf": sp_randint(1, 10),
     'max_depth' : [x for x in range(1,20)],
    "bootstrap": [True, False]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, 
                            random_state=1)
CV_rfc.fit(x_train, y_train)
391/38:

RFReg = RandomForestClassifier(n_estimators = 20, n_jobs = -1) 

param_grid = { 
    'max_features' : ['auto', 'sqrt', 'log2'],
    'min_samples_split' : np.linspace(0.1, 1.0, 10),
    "min_samples_leaf": sp_randint(1, 10),
     'max_depth' : [x for x in range(1,20)],
    "bootstrap": [True, False]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=RFReg, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, 
                            random_state=1)
CV_rfc.fit(x_train, y_train)
391/39: CV_rfc.best_params_
391/40: report(CV_rfc.cv_results_)
391/41:
# Utility function to report best scores
def report(results, n_top=3):
    for i in range(1, n_top + 1):
        candidates = np.flatnonzero(results['rank_test_score'] == i)
        for candidate in candidates:
            print("Model with rank: {0}".format(i))
            print("Mean validation score: {0:.3f} (std: {1:.3f})".format(
                  results['mean_test_score'][candidate],
                  results['std_test_score'][candidate]))
            print("Parameters: {0}".format(results['params'][candidate]))
            print("")
391/42: report(CV_rfc.cv_results_)
391/43:
#the top 3 models
report(CV_rfc.cv_results_)
391/44: best_params = CV_rfc.best_params_
391/45:
best_params = CV_rfc.best_params_
rf = RandomForestClassifier(n_estimators = 20, n_jobs = -1,best_params)
391/46:
best_params = CV_rfc.best_params_
rf = RandomForestClassifier(best_params,n_estimators = 20, n_jobs = -1)
391/47:
best_params = CV_rfc.best_params_
rf = RandomForestClassifier(best_params, n_jobs = -1)
391/48: CV_rfc.best_params_
391/49: rf.get_params
391/50:
best_params = CV_rfc.best_params_
rf = RandomForestClassifier(best_params)
391/51: rf.get_params
391/52: y_pred = CV_rfc.predict(x_test)
391/53: metrics.mean_squared_error(y_test, y_pred)
391/54:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
391/55:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, mean_square_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
391/56:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
391/57: metrics.mean_squared_error(y_test, y_pred)
391/58: mean_squared_error(y_test, y_pred)
391/59:
mean_squared_error(y_test, y_pred)
accuracy_score(y_test,y_pred)
391/60: rf=RandomForestClassifier(**best_params,jobs=-1)
391/61: rf=RandomForestClassifier(**best_params,njobs=-1)
391/62: rf=RandomForestClassifier(**best_params,n_jobs=-1)
391/63:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
for i in range(20,22):
    print(i)
391/64:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
for i in range(20,22):
    rf.n_estimators=i
391/65: rf.get_params
391/66:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
from sklearn.base import clone
391/67:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
for i in range(20,22):
    base(rf)
    rf.n_estimators=i
391/68:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
for i in range(20,22):
    sklearn.base(rf)
    rf.n_estimators=i
391/69:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
for i in range(20,22):
    clone(rf)
    rf.n_estimators=i
391/70: rf.get_params
391/71: best_params
391/72:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
for i in range(20,23):
    clone(rf)
    rf.n_estimators=i
391/73: rf.get_params
391/74:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
for i in range(20,22):
    clone(rf)
    rf.n_estimators=i
    rf.fit(x_train,y_train)
    y_pred = rf.predict(x_test)
    accuracy_score(y_pred,y_test)
391/75:
a=[]
a.append(1)
391/76:
a=[]
a.append(1)
a
391/77:
a=[]
a.append(1)
a.ppend(2)
391/78:
a=[]
a.append(1)
a.append(2)
391/79:
a=[]
a.append(1)
a.append(2)
a
391/80:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
accuracy = []
for i in range(20,22):
    clone(rf)
    rf.n_estimators=i
    rf.fit(x_train,y_train)
    y_pred = rf.predict(x_test)
    accuracy.append(accuracy_score(y_pred,y_test))
391/81: accuracy
391/82:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
accuracy = []
for i in range(20,30):
    clone(rf)
    rf.n_estimators=i
    rf.fit(x_train,y_train)
    y_pred = rf.predict(x_test)
    accuracy.append(accuracy_score(y_pred,y_test))
391/83: accuracy
391/84:
rf=RandomForestClassifier(**best_params,n_jobs=-1)
accuracy = []
for i in range(20,60):
    clone(rf)
    rf.n_estimators=i
    rf.fit(x_train,y_train)
    y_pred = rf.predict(x_test)
    accuracy.append(accuracy_score(y_pred,y_test))
391/85: accuracy
391/86:
plt.plot(range(20,60), accuracy, 'bx-')
plt.xlabel('Number of Decision Trees(DTs)')
plt.ylabel('Accuracy')
plt.title('Elbow Method For Optimal DTs')
plt.show()
391/87: rf.get_params
391/88: x=RandomForestClassifier(**best_params,n_jobs=-1,random_state=1)
391/89: np.arange(1, 28, 1)
391/90:

rf = RandomForestClassifier(n_estimators = 20, n_jobs = -1,random_state=1) 

param_grid = { 
    'max_features' : ['auto', 'sqrt', 'log2'],
    "min_samples_split": np.arange(1,150,1),
    "min_samples_leaf": np.arange(1,60,1),
     "max_leaf_nodes": np.arange(2,60,1),
    "min_weight_fraction_leaf": np.arange(0.1,0.4, 0.1)
     'max_depth' : [x for x in range(1,30)],
    "bootstrap": [True, False]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=rf, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, 
                            random_state=1)
CV_rfc.fit(x_train, y_train)
391/91:

rf = RandomForestClassifier(n_estimators = 20, n_jobs = -1,random_state=1) 

param_grid = { 
    'max_features' : ['auto', 'sqrt', 'log2'],
    "min_samples_split": np.arange(1,150,1),
    "min_samples_leaf": np.arange(1,60,1),
     "max_leaf_nodes": np.arange(2,60,1),
    "min_weight_fraction_leaf": np.arange(0.1,0.4, 0.1),
     'max_depth' : [x for x in range(1,30)],
    "bootstrap": [True, False]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=rf, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, 
                            random_state=1)
CV_rfc.fit(x_train, y_train)
391/92: best_params = CV_rfc.best_params_
391/93: y_pred = CV_rfc.predict(x_test)
391/94:
mean_squared_error(y_test, y_pred)
accuracy_score(y_test,y_pred)
391/95: y_pred = CV_rfc.predict(x_test)
391/96:
mean_squared_error(y_test, y_pred)
accuracy_score(y_test,y_pred)
391/97: best_params = CV_rfc.best_params_
391/98: y_pred = CV_rfc.predict(x_test)
391/99:
mean_squared_error(y_test, y_pred)
accuracy_score(y_test,y_pred)
391/100:
# Utility function to report best scores
def report(results, n_top=3):
    for i in range(1, n_top + 1):
        candidates = np.flatnonzero(results['rank_test_score'] == i)
        for candidate in candidates:
            print("Model with rank: {0}".format(i))
            print("Mean validation score: {0:.3f} (std: {1:.3f})".format(
                  results['mean_test_score'][candidate],
                  results['std_test_score'][candidate]))
            print("Parameters: {0}".format(results['params'][candidate]))
            print("")
391/101:
#the top 3 models
report(CV_rfc.cv_results_)
391/102:
rf=RandomForestClassifier(**best_params,n_jobs=-1,random_state=1)
accuracy = []
for i in range(20,500):
    clone(rf)
    rf.n_estimators=i
    rf.fit(x_train,y_train)
    y_pred = rf.predict(x_test)
    accuracy.append(accuracy_score(y_pred,y_test))
391/103:

rf = RandomForestClassifier(n_estimators = 20, n_jobs = -1,random_state=1) 

param_grid = { 
    'max_features' : ['auto', 'sqrt', 'log2'],
    "min_samples_split": np.arange(1,150,1),
    "min_samples_leaf": np.arange(1,60,1),
    # "max_leaf_nodes": np.arange(2,60,1),
    #"min_weight_fraction_leaf": np.arange(0.1,0.4, 0.1),
     'max_depth' : [x for x in range(1,30)],
    "bootstrap": [True, False]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=rf, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 20, 
                            random_state=1)
CV_rfc.fit(x_train, y_train)
391/104:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
from sklearn.base import clone
from sklearn.feature_selection import SelectFromModel
391/105:

rf = RandomForestClassifier(n_estimators = 20, n_jobs = -1,random_state=1) 

param_grid = { 
    'max_features' : ['sqrt', 'log2'],
    "min_samples_split": np.arange(1,150,10),
    "min_samples_leaf": np.arange(1,60,10),
    # "max_leaf_nodes": np.arange(2,60,1),
    #"min_weight_fraction_leaf": np.arange(0.1,0.4, 0.1),
     'max_depth' : [1,2,4,8,16,32,64,None]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=rf, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 100, 
                            random_state=1)
CV_rfc.fit(x_train, y_train)
391/106:

rf = RandomForestClassifier(n_estimators = 20, n_jobs = -1,random_state=1) 

param_grid = { 
    'max_features' : ['sqrt', 'log2'],
    "min_samples_split": [0.002,.01,.02,.04,.08,.1],
    "min_samples_leaf": np.arange(1,60,10),
    # "max_leaf_nodes": np.arange(2,60,1),
    #"min_weight_fraction_leaf": np.arange(0.1,0.4, 0.1),
     'max_depth' : [1,2,4,8,16,32,64,None]
     }
from sklearn.model_selection import RandomizedSearchCV
CV_rfc = RandomizedSearchCV(estimator=rf, param_distributions =param_grid, n_jobs = -1, cv= 10, n_iter = 100, 
                            random_state=1)
CV_rfc.fit(x_train, y_train)
393/1:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
from sklearn.base import clone
from sklearn.feature_selection import SelectFromModel
393/2: data=pd.read_csv('covtype.csv')
393/3: data.shape
393/4: data.isnull().sum()
393/5:
# Extracting all numerical features from data
num_fea = data.iloc[:, :10]

# extracting all binary features from data
binary_fea = data.iloc[:, 10:-1]


# statistics of numerical features
num_fea.describe()
393/6:
data['Cover_Type'].dtypes
data['Cover_Type'].value_counts()
393/7:
class_dist=data.groupby('Cover_Type').size()
class_label=pd.DataFrame(class_dist,columns=['Size'])
plt.figure(figsize=(8,6))
sns.barplot(x=class_label.index,y='Size',data=class_label)
393/8: data['Cover_Type'].dtypes
393/9: class_type=data['Cover_Type'].value_counts()
393/10: target_freq=data['Cover_Type'].value_counts()
393/11: target_label=data['Cover_Type'].value_counts()
393/12:

plt.figure(figsize=(8,6))
sns.barplot(x=target_label.index,y='Size',data=target_label)
393/13: target_freq=data['Cover_Type'].value_counts()
393/14:
plt.figure(figsize=(10,5))
sns.barplot(target_freq.index, target_freq.values, alpha=0.8)
plt.title('Starbucks in top 10 cities in the World')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('city', fontsize=12)
plt.show()
393/15:
plt.figure(figsize=(10,5))
sns.barplot(target_freq.index, target_freq.values, alpha=0.8)
plt.title('Starbucks in top 10 cities in the World')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Cover_Type', fontsize=12)
plt.show()
393/16:
plt.figure(figsize=(10,5))
sns.barplot(target_freq.index, target_freq.values, alpha=0.8)
plt.title('Distribution of the Cover Type')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Cover_Type', fontsize=12)
plt.show()
393/17: binary_fea
393/18: num_fea
393/19: num_fea.shape
393/20: num_fea[0]
393/21: num_fea[0,]
393/22: num_fea.iloc[0,]
393/23: num_fea.iloc[,0]
393/24: num_fea.iloc[:,0]
393/25: num_fea.iloc[:,]
393/26: num_fea.iloc[0,:]
393/27: binary_fea.iloc[0,:]
393/28: binary_fea.shape
393/29: binary_fea
393/30: binary_fea['Wilderness_Area1']
393/31: binary_fea['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']
393/32: binary_fea['Wilderness_Area1']
393/33: binary_fea['Wilderness_Area1','Wilderness_Area2']
393/34: binary_fea[{'Wilderness_Area1','Wilderness_Area2'}]
393/35: binary_fea[{'Wilderness_Area1','Wilderness_Area2','Wilderness_Area3'}]
393/36: binary_fea[{'Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4'}]
393/37: binary_fea[{'Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Wilderness_Area5'}]
393/38: binary_fea[{'Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4'}]
393/39:
binary_fea[{'Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4'}]

data['Wilderness_Area'] = binary_fea['Wilderness_Area1']+2*binary_fea['Wilderness_Area2']+
3*binary_fea['Wilderness_Area3']+4*binary_fea['Wilderness_Area4']
393/40:
binary_fea[{'Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4'}]

data['Wilderness_Area'] = binary_fea['Wilderness_Area1']+2*binary_fea['Wilderness_Area2']+3*binary_fea['Wilderness_Area3']+4*binary_fea['Wilderness_Area4']
393/41: data['Wilderness_Area']
393/42: data['Wilderness_Area'].value_types
393/43: data['Wilderness_Area'].value_counts
393/44: data['Wilderness_Area'].value_counts()
393/45: binary_fea['Wilderness_Area1']
393/46: binary_fea['Wilderness_Area1'].value_counts()
393/47: binary_fea['Wilderness_Area2'].value_counts()
393/48:
Wilderness_Area = data['Wilderness_Area'].value_counts()
plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area.index, Wilderness_Area.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/49: binary_fea
393/50: binary_fea[1:10]
393/51: binary_fea[4:10]
393/52: binary_fea[,3:50]
393/53: binary_fea.COLUMNS
393/54: binary_fea.columns
393/55: binary_fea.shape[0]
393/56: binary_fea.shape[1]
393/57: binary_fea.columns
393/58:
for i in range(0,40):
    print(i)
data['Soil_Type'] =
393/59:
for i in range(0,40):
    print(i)
#data['Soil_Type'] =
393/60:
for i in range(1,41):
    print(i)
#data['Soil_Type'] =
393/61:
data['Soil_Type'] = data['Soil_Type1']
for i in range(2,41):
    print('Soil_Type'+'i')
    #data['Soil_Type'] = data['Soil_Type']+i*data['Soil_Type'+'i']
393/62:
data['Soil_Type'] = data['Soil_Type1']
for i in range(2,41):
    print('Soil_Type'+str(i))
    #data['Soil_Type'] = data['Soil_Type']+i*data['Soil_Type'+'i']
393/63:
data['Soil_Type'] = data['Soil_Type1']
for i in range(2,41):
    data['Soil_Type'] = data['Soil_Type']+i*data['Soil_Type'+str(i)]
393/64:  data['Soil_Type'].value_counts()
393/65:
data['Soil_Type'] = data['Soil_Type1']
for i in range(2,41):
    data['Soil_Type'] = data['Soil_Type']+i*data['Soil_Type'+str(i)]
data['Soil_Type29']
393/66:  data['Soil_Type'].value_counts()
393/67:
data['Soil_Type'] = data['Soil_Type1']
for i in range(2,41):
    data['Soil_Type'] = data['Soil_Type']+i*data['Soil_Type'+str(i)]
data['Soil_Type29'].value_counts()
393/68:
soil_Type = data['Soil_Type'].value_counts()
plt.figure(figsize=(10,5))
sns.barplot(soil_Type.index, soil_Type.values, alpha=0.8)
plt.title('Distribution of the soil_Type')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/69: soil_Type
393/70: soil_Type[0:15]
393/71: soil_Type.values>100
393/72: sum(soil_Type.values>1000)
393/73: sum(soil_Type.values>2000)
393/74: sum(soil_Type.values>3000)
393/75: sum(soil_Type.values>10000)
393/76: sum(soil_Type.values)<100
393/77: sum(soil_Type.values)<100
393/78: sum(soil_Type.values)<300
393/79: sum(soil_Type.values<300)
393/80: sum(soil_Type.values<500)
393/81: sum(soil_Type.values<900)
393/82: sum(soil_Type.values<300)
393/83: np.where(soil_Type.values<300)
393/84: soil_type[np.where(soil_Type.values<300)]
393/85: soil_Type[np.where(soil_Type.values<300)]
393/86: data[np.where(soil_Type.values<300)]
393/87: np.where(soil_Type.values<300)
393/88: soil_Type.index==35
393/89: soil_Type
393/90: soil_Type.values<300
393/91: sum(soil_Type.values<300)
393/92: soil_Type.values
393/93: soil_Type
393/94: Wilderness_Area
393/95: sum(data['Wilderness_Area']==2 & data['soil_Type']==29)
393/96: sum(data['Wilderness_Area']==2 && data['soil_Type']==29)
393/97: sum(data['Wilderness_Area']==2 and data['soil_Type']==29)
393/98: sum(data['Wilderness_Area']==2 & data['soil_Type']==29)
393/99: sum(data['Wilderness_Area']==2 && data['soil_Type']==29)
393/100: sum(data['Wilderness_Area']==2 and data['soil_Type']==29)
393/101: sum((data['Wilderness_Area']=2) and (data['soil_Type']=29))
393/102: sum((data['Wilderness_Area']==2) and (data['soil_Type']=29))
393/103: sum((data['Wilderness_Area']==2) and (data['soil_Type']==29))
393/104: (data['Wilderness_Area']==2) and (data['soil_Type']==29)
393/105: (data['Wilderness_Area']==2) && (data['soil_Type']==29)
393/106: (data['Wilderness_Area']==2) and (data['soil_Type']==29)
393/107: (data['Wilderness_Area']=2) and (data['soil_Type']=29)
393/108: data['Wilderness_Area']=2 and data['soil_Type']=29
393/109: data['Wilderness_Area']==2 and data['soil_Type']==29
393/110: data['Wilderness_Area']==2 | data['soil_Type']==29
393/111: (data['Wilderness_Area']==2) | (data['soil_Type']==29)
393/112: (data['Wilderness_Area']=2) and (data['soil_Type']=29)
393/113: (data['Wilderness_Area']==2) and (data['soil_Type']==29)
393/114: (data['Wilderness_Area']=2) and (data['soil_Type']=29)
393/115: data['Wilderness_Area']=2 and data['soil_Type']=29
393/116: data['Wilderness_Area']=2
393/117: data['Wilderness_Area']=2 and data['soil_Type']=29
393/118: data['Wilderness_Area']==2 and data['soil_Type']==29
393/119: data['Wilderness_Area']==2
393/120: data['Wilderness_Area']==2.any()
393/121: (data['Wilderness_Area']==2).any()
393/122: data(data['Wilderness_Area']==2).any()
393/123: data((data['Wilderness_Area']==2).any())
393/124: data[(data['Wilderness_Area']==2).any()]
393/125: data[[(data['Wilderness_Area']==2).any()]]
393/126: data.loc[(data['Wilderness_Area']==2).any()]
393/127: data.loc[(data['Wilderness_Area']=2).any()]
393/128: data.groupby(['Wilderness_Area']).count()['cover_Type'].unstack().plot(ax=ax)
393/129: data.groupby(['Wilderness_Area']).count()['Cover_Type'].unstack().plot(ax=ax)
393/130: data.groupby(['Wilderness_Area'])['Cover_Type'].unstack().plot(ax=ax)
393/131: data.groupby(['Wilderness_Area'])
393/132: data.groupby(['Wilderness_Area'])['Cover_Type']
393/133: data[data.groupby(['Wilderness_Area'])['Cover_Type']]
393/134: plot(data.groupby(['Wilderness_Area'])['Cover_Type'])
393/135: plt(data.groupby(['Wilderness_Area'])['Cover_Type'])
393/136: data.groupby(['Wilderness_Area'])['Cover_Type']
393/137: data.groupby(['Wilderness_Area'])['Cover_Type'].count()
393/138: data.groupby(['Wilderness_Area']).count()
393/139: data.groupby(['Wilderness_Area'])
393/140: data.groupby(['Wilderness_Area']).count()
393/141:
data.groupby(['Wilderness_Area'])
ax = sns.countplot(x="Wilderness_Area", hue='Cover_Type',data=data)
393/142:
data.groupby(['Wilderness_Area'])
ax = sns.countplot(hue='Cover_Type',x="Wilderness_Area",data=data)
393/143:
data.groupby(['Wilderness_Area'])
ax = sns.countplot(x='Cover_Type',hue="Wilderness_Area",data=data)
393/144:

data['Wilderness_Area'] = binary_fea['Wilderness_Area1']+2*binary_fea['Wilderness_Area2']+3*binary_fea['Wilderness_Area3']+4*binary_fea['Wilderness_Area4']
393/145: binary_fea['Wilderness_Area2'].value_counts()
393/146:
Wilderness_Area = data['Wilderness_Area'].value_counts()
plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area.index, Wilderness_Area.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/147: Wilderness_Area
393/148:
data.groupby(['Wilderness_Area'])
ax = sns.countplot(x='Cover_Type',hue="Wilderness_Area",data=data)
393/149:
data.groupby(['Wilderness_Area'])
ax = sns.countplot(hue='Cover_Type',x="Wilderness_Area",data=data)
393/150:
data.groupby(['Wilderness_Area'])
ax = sns.countplot(x='Cover_Type',hue="Wilderness_Area",data=data)
393/151:

ax = sns.countplot(x='Cover_Type',hue="Soil_type",data=data)
393/152:

ax = sns.countplot(x='Cover_Type',hue="Soil_Type",data=data)
393/153: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data)
393/154: ax = sns.countplot(x='Cover_Type',hue="Soil_Type",data=data)
393/155: data.loc[soil_Type.values>10000]
393/156: ax = sns.countplot(x='Cover_Type',hue="Soil_Type",data=data.loc[soil_Type.values>10000])
393/157: data.loc[soil_Type.values>10000].shape
393/158: data.loc[soil_Type.values>1000].shape
393/159: ax = sns.countplot(x='Cover_Type',hue="Soil_Type",data=data.loc[soil_Type.values>1000])
393/160: data.loc[soil_Type.values>500].shape
393/161: ax = sns.countplot(x='Cover_Type',hue="Soil_Type",data=data.loc[soil_Type.values>500])
393/162: data.loc[soil_Type.values<500].shape
393/163: data.loc[soil_Type.values>500].shape
393/164: data.shape
393/165: data.loc[soil_Type.values>=500].shape
393/166: data.loc[soil_Type.values].shape
393/167: data.loc[soil_Type].shape
393/168: soil_Type
393/169: data.loc['soil_Type'].shape
393/170: data.loc['soil_Type']
393/171: data['soil_Type']
393/172: soil_Type.index(soil_Type.values>1000)
393/173: (soil_Type.values>1000)
393/174: soil_Type(soil_Type.values>1000)
393/175: soil_Type[soil_Type.values>1000]
393/176: soil_Type[soil_Type.values>1000].index
393/177: soil_Type
393/178: soil_Type[soil_Type.values>1000].index
393/179: len(soil_Type[soil_Type.values>1000].index)
393/180: (soil_Type[soil_Type.values>1000].index)
393/181: len(soil_Type[soil_Type.values>1000].index)
393/182: (soil_Type[soil_Type.values>1000].index)
393/183: (soil_Type[soil_Type.values>2000].index)
393/184: len(soil_Type[soil_Type.values>2000].index)
393/185: plt.hist(x, normed=True, bins=30)
393/186: plt.hist(soil_Type, normed=True, bins=30)
393/187: plt.hist(soil_Type, bins=30)
393/188: plt.hist(soil_Type, bins=10)
393/189: plt.hist(data['Soil_Type'], bins=10)
393/190: plt.hist(data['Soil_Type'], bins=20)
393/191: plt.hist(data['Soil_Type'], bins=4)
393/192: plt.hist(data['Soil_Type'], bins=30)
393/193: ax = sns.countplot(x='Cover_Type',hue="Soil_Type",data=data)
393/194: (soil_Type[soil_Type.values>2000].index)
393/195: data.loc[data.soil_Type is in (soil_Type[soil_Type.values>2000].index)]
393/196: data.loc[data.soil_Type isin(soil_Type[soil_Type.values>2000].index)]
393/197: data.loc[data.soil_Type.isin(soil_Type[soil_Type.values>2000].index)]
393/198: data.loc[data['soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
393/199: data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
393/200: data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)][0]
393/201: data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)][0,1]
393/202: data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)].shape()
393/203: data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
393/204:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
x.type
393/205:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
x.dtype
393/206:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
x.shape
393/207:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
x.shape
data.shape
393/208:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
x.shape
data.shape
393/209:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
x.shape
393/210:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>2000].index)]
x['Soil_Type']
393/211:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>3000].index)]
x['Soil_Type']
393/212:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>3000].index)]
x['Soil_Type'].shape
393/213:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>10000].index)]
x['Soil_Type'].shape
393/214:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>10000].index)]
x['Soil_Type']
393/215:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>10000].index)]
x.shape
393/216:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>10000].index)]
x.shape
data.shape
393/217:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index)]
x.shape
data.shape
393/218:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index)]
x.shape
393/219:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index)]
x.shape
soil_Type.values>100000
393/220:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index)]
x.shape
data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index)
393/221:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index)]
x.shape
sum(data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index))
393/222:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index)]
x.shape
393/223:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index)]
x
393/224:
x=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>100000].index)]
x.Soil_Type.value_counts()
393/225: ax = sns.countplot(hnu='Cover_Type',x="Soil_Type",data=data)
393/226: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data)
393/227:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>5000].index)]
                  )
393/228:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>50000].index)]
                  )
393/229:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values<50000 & soil_Type.values>200000].index)]
                  )
393/230:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values<50000 and soil_Type.values>200000].index)]
                  )
393/231:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<50000) and (soil_Type.values>200000)].index)]
                  )
393/232:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<50000) & (soil_Type.values>200000)].index)]
                  )
393/233:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values<50000 and ].index)]
                  )
393/234:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values<50000].index)]
                  )
393/235:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values<5000].index)]
                  )
393/236:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values<500].index)]
                  )
393/237:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>70000].index)]
                  )
393/238:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values>40000].index)]
                  )
393/239:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values<400000].index)]
                  )
393/240:
ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[soil_Type.values<400000 and soil_Type.values<400000].index)]
                  )
393/241: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin((soil_Type[soil_Type.values<400000]) & (soil_Type.values<400000)].index)])
393/242: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<400000) & (soil_Type.values>400000)].index)])
393/243: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<400000) & (soil_Type.values>200000)].index)])
393/244: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<400000) & (soil_Type.values>100000)].index)])
393/245: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<400000) & (soil_Type.values>10000)].index)])
393/246: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<400000) & (soil_Type.values>50000)].index)])
393/247: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<400000) & (soil_Type.values>10000)].index)])
393/248: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<400000) & (soil_Type.values>50000)].index)])
393/249: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<400000) & (soil_Type.values>50000)].index)])
393/250: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<50000) & (soil_Type.values>100)].index)])
393/251: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<50000) & (soil_Type.values>10000)].index)])
393/252: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<10000) & (soil_Type.values>5000)].index)])
393/253: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<5000) & (soil_Type.values>1000)].index)])
393/254: ax = sns.countplot(x='Cover_Type',hue="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<5000) & (soil_Type.values>1000)].index)])
393/255: ax = sns.countplot(hue='Cover_Type',x="Soil_Type",data=data.loc[data['Soil_Type'].isin(soil_Type[(soil_Type.values<5000) & (soil_Type.values>1000)].index)])
393/256:
target_freq=data['Cover_Type'].value_counts()
target_freq
393/257: data['Cover_Type']==4
393/258: data.loc[data['Cover_Type']==4]
393/259: data.loc[data['Cover_Type']==4]['Soil_Type'].value_counts()
393/260:
soil_cover4 = data.loc[data['Cover_Type']==4]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover4.index, soil_cover4.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 4')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/261:
soil_cover2 = data.loc[data['Cover_Type']==2]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover2.index, soil_cover2.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/262: soil_cover4
393/263:
soil_cover1 = data.loc[data['Cover_Type']==1]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover1.index, soil_cover1.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 1')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/264:
soil_cover3 = data.loc[data['Cover_Type']==3]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover3.index, soil_cover3.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 3')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/265:
soil_cover12 = data.loc[(data['Cover_Type']==1) | (data['Cover_Type']==2)]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover1.index, soil_cover12.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 1 or 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/266:
soil_cover12 = data.loc[(data['Cover_Type']==1) or (data['Cover_Type']==2)]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover1.index, soil_cover12.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 1 or 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/267:
soil_cover12 = data.loc[(data['Cover_Type']==1) | (data['Cover_Type']==2)]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover1.index, soil_cover12.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 1 or 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/268:
soil_cover12 = data.loc[(data['Cover_Type']==1) | (data['Cover_Type']==2)]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover12.index, soil_cover12.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 1 or 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/269:
soil_cover = data.loc[(data['Cover_Type']!=1) & (data['Cover_Type']!=2)]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover.index, soil_cover.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 1 or 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/270:
soil_cover4 = data.loc[data['Cover_Type']==4]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover4.index, soil_cover4.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 4')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/271:
soil_cover = data.loc[(data['Cover_Type']!=1) & (data['Cover_Type']!=2)]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover.index, soil_cover.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type not 1 nor 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/272:
soil_cover3 = data.loc[data['Cover_Type']==3]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover3.index, soil_cover3.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 3')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/273:
soil_cover5 = data.loc[data['Cover_Type']==5]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover5.index, soil_cover5.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 3')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/274:
soil_cover5 = data.loc[data['Cover_Type']==5]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover5.index, soil_cover5.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 5')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/275:
soil_cover6 = data.loc[data['Cover_Type']==6]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover6.index, soil_cover6.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 6')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/276:
soil_cover7 = data.loc[data['Cover_Type']==7]['Soil_Type'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(soil_cover7.index, soil_cover7.values, alpha=0.8)
plt.title('Distribution of the soil_Type for Cover Type 7')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/277: data.loc[data['Soil_Type']>37]['Cover_Type'].value_counts()
393/278: data.loc[data['Soil_Type']=10]['Cover_Type'].value_counts()
393/279: data.loc[data['Soil_Type']==10]['Cover_Type'].value_counts()
393/280: data.loc[data['Soil_Type']>37]['Cover_Type'=7].value_counts()
393/281: data.loc[data['Soil_Type']>37]['Cover_Type'==7].value_counts()
393/282: data.loc[data['Soil_Type']>37]['Cover_Type'].value_counts()
393/283: data.loc[(data['Soil_Type']>37) & (data['Cover_Type']==7)]
393/284: data.loc[(data['Soil_Type']>37) & (data['Cover_Type']==7)].shape
393/285: data.loc[(data['Cover_Type']==7)].shape
393/286: data.loc[(data['Cover_Type']==7)].shape[0]
393/287: data.loc[(data['Soil_Type']>37) & (data['Cover_Type']==7)].shape[0]
393/288: (data.loc[(data['Soil_Type']>37) & (data['Cover_Type']==7)].shape[0/(data.loc[(data['Cover_Type']==7)].shape[0])*100
393/289: (data.loc[(data['Soil_Type']>37) & (data['Cover_Type']==7)].shape[0/(data.loc[(data['Cover_Type']==7)].shape[0])
393/290: (data.loc[(data['Soil_Type']>37) & (data['Cover_Type']==7)].shape[0])/data.loc[(data['Cover_Type']==7)].shape[0]
393/291: ((data.loc[(data['Soil_Type']>37) & (data['Cover_Type']==7)].shape[0])/data.loc[(data['Cover_Type']==7)].shape[0])*100
393/292: ((data.loc[(data['Soil_Type']>37) & (data['Cover_Type']==1)].shape[0])/data.loc[(data['Cover_Type']==1)].shape[0])*100
393/293:
Wilderness_Area
Wilderness_Area1 = data.loc[data['Cover_Type']==1]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area1.index, Wilderness_Area1.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area1 for Cover Type 1')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('soil_Type', fontsize=12)
plt.show()
393/294:

Wilderness_Area1 = data.loc[data['Cover_Type']==1]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area1.index, Wilderness_Area1.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 1')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/295:

Wilderness_Area2 = data.loc[data['Cover_Type']==2]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area2.index, Wilderness_Area2.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/296:

Wilderness_Area3 = data.loc[data['Cover_Type']==3]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area3.index, Wilderness_Area3.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 3')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/297:

Wilderness_Area4 = data.loc[data['Cover_Type']==4]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area4.index, Wilderness_Area4.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 4')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/298:

Wilderness_Area5 = data.loc[data['Cover_Type']==5]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area5.index, Wilderness_Area5.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 5')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/299:

Wilderness_Area6 = data.loc[data['Cover_Type']==6]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area6.index, Wilderness_Area6.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 6')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/300:

Wilderness_Area7 = data.loc[data['Cover_Type']==7]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area1.index, Wilderness_Area7.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 27')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/301:

Wilderness_Area7 = data.loc[data['Cover_Type']==7]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area1.index, Wilderness_Area7.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 7')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/302:

Wilderness_Area12 = data.loc[(data['Cover_Type']==2) & (data['Cover_Type']==1)]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area12.index, Wilderness_Area12.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 1 and 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/303:

Wilderness_Area12 = data.loc[(data['Cover_Type']==2)|(data['Cover_Type']==1)]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area12.index, Wilderness_Area12.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 1 and 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/304:

Wilderness_Area12 = data.loc[(data['Cover_Type']!=2)& (data['Cover_Type']!=1)]['Wilderness_Area'].value_counts()

plt.figure(figsize=(10,5))
sns.barplot(Wilderness_Area12.index, Wilderness_Area12.values, alpha=0.8)
plt.title('Distribution of the Wilderness_Area for Cover Type 1 and 2')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Wilderness_Area', fontsize=12)
plt.show()
393/305:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
from sklearn.base import clone
from sklearn.feature_selection import SelectFromModel
from pandas.plotting import scatter_matrix
393/306: scatter_matrix(data)
   1:
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
from sklearn.base import clone
from sklearn.feature_selection import SelectFromModel
   2: data=pd.read_csv('covtype.csv')
   3: data.shape
   4: data.info()
   5: data.isnull().sum()
   6: data.describe()
   7:
# Extracting all numerical features from data
num_fea = data.iloc[:, :10]

# extracting all binary features from data
binary_fea = data.iloc[:, 10:-1]


# statistics of numerical features
num_fea.describe()
   8:
data['Cover_Type'].dtypes
data['Cover_Type'].value_counts()
   9: train,test = train_test_split(data,test_size=0.2,random_state=999)
  10: %history -g -f filename
  11: %history -g -f RF
